[
  {
    "objectID": "Stats/index.html",
    "href": "Stats/index.html",
    "title": "Statistics & Probability",
    "section": "",
    "text": "EDA PART 1, Summary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDA PART 2, Graphical Analysis\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Stats/2022-08-21-Stats1.html",
    "href": "Stats/2022-08-21-Stats1.html",
    "title": "EDA PART 1, Summary Statistics",
    "section": "",
    "text": "In this notebook we’ll go through some basic concepts that most if not all statisticians and data analysts look at when they first encouter a data set\nExploratory Data Analysis (EDA) is a systematic and unbiased approach to analyze and understand a dataset by employing statistical techniques and visualizations. It involves thoroughly examining the dataset from various perspectives, summarizing its characteristics, and identifying key patterns, trends, or anomalies without making any prior assumptions about the data’s nature or underlying relationships. EDA aims to provide meaningful insights and uncover relevant features of the data that can guide further analysis and decision-making processes.\nThere are two types of data, categorical and numerical data.\nCategorical data, also known as qualitative data, is a type of data that represents characteristics or attributes that belong to a specific category or group\nWhile, Numerical data, also known as quantitative data, is a type of data that consists of numeric values representing measurable quantities or variables.\nUnivariate Analysis: Univariate analysis is a type of exploratory data analysis that focuses on analyzing or dealing with only one variable at a time. It involves examining and describing the data of a single variable, aiming to identify patterns and characteristics within that variable. Univariate analysis does not consider causes or relationships and primarily serves the purpose of providing insights and summarizing data.\nBivariate Analysis: Bivariate analysis is a type of exploratory data analysis that involves the analysis of two different variables simultaneously. It explores the relationship between these two variables, aiming to understand how changes in one variable may impact the other. Bivariate analysis delves into causality and relationships, seeking to identify associations and dependencies between the two variables under investigation.\nMultivariate Analysis: Multivariate analysis is a type of exploratory data analysis that deals with datasets containing three or more variables. It examines the relationships and patterns between multiple variables, allowing for a more comprehensive analysis of the data. Multivariate analysis employs various statistical techniques and graphical representations to uncover complex relationships and interactions among the variables, facilitating a deeper understanding of the dataset as a whole.\nEDA consists of two parts,\nNon-graphical Analysis and Graphical Analysis\nNon-graphical Analysis: Non-graphical analysis in exploratory data analysis involves examining and analyzing data using statistical tools and measures such summary statistic that quantitatively describes or summarizes features of a dataset. It focuses on understanding the characteristics and patterns for mostly one variable but can also be used for two or more variables.\nGraphical Analysis: Graphical analysis is the most common part of exploratory data analysis that utilizes visualizations and charts to analyze and interpret data. It involves representing data in graphical forms to visually identify trends, patterns, distributions, relationships between variables or even compare different variables. Graphical analysis provides a comprehensive view of the data, allowing for a better understanding of the underlying structure and facilitating the exploration of multivariate relationships.\nWe’ll start off by first performing the non graphical part then we will finish with graphical analysis in the second part"
  },
  {
    "objectID": "Stats/2022-08-21-Stats1.html#measures-of-central-tendency",
    "href": "Stats/2022-08-21-Stats1.html#measures-of-central-tendency",
    "title": "EDA PART 1, Summary Statistics",
    "section": "Measures of central tendency:",
    "text": "Measures of central tendency:\nA measure of central tendency (also referred to as measures of center or central location) is a summary measure that attempts to describe a whole set of data with a single value that represents the middle or center of its distribution.\n\nMean:\nThe arithmetic mean is the sum of the observations divided by the number of observations. The arithmetic mean is by far the most frequently used measure of the middle or center of data. The mean is also referred to as the average The population mean, μ, is the arithmetic mean value of a population. For a finite population, the population mean is:\n\\[\\mu = \\dfrac{\\sum_{i=1}^N X_i}{N} \\]\nwhere \\(N\\) is the number of observations in the entire population and \\(X_i\\) is the \\(i\\)th observation.\nThe sample mean is the arithmetic mean computed for a sample. A sample is a percentage of the total population in statistics. You can use the data from a sample to make inferences about a population as a whole. The concept of the mean can be applied to the observations in a sample with a slight change in notation.\n\\[\\bar{x} = \\dfrac{\\sum_{i=1}^n X_i}{n} \\]\nwhere \\(n\\) is the number of observations in the sample.\n\n#importing relevant libraries\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nimport scipy.stats as stats\nimport statistics\n\n\n#Downloading data\nmsft_daily = yf.download('MSFT', '2016-01-01', '2021-12-31', interval= '1d')['Adj Close']\n\n\n#Calculating average return\nmsft_returns = msft_daily.pct_change(1).dropna()\naverage_return = str(round((np.mean(msft_returns) * 100),2)) + '%'\nprint(f'The average daily return for Microsoft stock is: {average_return}')\n\nThe average daily return for Microsoft stock is: 0.14%\n\n\n\n\nWeighted mean:\nThe ordinary arithmetic mean is where all sample observations are equally weighted by the factor 1/n (each of the data points contributes equally to the final average).\nBut with the weighted mean, some data points contribute more than others based on their weighting, the higher the weighting, the more it influences the final average. The weighted mean is also referred to as the weighted average\n\\[\\bar{X}_w = {\\sum_{i=1}^n w_i X_i} \\]\nwhere the sum of the weights equals 1; that is,\\(\\sum_{i} w_i = 1\\)\n\n#Downloading data\naapl = yf.download('AAPL', '2021-10-01', '2021-12-31', interval= '1d')['Adj Close']\nnvdia = yf.download('NVDA', '2021-10-01', '2021-12-31', interval= '1d')['Adj Close']\nmsft = yf.download('MSFT', '2021-10-01', '2021-12-31', interval= '1d')['Adj Close']\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n#Calculating 2021 Q4 return\nmsft_ret = (msft[-1] - msft[0])/msft[0]\naapl_ret = (aapl[-1] - aapl[0])/aapl[0]\nnvda_ret = (nvdia[-1] - nvdia[0])/aapl[0]\n\n#portfolio return if 50% of capital was deplyoed in microsoft,30% in apple and 20% in nvidia\nWavg = (msft_ret * .50 + aapl_ret + .30 + nvda_ret * .20)/3\navg = (msft_ret + aapl_ret + nvda_ret)/3\n\nweighted = str(round(Wavg,2)) + '%'\narith = str(round(avg,2)) + '%'\n\nprint(f\"The Weighted mean return of the portfolio assuming a 50/30/20 split is: {weighted}\")\nprint(f\"The Arithmetic mean return of the portfolio assuming no split is:\", arith)\n\nThe Weighted mean return of the portfolio assuming a 50/30/20 split is: 0.25%\nThe Arithmetic mean return of the portfolio assuming no split is: 0.35%\n\n\nThe weighted mean is also very useful when calculating a theoretically expected outcome where each outcome has a different probability of occurring (more on this in probability concepts)\n\n\nHarmonic mean:\nThe harmonic mean is a type of numerical average. It is calculated by dividing the number of observations by the reciprocal of each number in the series. Thus, the harmonic mean is the reciprocal of the arithmetic mean of the reciprocals.\n\\[\\bar{X}_h = \\dfrac{n}{\\sum_{i=1}^n \\dfrac1X_i} \\]\n\n\nGeometric mean:\nThe geometric mean is most frequently used to average rates of change over time or to compute the growth rate of a variable.\nFor volatile numbers like stock returns, the geometric average provides a far more accurate measurement of the true return by taking into account year-over-year compounding that smooths the average.\n\\[ G = \\sqrt[n]{X_1,X_2,X_3....X_n} \\]\nData Source: https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information?resource=download\n\nSnP_500 = pd.read_csv('/home/mj22/data/financials.csv')\nSnP_500\n\n\n\n\n\n\n\n\nSymbol\nName\nSector\nPrice\nPrice/Earnings\nDividend Yield\nEarnings/Share\n52 Week Low\n52 Week High\nMarket Cap\nEBITDA\nPrice/Sales\nPrice/Book\nSEC Filings\n\n\n\n\n0\nMMM\n3M Company\nIndustrials\n222.89\n24.31\n2.332862\n7.92\n259.77\n175.490\n1.387211e+11\n9.048000e+09\n4.390271\n11.34\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n1\nAOS\nA.O. Smith Corp\nIndustrials\n60.24\n27.76\n1.147959\n1.70\n68.39\n48.925\n1.078342e+10\n6.010000e+08\n3.575483\n6.35\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n2\nABT\nAbbott Laboratories\nHealth Care\n56.27\n22.51\n1.908982\n0.26\n64.60\n42.280\n1.021210e+11\n5.744000e+09\n3.740480\n3.19\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n3\nABBV\nAbbVie Inc.\nHealth Care\n108.48\n19.41\n2.499560\n3.29\n125.86\n60.050\n1.813863e+11\n1.031000e+10\n6.291571\n26.14\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n4\nACN\nAccenture plc\nInformation Technology\n150.51\n25.47\n1.714470\n5.44\n162.60\n114.820\n9.876586e+10\n5.643228e+09\n2.604117\n10.62\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n500\nXYL\nXylem Inc.\nIndustrials\n70.24\n30.94\n1.170079\n1.83\n76.81\n46.860\n1.291502e+10\n7.220000e+08\n2.726209\n5.31\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n501\nYUM\nYum! Brands Inc\nConsumer Discretionary\n76.30\n27.25\n1.797080\n4.07\n86.93\n62.850\n2.700330e+10\n2.289000e+09\n6.313636\n212.08\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n502\nZBH\nZimmer Biomet Holdings\nHealth Care\n115.53\n14.32\n0.794834\n9.01\n133.49\n108.170\n2.445470e+10\n2.007400e+09\n3.164895\n2.39\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n503\nZION\nZions Bancorp\nFinancials\n50.71\n17.73\n1.480933\n2.60\n55.61\n38.430\n1.067068e+10\n0.000000e+00\n3.794579\n1.42\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n504\nZTS\nZoetis\nHealth Care\n71.51\n32.80\n0.682372\n1.65\n80.13\n52.000\n3.599111e+10\n1.734000e+09\n9.280896\n18.09\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n\n\n505 rows × 14 columns\n\n\n\n\n# Use scipy's gmean&hmean function to compute the geometric mean and harmonic mean\nprint(\"The Harmonic mean of Price to Sales Ratio for companies in the S&P 500 is:\", round(stats.hmean(SnP_500['Price/Sales']),2))\nprint(\"The Geometric mean of  Price to Sales Ratio  for companies in the S&P 500 is:\", round(stats.gmean(SnP_500['Price/Sales']),2))\nprint(\"The Arithmetic mean of  Price to Sales Ratio  for companies in the S&P 500 is:\", round(np.mean(SnP_500['Price/Sales']),2))\n\nThe Harmonic mean of Price to Sales Ratio for companies in the S&P 500 is: 1.95\nThe Geometric mean of  Price to Sales Ratio  for companies in the S&P 500 is: 2.83\nThe Arithmetic mean of  Price to Sales Ratio  for companies in the S&P 500 is: 3.94\n\n\n\n“A mathematical fact concerning the harmonic, geometric, and arithmetic means is that unless all the observations in a data set have the same value, the harmonic mean is less than the geometric mean, which in turn is less than the arithmetic mean” – Quantitative Investment Analysis, by DeFusco, McLeavey, Pinto, and Runkle\n\n\n\nTrimmed mean:\nA trimmed mean is a method of averaging that removes a small designated percentage of the largest and smallest values before calculating the mean. After removing the specified outlier observations, the trimmed mean is found using a standard arithmetic averaging formula. The use of a trimmed mean helps eliminate the influence of outliers or data points on the tails that may unfairly affect the traditional or arithmetic mean.\nTo trim the mean by a total of 40%, we remove the lowest 20% and the highest 20% of values, eliminating the scores of 8 and 9\n\ndata = [8, 2, 3, 4, 9]\nmean = np.mean(data)\n\ntrimmed_data = [2, 3, 4]\ntrimmed_mean = np.mean(trimmed_data)\n\nprint(f\"Hence, a mean trimmed at 40% would equal {trimmed_mean} versus {mean}\")\n\nHence, a mean trimmed at 40% would equal 3.0 versus 5.2\n\n\n\n\nMedian:\nThe median is the middle number in a sorted, ascending, or descending list of numbers and can be more descriptive of that data set than the average. It is the point above and below which half (50%) of the observed data falls, and so represents the midpoint of the data.\n\nprint(f'The Median Price to Sales Ratio for Companies in the S&P 500 is:', round(np.median(SnP_500['Price/Sales']),2))\n\nThe Median Price to Sales Ratio for Companies in the S&P 500 is: 2.9\n\n\n\n\nMode:\nThe mode is the value that appears most frequently in a data set A distribution can have more than one mode or even no mode. When a distribution has one most frequently occurring value, the distribution is said to be unimodal. If a distribution has two most frequently occurring values, then it has two modes and we say it is bimodal. If the distribution has three most frequently occurring values, then it is trimodal. When all the values in a data set are different, the distribution has no mode because no value occurs more frequently than any other value.\n\nmode =  round(statistics.mode(SnP_500['Price/Sales']),2)\nprint(f'The Mode of the Price to Sales ratio for Companies in the S&P 500 is {mode}, indicating that it is the most commonly occurring value among the dataset')\n\nThe Mode of the Price to Sales ratio for Companies in the S&P 500 is 4.39, indicating that it is the most commonly occurring value among the dataset"
  },
  {
    "objectID": "Stats/2022-08-21-Stats1.html#measures-of-dispersion",
    "href": "Stats/2022-08-21-Stats1.html#measures-of-dispersion",
    "title": "EDA PART 1, Summary Statistics",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nLocation is just one dimension in describing data. A second dimension, variability, also referred to as dispersion, measures whether the data values are tightly clustered or spread out.\n\nRange:\nRange, the range of a set of data is the difference between the largest and smallest values, the result of subtracting the sample maximum and minimum. It is expressed in the same units as the data.\n\nmaX = np.max(dividends)\nmiN = np.min(dividends)\nRange= np.ptp(dividends)\n\nprint(f'The maximum dividend per share paid by Microsoft is {maX}$') \nprint(f'The minimum dividend per share paid by Microsoft is {miN}$')\nprint(f'Hence, the range is {Range}$')\n\nThe maximum dividend per share paid by Microsoft is 3.08$\nThe minimum dividend per share paid by Microsoft is 0.08$\nHence, the range is 3.0$\n\n\n\n\nVariance sample and population:\nThe variance and standard deviation are the two most widely used measures of dispersion Variance is defined as the average of the squared deviations around the mean. Population variance is a measure of the spread of population data. Hence, population variance can be defined as the average of the distances from each data point in a particular population to the mean squared, and it indicates how data points are spread out in the population. we can compute the population variance.Denoted by the symbol σ2\nPopulation formula:\n\\[\\sigma^2 = \\dfrac{\\sum_{i=1}^N(x_i - \\mu)^2}N \\]\nWhile sample formula is:\n\\[s^2 = \\dfrac{\\sum_{i=1}^n(x_i - \\bar x)^2}{n-1} \\]\n\n\nStandard deviation sample and population:\nStandard Deviation (SD) is the positive square root of the variance. It is represented by the Greek letter ‘σ’ and is used to measure the amount of variation or dispersion of a set of data values relative to its mean (average), thus interpret the reliability of the data. If it is smaller then the data points lie close to the mean value, thus showing reliability. But if it is larger then data points spread far from the mean.\nPopulation formula:\n\\[\\sigma^2 = \\sqrt{\\dfrac{\\sum_{i=1}^N(x_i - \\mu)^2}N} \\]\nWhile sample formula is:\n\\[s^2 = \\sqrt{\\dfrac{\\sum_{i=1}^n(x_i - \\bar x)^2}{n-1}} \\]\nHence, Variance Measures Dispersion within the Data Set while the standard deviation measures spread around the mean!\n\nprint(\"The variance of Microsoft's daily stock returns:\", round(np.var(msft_returns),5))\nprint(\"The standard deviation of Microsoft's daily stock returns:\", round(np.std(msft_returns),5))\n\nThe variance of Microsoft's daily stock returns: 0.00028\nThe standard deviation of Microsoft's daily stock returns: 0.01684"
  },
  {
    "objectID": "Stats/2022-08-21-Stats1.html#relationship-between-variables",
    "href": "Stats/2022-08-21-Stats1.html#relationship-between-variables",
    "title": "EDA PART 1, Summary Statistics",
    "section": "Relationship between variables",
    "text": "Relationship between variables\nRelationship between variables means, In a dataset, the values of one variable correspond to the values of another variable.\nBy conducting a non-graphical analysis of the relationship between variables, you can quantitatively assess their associations, dependencies, and impacts, providing valuable insights for further analysis and decision-making\nMoreover, non-graphical analysis of the relationship between variables involves examining the numerical values in a matrix to understand the connections between variables. A covariance matrix and a correlation matrix are square matrices that display the pairwise relationships between different variables in a dataset. They provide valuable insights into the strength and direction of the relationships between variables\n\nCovariance\nCovariance provides insight into how two variables are related to one another. More precisely, covariance refers to the measure of how two random variables in a data set will change together. A positive covariance means that the two variables at hand are positively related, and they move in the same direction. A negative covariance means that the variables are inversely related, or that they move in opposite directions. Both variance and covariance measure how data points are distributed around a calculated mean. However, variance measures the spread of data along a single axis, while covariance examines the directional relationship between two variables.\nPopulation formula:\n\\[\\sigma_{xy} = \\dfrac{\\sum_{i=1}^N(x_i - \\mu_x)*(y_i - \\mu_y)}N \\]\nWhile sample formula is:\n\\[s_{xy} = \\dfrac{\\sum_{i=1}^n(x_i - \\bar x)*(y_i - \\bar y)}{n-1} \\]\n\nassets = ['META','AMZN','NFLX','GOOG','MSFT','NVDA','TSLA']\npf_data = pd.DataFrame()\nfor a in assets:\n    pf_data[a] = yf.download(a, start=\"2021-10-01\", end=\"2021-12-31\", index_col = 'Date', parse_dates=True)['Adj Close']\n\nreturns = pf_data.pct_change(1).dropna()\ncov = returns.cov()\ncorr = returns.corr()\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\ncov\n\n\n\n\n\n\n\n\nMETA\nAMZN\nNFLX\nGOOG\nMSFT\nNVDA\nTSLA\n\n\n\n\nMETA\n0.000404\n0.000137\n0.000143\n0.000135\n0.000134\n0.000252\n0.000189\n\n\nAMZN\n0.000137\n0.000253\n0.000075\n0.000142\n0.000111\n0.000362\n0.000064\n\n\nNFLX\n0.000143\n0.000075\n0.000319\n0.000098\n0.000115\n0.000196\n0.000151\n\n\nGOOG\n0.000135\n0.000142\n0.000098\n0.000221\n0.000177\n0.000284\n0.000074\n\n\nMSFT\n0.000134\n0.000111\n0.000115\n0.000177\n0.000207\n0.000287\n0.000182\n\n\nNVDA\n0.000252\n0.000362\n0.000196\n0.000284\n0.000287\n0.001286\n0.000490\n\n\nTSLA\n0.000189\n0.000064\n0.000151\n0.000074\n0.000182\n0.000490\n0.001463\n\n\n\n\n\n\n\n\n\nCorrelation coefficient\nCorrelation shows the strength of a relationship between two variables and is expressed numerically by the correlation coefficient. While covariance measures the direction of a relationship between two variables, correlation measures the strength of that relationship. There are many different measures of correlation but the most common one, and the one I use is the Pearson Coefficient of Correlation.\nOutput values of the Pearson Correlation Coefficient range between values of +1 and -1, or 100% and -100%, where +1 represents perfect positive correlation and -1 perfect negative correlation. A measure of 0 would suggest the two variables are perfectly uncorrelated, and there is no linear relationship between them. However, that doesn’t necessarily mean the variables are independent – as they might have a relationship that is not linear. Scatterplot charts are a good way of visualizing various values for correlation\nPopulation formula:\n\\[p = \\dfrac{\\sigma_{xy}}{\\sigma_y\\sigma_x} \\]\nWhile sample formula is:\n\\[r = \\dfrac{s_{xy}}{s_ys_x} \\]\n\ncorr\n\n\n\n\n\n\n\n\nMETA\nAMZN\nNFLX\nGOOG\nMSFT\nNVDA\nTSLA\n\n\n\n\nMETA\n1.000000\n0.426954\n0.399251\n0.451141\n0.462463\n0.349047\n0.245190\n\n\nAMZN\n0.426954\n1.000000\n0.263913\n0.599965\n0.486596\n0.635210\n0.105951\n\n\nNFLX\n0.399251\n0.263913\n1.000000\n0.369411\n0.449536\n0.306624\n0.221220\n\n\nGOOG\n0.451141\n0.599965\n0.369411\n1.000000\n0.829273\n0.532455\n0.130574\n\n\nMSFT\n0.462463\n0.486596\n0.449536\n0.829273\n1.000000\n0.556526\n0.331061\n\n\nNVDA\n0.349047\n0.635210\n0.306624\n0.532455\n0.556526\n1.000000\n0.357039\n\n\nTSLA\n0.245190\n0.105951\n0.221220\n0.130574\n0.331061\n0.357039\n1.000000\n\n\n\n\n\n\n\nOne of the most common pitfalls of correlation analysis is that correlation is not causation!\nJust because two variables have shown a historic correlation doesn’t mean that one of the variables causes the other to move. The causation of the two variables moving with a positive or negative correlation could be a third completely unconsidered variable OR a combination of many factors. In theory, we want to try and understand the causes for relationships between variables so we can have a more accurate idea about when those relationships might change and if they will. The reality is that this is very hard to achieve and so practically speaking correlation analysis is often used to summarise relationships and use them as forward-looking predicator under the caveat that we understand it is likely that there are many factors at play that are responsible for the causation of the relationship.\n\n\nReferences\n\n“Quantitative Investment Analysis”, by DeFusco, McLeavey, Pinto, and Runkle\n“Practical Statistics for Data Scientists”, by Andrew Bruce, Peter C. Bruce, and Peter Gedeck"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html",
    "title": "Introduction",
    "section": "",
    "text": "In this project, I explored the impact of localized and often underused economic indicators on the UAE’s Non-Oil GDP growth. This work was part of research conferenc ei participated in under the research paper and Reeseach poster catgeories, which won 1st place for my research poster and received a special mention in the research paper track.\nThe UAE has made major progress in diversifying its economy away from oil. hence it becomes important to understand what drives short-term economic performance in the non oil areas like real estate, tourism, and credit markets.\nTraditional indicators like M2 supply, oil production, unemployment, or CPI often miss these localized shifts. That’s what motivated my research: to explore whether more localized and underused indicators can help better predict short-term non-oil GDP movements and provide more timely insights. This is beneficial for policymakers, businesses, and investors to help them make better-informed decisions.\nWhile several studies have shown that models like ARIMA, VAR, and OLS can forecast UAE GDP (McCloskey & Remor, 2025), these efforts have largely relied on broad macro variables such as oil prices, inflation, or global demand. Similarly, Bentour and Fund (2022) and Cherif et al. (2011) focused on traditional macroeconomic factors, with limited attention to localized, sector-specific dynamics. El Mahmah (2017) came closest to this study by using quarterly data and including the Purchasing Managers’ Index (PMI), but still relied heavily on conventional indicators.\nI will walk through the steps I took during the research process, from data acquisition to model development and evaluation. During my research poster and paper presentation, I had limited space and time to showcase the full process, so this notebook includes additional sections I couldn’t cover.\nAs always, I started by importing the core libraries that I frequently use. Additional libraries were imported later as needed throughout the analysis.\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.ticker import MaxNLocator"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#data-collection-preprocessing",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#data-collection-preprocessing",
    "title": "Introduction",
    "section": "Data Collection & Preprocessing",
    "text": "Data Collection & Preprocessing\nThe first step was acquiring the datasets and performing initial preprocessing to prepare the data for analysis. Some datasets were already available in Excel format, while others required manual entry. I prefer using Excel as my primary method of storing data since it allows for easy review, manual adjustments if any, and convenient retrieval.\n\nThe Dependent Variable\nThe dependent variable is the quarter-on-quarter growth rate (in %) of Non-Oil GDP at constant prices, sourced from the Central Bank of the UAE’s Quarterly Economic Review.\nPrimary Source: Central Bank Publications\nThe sample covers the period from Q1 2014 to Q2 2024, providing 42 quarterly observations. All independent variables were transformed to this same quarterly frequency to ensure proper alignment.\nAlternative sources for this data include:\n- UAE Quarterly National Accounts (FCSC)\n- FCSC Data Portal\nWhile multiple sources are available, it’s important to always cross-check the data and reference the primary source to ensure accuracy and validity.\n\n# Set relative path to the Excel file\nngdp_data_path = Path(\"/home/mj22/UAE GDP Research/Data/Non Oil GDP.xlsx\")\n# Define relative path to the PMI Excel file\npmi_path = Path(\"/home/mj22/UAE GDP Research/Data/PMI.xlsx\")\npersonal_path = Path(\"/home/mj22/UAE GDP Research/Data/Personal.xlsx\")\nbusiness_path = Path(\"/home/mj22/UAE GDP Research/Data/Business.xlsx\")\nresidential_path = Path(\"/home/mj22/UAE GDP Research/Data/Residential.xlsx\")\nvisitors_path = Path(\"/home/mj22/UAE GDP Research/Data/Visitors.xlsx\")\n\n\n\n# import the compiled data\nngdp = pd.read_excel(ngdp_data_path) # Use your own relevant file path, personal file path hidden for privacy purposes\n\nngdp.set_index('Date',inplace=True)\n\nngdp.index = ngdp.index.to_period('M') # Convert the date fromat to a PeriodIndex with monthly frequency (year and month)\n\nprint(ngdp.info())\n\nngdp.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 42 entries, 2014-03 to 2024-06\nFreq: M\nData columns (total 1 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   Non Oil GDP  42 non-null     float64\ndtypes: float64(1)\nmemory usage: 672.0 bytes\nNone\n\n\n\n\n\n\n\n\n\nNon Oil GDP\n\n\nDate\n\n\n\n\n\n2014-03\n5.0\n\n\n2014-06\n6.2\n\n\n2014-09\n5.1\n\n\n2014-12\n9.1\n\n\n2015-03\n7.4\n\n\n\n\n\n\n\n\n\nIndependent Variables\nThe following independent variables were selected for their potential to reflect localized economic activity that is often overlooked by traditional macroeconomic indicators.\nSome variables were originally reported at different frequencies (e.g., monthly) and covered varying time periods. As part of the preprocessing workflow, each series was first transformed to a quarterly frequency to ensure consistency with the dependent variable and was later on trimmed to match the observation window of the dependent variable in future sections, ensuring full alignment across the dataset.\n\nUAE Purchasing Managers’ Index (PMI)\nPMI data was collected from Trading Economics for recent periods and supplemented with earlier records from the OPEC Monthly Oil Market Reports for 2014 and 2015.\nI computed the quarterly average of the monthly PMI values to produce a single value per quarter, reflecting the overall sentiment of business activity in the UAE during that period. While I also considered using the quarter-on-quarter change, the PMI index itself is already informative, values above 50 indicate economic expansion, while values below 50 signal contraction.\nData Sources:\n- Trading Economics – UAE PMI (Account required to view full data)\n- OPEC Monthly Oil Market Report – 2015\n- OPEC Monthly Oil Market Report – 2014\n\n# Load the monthly PMI data\npmi = pd.read_excel(pmi_path) # Use your own relevant file path \n\n# Set the 'Date' column as the index\npmi.set_index('Date', inplace=True)\n\n# Convert index to monthly PeriodIndex (e.g., 2015-03)\npmi.index = pmi.index.to_period('M')\n\n# Resample monthly data to quarterly by taking the average of each quarter\npmi_quarterly = pmi.to_timestamp().resample('Q').mean()\n\n# Optional: Convert index back to monthly period for consistency with other series\npmi_quarterly.index = pmi_quarterly.index.to_period('M')\n\n\nprint(pmi_quarterly.info())\n\npmi_quarterly.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 44 entries, 2014-03 to 2024-12\nFreq: M\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   PMI     44 non-null     float64\ndtypes: float64(1)\nmemory usage: 704.0 bytes\nNone\n\n\n\n\n\n\n\n\n\nPMI\n\n\nDate\n\n\n\n\n\n2014-03\n57.366667\n\n\n2014-06\n57.933333\n\n\n2014-09\n58.000000\n\n\n2014-12\n59.300000\n\n\n2015-03\n57.900000\n\n\n\n\n\n\n\n\n\nDubai Residential Sales Price Index\nReal estate data was sourced from the Dubai Pulse Open Data Platform, based on the Mo’asher House Price Index developed by the Dubai Land Department in collaboration with Property Finder.\nThe index applies a hedonic regression methodology to adjust for property characteristics, allowing for meaningful comparisons over time. I calculated quarterly change using price levels from the first and last months of each quarter.\nUsing returns instead of average prices provides a clearer measure of price momentum and directional change, making it more suitable for time-series modeling and capturing economic signals relevant to GDP growth.\nData Source:\n- Dubai Pulse – Residential Sales Price Index\n\n# Load the monthly residential data\nresidential = pd.read_excel(residential_path) # Use your own relevant file path \n\n# Set the 'Date' column as the index\nresidential.set_index('Date', inplace=True)\n\n# Convert index to monthly PeriodIndex (e.g., 2015-03)\nresidential.index = residential.index.to_period('M')\n\n# Convert back to timestamp for resampling\nresidential_ts = residential.to_timestamp()\n\n# Get first and last month of each quarter and compute log return\nquarterly_open = residential_ts.resample('Q').first()\nquarterly_close = residential_ts.resample('Q').last()\n\n# Calculate log change: ln(P_last / P_first)\nresidential_log = np.log(quarterly_close / quarterly_open)\n\n# Convert index back to PeriodIndex for consistency\nresidential_log.index = residential_log.index.to_period('M')\n\nprint(residential_log.info())\n\nresidential_log.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 44 entries, 2014-03 to 2024-12\nFreq: M\nData columns (total 1 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Residential Sales Index  44 non-null     float64\ndtypes: float64(1)\nmemory usage: 704.0 bytes\nNone\n\n\n\n\n\n\n\n\n\nResidential Sales Index\n\n\nDate\n\n\n\n\n\n2014-03\n0.026271\n\n\n2014-06\n0.031828\n\n\n2014-09\n0.021766\n\n\n2014-12\n0.019616\n\n\n2015-03\n0.003086\n\n\n\n\n\n\n\n\n\nInternational Visitors to Dubai (Tourism)\nMonthly data on international visitor arrivals was sourced from the Dubai Department of Economy and Tourism (DDET), accessed via the Dubai Pulse Open Data Platform.\nTo capture the evolution of tourism activity in Dubai, I first calculated quarterly totals of international visitors. I then calculated the change of these quarterly totals to reflect the percentage change in tourism inflows from one quarter to the next. This approach focuses on how tourism activity is changing over time rather than its absolute level, which is more useful for identifying growth patterns and economic turning points that may influence non-oil GDP.\nData Sources:\n- Dubai Pulse – Tourism Data (2014–2023)\n- Dubai DET – 2024 Tourism Reports\nImportant Note for 2024 Data:\nThe 2024 tourism data was sourced manually from the Dubai DDET webpage reports. Since it is reported cumulatively, the previous month’s total must be subtracted from the current month to obtain the monthly figure. For example:\n- January visitors = 1.47 million\n- January–February cumulative visitors = 3.10 million\n- February visitors = 3.10 – 1.47 = 1.63 million\nI manually calculated it in Excel as it was a quicker and more convenient option than performing the adjustments in Python.\n\n# Load the monthly visitors data\nvisitors = pd.read_excel(visitors_path) # Use your own relevant file path \n\n# Set the 'Date' column as the index\nvisitors.set_index('Date', inplace=True)\n\n# Convert index to monthly PeriodIndex (e.g., 2015-03)\nvisitors.index = visitors.index.to_period('M')\n\n# Convert back to TimestampIndex for resampling\nvisitors_ts = visitors.to_timestamp()\n\n# Aggregate monthly visitor counts into quarterly totals\nvisitors_q = visitors_ts.resample(\"Q\").sum()\n\n# Replace zero visitors with small value to avoid log(0)\nvisitors_q['Visitors'] = visitors_q['Visitors'].replace(0, 1e-6)\n\n# Calculate log change to get quarter-over-quarter growth in visitor inflows\nvisitors_log = np.log(visitors_q / visitors_q.shift(1))\n\n# Drop the first NaN resulting from the shift\nvisitors_log = visitors_log.dropna()\n\n# Convert index back to PeriodIndex (optional, for consistency)\nvisitors_log.index = visitors_log.index.to_period('M')\n\nprint(visitors_log.info())\n\n# Preview the result\nvisitors_log.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 43 entries, 2014-06 to 2024-12\nFreq: M\nData columns (total 1 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Visitors  43 non-null     float64\ndtypes: float64(1)\nmemory usage: 688.0 bytes\nNone\n\n\n\n\n\n\n\n\n\nVisitors\n\n\nDate\n\n\n\n\n\n2014-06\n-0.119338\n\n\n2014-09\n-0.145027\n\n\n2014-12\n0.263317\n\n\n2015-03\n0.077719\n\n\n2015-06\n-0.153976\n\n\n\n\n\n\n\n\n\nPersonal and Business Lending (Net Balance Indices)\nThese indicators were sourced from the Credit Sentiment Survey published by the Central Bank of the UAE. The survey gathers responses from senior credit officers across banks and financial institutions in the UAE. The results are presented as net balance indices, representing the weighted percentage difference between respondents expecting an increase versus those expecting a decrease in loan demand.\nSpecifically, I used responses to the following forward-looking questions:\n- Personal Lending Demand: Derived from Question 7 asked to individuals — “Over the next quarter, how do you expect demand for personal loans to change?”\n- Business Lending Demand: Derived from Question 8 asked to businesses — “Over the next quarter, how do you expect demand for business loans to change?”\nEach index directly captures expectations about future loan demand and is reported quarterly, so no frequency adjustments were required.\nData Source:\n- Central Bank of the UAE – Publications (The Excel files contain all survey questions and corresponding indices.)\n\n# import the compiled data\npersonal = pd.read_excel(personal_path) # Use your own relevant file path \n\npersonal.set_index('Date',inplace=True)\n\npersonal.index = personal.index.to_period('M') # Convert the date fromat to a PeriodIndex with monthly frequency (year and month)\n\nprint(personal.info())\n\npersonal.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 44 entries, 2014-03 to 2024-12\nFreq: M\nData columns (total 1 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Personal Lending Demand  44 non-null     float64\ndtypes: float64(1)\nmemory usage: 704.0 bytes\nNone\n\n\n\n\n\n\n\n\n\nPersonal Lending Demand\n\n\nDate\n\n\n\n\n\n2014-03\n35.267857\n\n\n2014-06\n15.300000\n\n\n2014-09\n14.285714\n\n\n2014-12\n22.784810\n\n\n2015-03\n6.896552\n\n\n\n\n\n\n\n\n# import the compiled data\nbusiness = pd.read_excel(business_path) # Use your own relevant file path \n\nbusiness.set_index('Date',inplace=True)\n\nbusiness.index = business.index.to_period('M') # Convert the date fromat to a PeriodIndex with monthly frequency (year and month)\n\nprint(business.info())\n\nbusiness.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 44 entries, 2014-03 to 2024-12\nFreq: M\nData columns (total 1 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   Business Lending Demand  44 non-null     float64\ndtypes: float64(1)\nmemory usage: 704.0 bytes\nNone\n\n\n\n\n\n\n\n\n\nBusiness Lending Demand\n\n\nDate\n\n\n\n\n\n2014-03\n43.589744\n\n\n2014-06\n32.777778\n\n\n2014-09\n32.743363\n\n\n2014-12\n33.333333\n\n\n2015-03\n24.576271"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#exploratory-data-analysis",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#exploratory-data-analysis",
    "title": "Introduction",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nIn this section, I conducted a quick review of all the variables.\nI first plotted each series to inspect its raw behavior trends, seasonality and any outliers. After that, I reviewed summary statistics and distribution plots (with KDE overlays) to get a quick sense of central tendency, volatility and skewness.\nBelow are the key takeaways for each indicator without going into exhaustive detail.\n\nVisual Overview & Key Patterns\nNon-Oil GDP Quarterly Change (%)\nGDP growth oscillated around 5–7% before 2020, plunged to –9% in the COVID quarter, then sprang back above 10% briefly in late 2020. Since then, growth settled in a 3–7% band, showing a return to moderate expansion.\nUAE PMI Monthly Index\nThe PMI is hovering comfortably above 50 from 2014 to 2019, dipped sharply below 50 during early 2020 (COVID lockdowns), and then climbed back to mid-50s in the recovery phase. Seasonality was mild, but the COVID shock stood out clearly.\nResidential Sales Price Index\nHome prices were flat-to-slightly up from 2014–2019, dipped modestly in 2020, then accelerated steadily from 2021 onward rising from about 1.10 to 1.68 by 2024, reflecting a robust post-COVID housing boom.\nInternational Monthly Visitors (Millions)\nVisitor arrivals showed clear seasonal peaks each year, then collapsed almost to zero in early 2020. After COVID restrictions eased, arrivals rebounded strongly past pre-pandemic levels, topping 1.5–1.8 m by 2023.\nPersonal Lending Demand Index\nSentiment started around the mid-teens, drifted lower into single digits by 2016–2018, then plunged into negative territory in 2020. From late 2020 onward, optimism surged back to 20–30% readings, showing households regained confidence quickly.\nBusiness Lending Demand Index\nBusiness sentiment followed a similar shape with high optimism in 2014, a trough in 2016, a steep drop to near zero in 2020, then a strong rebound into the mid-20s and above by 2022. The bounce-back wasn’t quite as sharp as personal lending, but still pronounced.\nOverall Takeaway\nAll series showed a pronounced COVID-period disruption: a sharp dip followed by a synchronized rebound. Lending and PMI turned negative or below-trend in 2020, then recovered steadily. Visitor arrivals and house prices not only rebounded but overshot prior peaks, while GDP growth bounced back sharply and then normalized. This coordinated “dip-and-recover” pattern underscored the broad, economy-wide shock and subsequent stimulus-driven revival.\n\nfrom plotly.subplots import make_subplots\n\n# Dictionary of your dataframes\ndataframes = {\n    'Non-Oil GDP Quarterly Change': ngdp,    \n    'UAE PMI Monthly Index': pmi,\n    'Residential Sales Price Index': residential,\n    'International Monthly Visitors (In Millions)': visitors,\n    'Personal Lending Demand Index': personal,\n    'Business Lending Demand Index': business\n}\n\n# Single color for all plots\nline_color = 'red'\n\n# Create 2x3 subplot grid\nfig = make_subplots(\n    rows=2, cols=3,\n    subplot_titles=list(dataframes.keys()),\n    horizontal_spacing=0.1,\n    vertical_spacing=0.15\n)\n\n# Add traces\nfor idx, (name, df) in enumerate(dataframes.items()):\n    row = idx // 3 + 1\n    col = idx % 3 + 1\n\n    x_values = df.index.to_timestamp() if isinstance(df.index, pd.PeriodIndex) else df.index\n    y_values = df.iloc[:, 0]\n\n    fig.add_trace(\n        go.Scatter(\n            x=x_values,\n            y=y_values,\n            mode=\"lines+markers\",\n            line=dict(color=line_color, width=2),\n            marker=dict(size=4),\n            name=name,\n            showlegend=False\n        ),\n        row=row, col=col\n    )\n\n# Layout updates\nfig.update_layout(\n    height=700,\n    width=1200,  # Increased width for better spacing\n    paper_bgcolor='black',\n    plot_bgcolor='black',\n    font=dict(color='white'),\n    margin=dict(t=50)\n)\n\n# Remove gridlines and zero lines from all subplots\nfor i in range(1, 3):  # Rows\n    for j in range(1, 4):  # Columns\n        fig.update_xaxes(\n            row=i, col=j, \n            showgrid=False, \n            zeroline=False, \n            linecolor='white'\n        )\n        fig.update_yaxes(\n            row=i, col=j, \n            showgrid=False, \n            zeroline=False, \n            linecolor='white'\n        )\n\n# Ensure subplot titles are white\nfor ann in fig['layout']['annotations']:\n    ann['font'] = dict(color='white')\n\nfig.show()\n\n                                                \n\n\n\n\nSummary Statistics: QoQ Non-Oil GDP Growth (%)\nTo get a quick understanding of the distribution of the dependent variable, I reviewed the summary statistics and visualized the data using a histogram with a KDE (Kernel Density Estimate) line.\n\nMean = 3.99%: Shows that, on average, the non-oil sector experienced moderate growth during the period.\n\nStandard Deviation = 4.30%: Indicates noticeable fluctuations in quarterly growth rates.\n\nMinimum = -9.2%: The lowest recorded value, pointing to a sharp contraction during a tough quarter.\n\n25th Percentile = 2.55%: A quarter of the data falls below this value, suggesting that slower growth periods were fairly common.\n\nMedian = 5.0%: Half of the quarters recorded growth below this rate, highlighting moderate growth as the typical case.\n\n75th Percentile = 6.6%: Stronger growth periods were less frequent but still present.\n\nMaximum = 11.2%: The highest recorded growth rate, likely tied to an exceptional recovery or expansion period.\n\nInterquartile Range (IQR) = 4.05%: Shows there’s a decent spread in the middle 50% of the data.\n\nRange = 20.4%: From the sharpest contraction (-9.2%) to the strongest expansion (11.2%), indicating high volatility.\n\nOverall, non-oil GDP growth was quite volatile, with noticeable swings between strong growth and sharp contractions. This highlights the value of identifying early indicators to better anticipate these shifts.\n\nngdp.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nNon Oil GDP\n42.0\n3.992857\n4.295624\n-9.2\n2.55\n5.0\n6.6\n11.2\n\n\n\n\n\n\n\n\n\nAdditional Interpretations from the Plot\nShape of the Distribution: The distribution is moderately right-skewed, with the bulk of the values between 2.5% and 7%. This indicates that while positive growth is typical, there are a few unusually high growth rates that pull the distribution’s tail to the right.\nPresence of Outliers or Extremes: The histogram shows a small number of extreme values on both ends. The lowest observed growth rate (−9.2%) is a clear outlier, possibly representing a severe external shock. On the other end, a few quarters exceeded 10%, which may reflect one-off surges in economic activity.\nOutliers are identified using the 1.5 × IQR rule:\n\\[\n\\text{Lower bound} = Q_1 - 1.5 \\times \\text{IQR} = 2.55 - 1.5 \\times 4.05 = -3.525\n\\]\n\\[\n\\text{Upper bound} = Q_3 + 1.5 \\times \\text{IQR} = 6.6 + 1.5 \\times 4.05 = 12.675\n\\]\nVolatility and Economic Cycles: The plot confirms again that GDP growth values are widely spread, indicating higher volatility in the non-oil sector.\n\nfrom scipy.stats import gaussian_kde\n# Prepare ngdp_data\nngdp_data = ngdp[\"Non Oil GDP\"]\n\n# KDE estimation \nkde = gaussian_kde(ngdp_data)\nx_vals = np.linspace(min(ngdp_data), max(ngdp_data), 200)\ny_vals = kde(x_vals)\n\n# Create figure\nfig = go.Figure()\n\n# Add histogram \nfig.add_trace(go.Histogram(\n    x=ngdp_data,\n    nbinsx=15,\n    marker_color='red',\n    opacity=0.6,\n    name='Histogram (Count)'\n))\n\n# Scale KDE line to match histogram count scale\n# Multiply by bin width and total count to match scale\nbin_width = (max(ngdp_data) - min(ngdp_data)) / 15\nscaled_y_vals = y_vals * len(ngdp_data) * bin_width\n\n# Add KDE line (scaled)\nfig.add_trace(go.Scatter(\n    x=x_vals,\n    y=scaled_y_vals,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (scaled to count)'\n))\n\n# Update layout\nfig.update_layout(\n    title=\"Distribution of QoQ Non-Oil GDP Growth (%)\",\n    title_x=0.5,\n    template=\"plotly_dark\",\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    xaxis=dict(showgrid=False),\n    yaxis=dict(showgrid=False),\n    bargap=0.05,\n    width=1300,  \n    height=600\n)\n\nfig.show()\n\n                                                \n\n\n\nSummary Statistics: UAE Purchasing Managers Index (PMI)\nMonthly Index Levels\nThe average monthly PMI is about 54.76, with most readings between 53.8 and 56.6. A trough as low as 44.1 and a peak at 61.2 show occasional soft patches and strong expansion periods. The standard deviation of 2.74 indicates moderate month-to-month swings.\nDistribution Plot\nMost months cluster around 55, and the KDE curve tilts slightly to the left, those deeper dips below 50 are rarer but stretch the left tail.\nQuarterly Index Levels\nAt the quarterly frequency, the PMI still averages 54.76, with half of the quarters between 53.85 and 56.25. The softest quarter sits at 47.07, while the strongest hits 59.3. A lower standard deviation of 2.62 reflects that aggregating to quarters slightly smooths out some monthly noise.\nDistribution Plot\nQuarterly values remain centered near 55, and the KDE again shows a mild left skew as few softer quarters pull the tail on the downside.\n\npmi.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nPMI\n132.0\n54.759091\n2.74393\n44.1\n53.8\n55.1\n56.6\n61.2\n\n\n\n\n\n\n\n\npmi_quarterly.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nPMI\n44.0\n54.759091\n2.624363\n47.066667\n53.85\n55.1\n56.25\n59.3\n\n\n\n\n\n\n\n\n# Prepare datasets\npmi_data = pmi[\"PMI\"]\npmi_quarterly_data = pmi_quarterly[\"PMI\"]\n\n# Setup subplot layout\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=[\"PMI Monthly Index\", \"PMI Quarterly Index\"]\n)\n\n# Colors and common settings\ncolors = ['red', 'white']\nnbins = 15\n\n# --- Plot Original Data ---\nkde = gaussian_kde(pmi_data)\nx_vals = np.linspace(min(pmi_data), max(pmi_data), 200)\ny_vals = kde(x_vals)\nbin_width = (max(pmi_data) - min(pmi_data)) / nbins\nscaled_y_vals = y_vals * len(pmi_data) * bin_width\n\nfig.add_trace(go.Histogram(\n    x=pmi_data,\n    nbinsx=nbins,\n    marker_color='red',\n    opacity=0.6,\n    name='Original Histogram',\n    showlegend=False\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    x=x_vals,\n    y=scaled_y_vals,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (Original)',\n    showlegend=False\n), row=1, col=1)\n\n# --- Plot Log-Transformed Data ---\nkde_log = gaussian_kde(pmi_quarterly_data)\nx_vals_log = np.linspace(min(pmi_quarterly_data), max(pmi_quarterly_data), 200)\ny_vals_log = kde_log(x_vals_log)\nbin_width_log = (max(pmi_quarterly_data) - min(pmi_quarterly_data)) / nbins\nscaled_y_vals_log = y_vals_log * len(pmi_quarterly_data) * bin_width_log\n\nfig.add_trace(go.Histogram(\n    x=pmi_quarterly_data,\n    nbinsx=nbins,\n    marker_color='red',\n    opacity=0.6,\n    name='Log-Transformed Histogram',\n    showlegend=False\n), row=1, col=2)\n\nfig.add_trace(go.Scatter(\n    x=x_vals_log,\n    y=scaled_y_vals_log,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (Log-Transformed)',\n    showlegend=False\n), row=1, col=2)\n\n# Layout Styling\nfig.update_layout(\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    width=1300,\n    height=600,\n    bargap=0.05\n)\n\n# Hide gridlines\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False,zeroline=False)\n\nfig.show()\n\n                                                \n\n\n\n\nSummary Statistics: Dubai Residential Sales Price Index\nMonthly Index Levels\nThe average index level is about 1.28, with most values between 1.15 and 1.30. A few higher values (up to 1.68) point to occasional price surges in the housing market. The standard deviation of 0.16 indicates moderate month-to-month variability.\nDistribution Plot\nMost months cluster around the mean, and the KDE curve reveals a slight right tail as those price jumps are less frequent but noticeable.\nQuarterly Log Returns\nAverage quarterly return is small but positive (0.6%), suggesting a gentle upward trend overall. Returns are fairly symmetric around zero, with occasional spikes (~ 4%) and dips (~ –2%). A standard deviation of about 1.7% shows quarterly volatility exceeds month-to-month variability.\nDistribution Plot\nQuarterly changes mostly hover near zero, with a bell-shaped KDE and slightly heavy tails reflecting those rare big jumps or drops in residential prices.\n\nresidential.describe().T \n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nResidential Sales Index\n132.0\n1.277955\n0.161465\n1.07\n1.15525\n1.255\n1.29875\n1.684\n\n\n\n\n\n\n\n\nresidential_log.describe().T \n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nResidential Sales Index\n44.0\n0.006272\n0.016799\n-0.021142\n-0.00485\n0.001935\n0.019999\n0.042525\n\n\n\n\n\n\n\n\n# Prepare datasets\nresidential_data = residential[\"Residential Sales Index\"]\nresidential_log_data = residential_log[\"Residential Sales Index\"]\n\n# Setup subplot layout\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=[\"Residential Sales Index\", \"Quarterly Change\"]\n)\n\n# Colors and common settings\ncolors = ['red', 'white']\nnbins = 15\n\n# --- Plot Original Data ---\nkde = gaussian_kde(residential_data)\nx_vals = np.linspace(min(residential_data), max(residential_data), 200)\ny_vals = kde(x_vals)\nbin_width = (max(residential_data) - min(residential_data)) / nbins\nscaled_y_vals = y_vals * len(residential_data) * bin_width\n\nfig.add_trace(go.Histogram(\n    x=residential_data,\n    nbinsx=nbins,\n    marker_color='red',\n    opacity=0.6,\n    name='Original Histogram',\n    showlegend=False\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    x=x_vals,\n    y=scaled_y_vals,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (Original)',\n    showlegend=False\n), row=1, col=1)\n\n# --- Plot Log-Transformed Data ---\nkde_log = gaussian_kde(residential_log_data)\nx_vals_log = np.linspace(min(residential_log_data), max(residential_log_data), 200)\ny_vals_log = kde_log(x_vals_log)\nbin_width_log = (max(residential_log_data) - min(residential_log_data)) / nbins\nscaled_y_vals_log = y_vals_log * len(residential_log_data) * bin_width_log\n\nfig.add_trace(go.Histogram(\n    x=residential_log_data,\n    nbinsx=nbins,\n    marker_color='red',\n    opacity=0.6,\n    name='Log-Transformed Histogram',\n    showlegend=False\n), row=1, col=2)\n\nfig.add_trace(go.Scatter(\n    x=x_vals_log,\n    y=scaled_y_vals_log,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (Log-Transformed)',\n    showlegend=False\n), row=1, col=2)\n\n# Layout Styling\nfig.update_layout(\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    width=1300,\n    height=600,\n    bargap=0.05\n)\n\n# Hide gridlines\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False,zeroline=False)\n\nfig.show()\n\n                                                \n\n\n\n\nSummary Statistics: Monthly International Visitors to Dubai (In Millions)\nAs seen earlier in the time series plots, For the international visitors data, values dropped to almost zero during the COVID-19 period and here we can see that these extreme outliers resulted in unusually sharp changes when tourism rebounded post-COVID. Hence, I removed those periods from the tourism data.\nNote: For the other indicators, although they were affected by the economic downturn, the data still contained valuable information about underlying economic activity, so I chose to retain them.\n\nvisitors.describe().T # Figures are in millions\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nVisitors\n132.0\n1.164701\n0.42191\n0.0\n1.027895\n1.224328\n1.44064\n1.93\n\n\n\n\n\n\n\n\nvisitors_log.describe().T \n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nVisitors\n43.0\n0.009462\n3.088058\n-15.159777\n-0.132182\n0.063523\n0.155066\n12.940139\n\n\n\n\n\n\n\n\n# Prepare datasets\nvisitors_data = visitors[\"Visitors\"]\nvisitors_log_data = visitors_log[\"Visitors\"]\n\n# Setup subplot layout\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=[\"Dubai International visitors\", \"Quarterly Change\"]\n)\n\n# Colors and common settings\ncolors = ['red', 'white']\nnbins = 15\n\n# --- Plot Original Data ---\nkde = gaussian_kde(visitors_data)\nx_vals = np.linspace(min(visitors_data), max(visitors_data), 200)\ny_vals = kde(x_vals)\nbin_width = (max(visitors_data) - min(visitors_data)) / nbins\nscaled_y_vals = y_vals * len(visitors_data) * bin_width\n\nfig.add_trace(go.Histogram(\n    x=visitors_data,\n    nbinsx=nbins,\n    marker_color='red',\n    opacity=0.6,\n    name='Original Histogram',\n    showlegend=False\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    x=x_vals,\n    y=scaled_y_vals,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (Original)',\n    showlegend=False\n), row=1, col=1)\n\n# --- Plot Log-Transformed Data ---\nkde_log = gaussian_kde(visitors_log_data)\nx_vals_log = np.linspace(min(visitors_log_data), max(visitors_log_data), 200)\ny_vals_log = kde_log(x_vals_log)\nbin_width_log = (max(visitors_log_data) - min(visitors_log_data)) / nbins\nscaled_y_vals_log = y_vals_log * len(visitors_log_data) * bin_width_log\n\nfig.add_trace(go.Histogram(\n    x=visitors_log_data,\n    nbinsx=nbins,\n    marker_color='red',\n    opacity=0.6,\n    name='Log-Transformed Histogram',\n    showlegend=False\n), row=1, col=2)\n\nfig.add_trace(go.Scatter(\n    x=x_vals_log,\n    y=scaled_y_vals_log,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (Log-Transformed)',\n    showlegend=False\n), row=1, col=2)\n\n# Layout Styling\nfig.update_layout(\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    width=1300,\n    height=600,\n    bargap=0.05\n)\n\n# Hide gridlines\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False,zeroline=False)\n\nfig.show()\n\n                                                \n\n\n\n# Copy original data to avoid modifying the original\nvisitors_covid_excluded = visitors.copy()\nvisitors_log_covid_excluded = visitors_log.copy()\n\n# Drop the specific outliers\nvisitors_covid_excluded = visitors_covid_excluded.drop(['2020-04','2020-05','2020-06','2020-07'])\nvisitors_log_covid_excluded = visitors_log_covid_excluded.drop(['2020-06', '2020-09'])\n\nQuarterly Totals (Millions)\nThe average quarterly arrivals are about 1.20 m, with most values between 1.06 m and 1.45 m. The busiest quarter reaches 1.93 m, while the slowest dips to 0.13 m. With a standard deviation of 0.37 m, we see moderate swings, reflecting peak tourist seasons and quieter months.\nDistribution Plot\nThe distribution shows a slight left skew, since those unusually low‐inflow quarters (off-peak or residual COVID effects) are less common but pull the left tail. Most quarters cluster around 1.2 m, matching typical demand. The KDE curve’s peak sits near the median, and its gentle lean to the left highlights those few low-visitor outliers against an otherwise consistent inflow.\nQuarterly Log Returns\nAverage quarterly growth is 6.4%, indicating healthy tourism recovery trends. Half the changes lie between –11.9% and 14.5%, but volatility is high (std ≈ 27.8%), while a few quarters see dramatic rebounds (up to 110%)\nDistribution Plot\nThe change mostly hover near zero, showing small quarter-to-quarter changes are typical. The KDE curve is clearly right-skewed: modest growth rates dominate, but occasional sharp rebounds stretch out the right tail even when covid years were excluded.\n\nvisitors_covid_excluded.describe().T# Figures are in millions\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nVisitors\n128.0\n1.200785\n0.374566\n0.134203\n1.063183\n1.231606\n1.4476\n1.93\n\n\n\n\n\n\n\n\nvisitors_log_covid_excluded.describe().T \n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nVisitors\n41.0\n0.064061\n0.277754\n-0.299372\n-0.119338\n0.063523\n0.145466\n1.101037\n\n\n\n\n\n\n\n\n# Prepare datasets\nvisitors_covid_excluded = visitors_covid_excluded[\"Visitors\"]\nvisitors_log_covid_excluded = visitors_log_covid_excluded[\"Visitors\"]\n\n# Setup subplot layout\nfig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=[\"Dubai International visitors\", \"Quarterly Chaneg\"]\n)\n\n# Colors and common settings\ncolors = ['red', 'white']\nnbins = 15\n\n# --- Plot Original Data ---\nkde = gaussian_kde(visitors_covid_excluded)\nx_vals = np.linspace(min(visitors_covid_excluded), max(visitors_covid_excluded), 200)\ny_vals = kde(x_vals)\nbin_width = (max(visitors_covid_excluded) - min(visitors_covid_excluded)) / nbins\nscaled_y_vals = y_vals * len(visitors_covid_excluded) * bin_width\n\nfig.add_trace(go.Histogram(\n    x=visitors_covid_excluded,\n    nbinsx=nbins,\n    marker_color='red',\n    opacity=0.6,\n    name='Original Histogram',\n    showlegend=False\n), row=1, col=1)\n\nfig.add_trace(go.Scatter(\n    x=x_vals,\n    y=scaled_y_vals,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (Original)',\n    showlegend=False\n), row=1, col=1)\n\n# --- Plot Log-Transformed Data ---\nkde_log = gaussian_kde(visitors_log_covid_excluded)\nx_vals_log = np.linspace(min(visitors_log_covid_excluded), max(visitors_log_covid_excluded), 200)\ny_vals_log = kde_log(x_vals_log)\nbin_width_log = (max(visitors_log_covid_excluded) - min(visitors_log_covid_excluded)) / nbins\nscaled_y_vals_log = y_vals_log * len(visitors_log_covid_excluded) * bin_width_log\n\nfig.add_trace(go.Histogram(\n    x=visitors_log_covid_excluded,\n    nbinsx=nbins,\n    marker_color='red',\n    opacity=0.6,\n    name='Log-Transformed Histogram',\n    showlegend=False\n), row=1, col=2)\n\nfig.add_trace(go.Scatter(\n    x=x_vals_log,\n    y=scaled_y_vals_log,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE (Log-Transformed)',\n    showlegend=False\n), row=1, col=2)\n\n# Layout Styling\nfig.update_layout(\n    title= 'Excluding COVID Period',\n   title_x=0.5,    \n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    width=1300,\n    height=600,\n    bargap=0.05\n)\n\n# Hide gridlines\nfig.update_xaxes(showgrid=False)\nfig.update_yaxes(showgrid=False,zeroline=False)\n\nfig.show()\n\n                                                \n\n\n\n\nSummary Statistics: Personal Lending Index\nKey Insights\nThe average net-balance reading is about 14.7%, with half of respondents falling between 7.2% and 23.9%. A low of –9% shows a few officers expect a pullback, while a high of 35.3% signals strong optimism. The standard deviation of 10.9% points to moderate quarter-to-quarter swings in sentiment.\nDistribution Plot\nThe distribution is right-skewed, with most readings between 5 % and 25 % and a long tail toward higher values—occasional strong optimism spikes stretch the right side. The KDE curve also tilts right, reinforcing that while big positive jumps are rarer, they pull the tail out on the high end.\n\npersonal.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nPersonal Lending Demand\n44.0\n14.69847\n10.912432\n-8.988764\n7.201666\n13.868826\n23.882286\n35.267857\n\n\n\n\n\n\n\n\n# Prepare personal_data\npersonal_data = personal[\"Personal Lending Demand\"]\n\n# KDE estimation \nkde = gaussian_kde(personal_data)\nx_vals = np.linspace(min(personal_data), max(personal_data), 200)\ny_vals = kde(x_vals)\n\n# Create figure\nfig = go.Figure()\n\n# Add histogram \nfig.add_trace(go.Histogram(\n    x=personal_data,\n    nbinsx=15,\n    marker_color='red',\n    opacity=0.6,\n    name='Histogram'\n))\n\n# Scale KDE line to match histogram count scale\n# Multiply by bin width and total count to match scale\nbin_width = (max(personal_data) - min(personal_data)) / 15\nscaled_y_vals = y_vals * len(personal_data) * bin_width\n\n# Add KDE line (scaled)\nfig.add_trace(go.Scatter(\n    x=x_vals,\n    y=scaled_y_vals,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE'\n))\n\n# Update layout\nfig.update_layout(\n    template=\"plotly_dark\",\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    xaxis=dict(showgrid=False),\n    yaxis=dict(showgrid=False),\n    xaxis_title=\"Personal Lending Demand Index (%)\",\n    yaxis_title=\"Frequency\",    \n    bargap=0.05,\n    width=1300,  \n    height=600\n)\n\nfig.show()\n\n                                                \n\n\n\n\nSummary Statistics: Business Lending Index\nThe average net‐balance reading sits at 21.9%, with half of the quarters between 15.0% and 28.6%. Sentiment never falls below 2.2%, and peaks at 43.6%, showing occasional bursts of strong business lending optimism. A standard deviation of 9.17% indicates moderate quarter-to-quarter swings.\nDistribution Plot\nThe histogram is right-skewed, with most values clustering in the 15–30% range and a long tail toward higher readings. The KDE curve also leans right, highlighting that while very high optimism quarters are less common, they pull the tail out on the high end.\n\nbusiness.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nBusiness Lending Demand\n44.0\n21.905948\n9.174431\n2.173913\n15.038371\n22.488558\n28.613401\n43.589744\n\n\n\n\n\n\n\n\n# Prepare business_data\nbusiness_data = business[\"Business Lending Demand\"]\n\n# KDE estimation \nkde = gaussian_kde(business_data)\nx_vals = np.linspace(min(business_data), max(business_data), 200)\ny_vals = kde(x_vals)\n\n# Create figure\nfig = go.Figure()\n\n# Add histogram \nfig.add_trace(go.Histogram(\n    x=business_data,\n    nbinsx=15,\n    marker_color='red',\n    opacity=0.6,\n    name='Histogram'\n))\n\n# Scale KDE line to match histogram count scale\n# Multiply by bin width and total count to match scale\nbin_width = (max(business_data) - min(business_data)) / 15\nscaled_y_vals = y_vals * len(business_data) * bin_width\n\n# Add KDE line (scaled)\nfig.add_trace(go.Scatter(\n    x=x_vals,\n    y=scaled_y_vals,\n    mode='lines',\n    line=dict(color='white', width=2),\n    name='KDE'\n))\n\n# Update layout\nfig.update_layout(\n    template=\"plotly_dark\",\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    xaxis=dict(showgrid=False),\n    yaxis=dict(showgrid=False),\n    xaxis_title=\"Business Lending Demand Index (%)\",\n    yaxis_title=\"Frequency\",      \n    bargap=0.05,\n    width=1300,  \n    height=600\n)\n\nfig.show()"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#correlation-analysis",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#correlation-analysis",
    "title": "Introduction",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nTo assess the predictive potential of each indicator, I conducted a lead-lag correlation analysis between the independent variables and future values of non-oil GDP growth. I shifted the dependent variable (non-oil GDP) forward by up to four quarters, and then computed its correlation with the current values of each predictor.\nThe goal was to identify whether changes in indicators such as lending, PMI, or real estate activity are associated with GDP growth one or more quarters ahead. For example, if real estate prices rises today, its effect on output may take time to materialize so a strong correlation with GDP in later quarters (e.g., Q+1 or Q+2) may suggest its usefulness as a short-term leading indicator.\nThis methodology is consistent with macroeconomic forecasting literature, where multi-quarter lags are often utilized to capture delayed effects and dynamic relationships between variables (Hann et al., 2017)\nTo ensure accurate correlation analysis, all independent variables were joined with the lagged versions of non-oil GDP using their shared quarterly date index.\nSince some series have different starting points and may include missing values (especially after applying time shifts), we applied .dropna() to remove any rows containing NaN values. This ensures that the correlation coefficients are computed over consistent and complete samples across all lag levels.\nThe resulting datasets include: - ngdp_df_lag1 (GDP shifted one quarter ahead) - ngdp_df_lag2, ngdp_df_lag3, ngdp_df_lag4 (up to four quarters ahead)\n\n# Create leaded versions of GDP by shifting it backward (-) in time\nngdp_lag1 = ngdp.shift(-1)  # GDP one quarter ahead\nngdp_lag2 = ngdp.shift(-2)  # GDP two quarters ahead\nngdp_lag3 = ngdp.shift(-3)  # GDP three quarters ahead\nngdp_lag4 = ngdp.shift(-4)  # GDP four quarters ahead\n\n\n# Base list of predictors\npredictors = [\n    residential_log,\n    visitors_log,\n    personal,\n    business,\n    pmi_quarterly\n]\n\n# Join with current GDP\nngdp_df = ngdp.join(predictors).dropna()\n\n# Join with lagged GDPs\nngdp_df_lag1 = ngdp_lag1.join(predictors).dropna()\nngdp_df_lag2 = ngdp_lag2.join(predictors).dropna()\nngdp_df_lag3 = ngdp_lag3.join(predictors).dropna()\nngdp_df_lag4 = ngdp_lag4.join(predictors).dropna()\n\n\nCorrelation Results\nAfter aligning and cleaning the datasets, I calculated the Pearson correlation between each independent variable and the lagged versions of non-oil GDP (from 1 to 4 quarters ahead).\nThe analysis showed that most indicators had their strongest relationship with non-oil GDP at a one-quarter lead, while correlations at longer lags were generally weak. As it seems to point out that changes in business sentiment, lending activity, and property markets has an effect within the next quarter, while their impact tends to fade over longer horizons probably due to other intervening economic factors.\nPersonal Lending Index, Business Lending Index, Residential Sales Price Index, and the UAE Manufacturing PMI all showed moderate positive correlations around 0.55 to 0.6 at lag 1, while the International Visitor showed weak correlations across all lag periods\n\n# Compute correlation matrices\ncorr = ngdp_df.corr()\ncorr_lag1 = ngdp_df_lag1.corr()\ncorr_lag2 = ngdp_df_lag2.corr()\ncorr_lag3 = ngdp_df_lag3.corr()\ncorr_lag4 = ngdp_df_lag4.corr()\n\n# Display sorted correlations with future GDP\nfor i, c in enumerate([corr_lag1, corr_lag2, corr_lag3, corr_lag4], start=1):\n    print(f\"\\nLag {i} Correlations with Future Non-Oil GDP:\")\n    print(c['Non Oil GDP'].sort_values())\n\n\nLag 1 Correlations with Future Non-Oil GDP:\nVisitors                   0.043703\nPMI                        0.515123\nResidential Sales Index    0.557546\nPersonal Lending Demand    0.563303\nBusiness Lending Demand    0.591599\nNon Oil GDP                1.000000\nName: Non Oil GDP, dtype: float64\n\nLag 2 Correlations with Future Non-Oil GDP:\nVisitors                   0.158510\nResidential Sales Index    0.393571\nPMI                        0.401689\nPersonal Lending Demand    0.476747\nBusiness Lending Demand    0.509697\nNon Oil GDP                1.000000\nName: Non Oil GDP, dtype: float64\n\nLag 3 Correlations with Future Non-Oil GDP:\nPMI                        0.199294\nResidential Sales Index    0.275676\nVisitors                   0.352320\nPersonal Lending Demand    0.377320\nBusiness Lending Demand    0.409587\nNon Oil GDP                1.000000\nName: Non Oil GDP, dtype: float64\n\nLag 4 Correlations with Future Non-Oil GDP:\nPMI                       -0.174258\nVisitors                  -0.087400\nPersonal Lending Demand    0.162761\nBusiness Lending Demand    0.224950\nResidential Sales Index    0.280447\nNon Oil GDP                1.000000\nName: Non Oil GDP, dtype: float64\n\n\n\nfig = px.imshow(corr_lag1, color_continuous_scale='inferno')\n\nfig.update_layout(\n    width=800, \n    height=600, \n    template=\"plotly_dark\", \n    title=\"The Correlation Coefficient Of the Variables: Lag 1\",\n    title_x=0.5  \n)\nfig.show()"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#multiple-linear-regression-ols",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#multiple-linear-regression-ols",
    "title": "Introduction",
    "section": "Multiple Linear Regression (OLS)",
    "text": "Multiple Linear Regression (OLS)\nTo evaluate the joint impact of multiple localized macroeconomic indicators on non-oil GDP growth, I estimated a Multiple Linear Regression model using data from the Lag 1 setup, where the independent variables are current, and the dependent variable is GDP one quarter ahead as this setup showed the strongest relationships.\nThe model was estimated using the Ordinary Least Squares (OLS) method. OLS is a linear estimation technique that minimizes the sum of squared differences between observed and predicted values of the dependent variable. It’s a widely used approach in econometric studies (Wooldridge, 2013).\nI’ve used this model before in a Bitcoin-USD Price Prediction Study, where I explored how indicators like moving averages, trading volume, and external factors such as gold prices if they could help predict Bitcoin prices.\n\nModel Results\n\\[\n\\text{NonOilGDP}_{t+1} = \\beta_0 + \\beta_1 \\cdot \\text{Residential}_{t} + \\beta_2 \\cdot \\text{PersonalLending}_{t} + \\beta_3 \\cdot \\text{BusinessLending}_{t} + \\beta_4 \\cdot \\text{PMI}_{t} + \\beta_5 \\cdot \\text{Visitors}_{t} + \\varepsilon_t\n\\]\n\\[\n\\text{NonOilGDP}_{t+1} = -28.48 + 43.79 \\cdot \\text{Residential}_{t} + 0.13 \\cdot \\text{PersonalLending}_{t} + 0.04 \\cdot \\text{BusinessLending}_{t} + 0.54 \\cdot \\text{PMI}_{t} - 0.11 \\cdot \\text{Visitors}_{t} + \\varepsilon_t\n\\]\n\n# Import regression and evaluation tools\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Define predictors and target from Lag 1 dataset\nX_train = ngdp_df_lag1[['Residential Sales Index',\n                        'Personal Lending Demand',\n                        'Business Lending Demand',\n                        'PMI',\n                        'Visitors']]  # Independent variables\n\ny_train = ngdp_df_lag1['Non Oil GDP']  # Dependent variable (1 quarter ahead)\n\n# Add a constant (intercept term) to the regression model\nX_train = sm.add_constant(X_train)\n\n# Fit the OLS model\nols_model = sm.OLS(y_train, X_train).fit()\n\n# Display model summary\nols_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nNon Oil GDP\nR-squared:\n0.484\n\n\nModel:\nOLS\nAdj. R-squared:\n0.408\n\n\nMethod:\nLeast Squares\nF-statistic:\n6.383\n\n\nDate:\nThu, 15 May 2025\nProb (F-statistic):\n0.000279\n\n\nTime:\n15:08:39\nLog-Likelihood:\n-102.15\n\n\nNo. Observations:\n40\nAIC:\n216.3\n\n\nDf Residuals:\n34\nBIC:\n226.4\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-28.4825\n12.153\n-2.344\n0.025\n-53.181\n-3.784\n\n\nResidential Sales Index\n43.7870\n49.551\n0.884\n0.383\n-56.913\n144.487\n\n\nPersonal Lending Demand\n0.1268\n0.086\n1.473\n0.150\n-0.048\n0.302\n\n\nBusiness Lending Demand\n0.0433\n0.122\n0.355\n0.725\n-0.205\n0.292\n\n\nPMI\n0.5400\n0.233\n2.321\n0.026\n0.067\n1.013\n\n\nVisitors\n-0.1070\n0.182\n-0.587\n0.561\n-0.478\n0.263\n\n\n\n\n\n\n\n\nOmnibus:\n1.417\nDurbin-Watson:\n0.859\n\n\nProb(Omnibus):\n0.492\nJarque-Bera (JB):\n1.381\n\n\nSkew:\n0.399\nProb(JB):\n0.501\n\n\nKurtosis:\n2.563\nCond. No.\n5.62e+03\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 5.62e+03. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\nKey Insights:\n\nUAE Manufacturing PMI was statistically significant at the 5% level (p = 0.026), with a positive coefficient (0.540). This suggests that stronger business sentiment is associated with higher non-oil GDP growth in the subsequent quarter.\nResidential Sales Price Index, Personal Lending Demand, and Business Lending Demand were not statistically significant (p-values &gt; 0.10), although their positive coefficients are directionally consistent with economic expectations.\nMonthly Visitor Numbers had a small negative coefficient (-0.107) and was statistically insignificant (p = 0.561), suggesting it does not meaningfully contribute to short-term GDP prediction at Lag 1.\n\nModel Diagnostics:\n\nThe F-statistic = 6.383 (p &lt; 0.001) confirms that the independent variables are jointly significant predictors of non-oil GDP growth.\nResidual normality is supported by the Jarque-Bera test (p = 0.501), indicating an approximately normal distribution.\nThe Durbin-Watson statistic = 0.859 points to positive autocorrelation in the residuals, which may violate the OLS assumption of independence. Which also justifies the use of the VAR model in the coming sections since it accounts for autocorrelation and models how variables interact over time\n\n\n\nModel Evaluation\nTo assess the predictive performance of the OLS model, I evaluated its in-sample predictions using four standard regression metrics:\n\nR-squared (R²) = 0.48: The model explains approximately 48% of the variance in non-oil GDP growth, indicating a moderate fit.\nMean Absolute Error (MAE) = 2.54: On average, the model’s predictions differ from actual values by about 2.54 percentage points.\nRoot Mean Squared Error (RMSE) = 3.11: The RMSE penalizes larger errors more heavily than MAE and reflects the standard deviation of residuals.\nMean Absolute Percentage Error (MAPE) = 124.5%: This high value suggests the model performs poorly for smaller or near-zero GDP growth rates, as percentage errors become exaggerated. MAPE can be misleading when actual values are close to zero.\n\nOverall, while the model captures general trends (as seen in the R²), its relatively high MAPE and RMSE highlight potential limitations in forecasting precision — especially during volatile quarters. This supports the case for exploring dynamic models like VAR, which can could probably better handle time dependencies.\n\n# Predict using the fitted OLS model on training data\nols_predictions = ols_model.predict(X_train)\n\n# Evaluate model performance using standard metrics\nols_mae = mean_absolute_error(y_train, ols_predictions)  # Average magnitude of errors\nols_rmse = mean_squared_error(y_train, ols_predictions, squared=False)  # Penalizes larger errors\nols_mape = (np.abs((y_train - ols_predictions) / y_train).mean()) * 100  # Relative percentage error\nols_r2 = ols_model.rsquared  # Proportion of variance explained by the model\n\nprint(f\"Mean Absolute Error (MAE): {ols_mae:.2f}\")\nprint(f\"Root Mean Squared Error (RMSE): {ols_rmse:.2f}\")\nprint(f\"Mean Absolute Percentage Error (MAPE): {ols_mape:.1f}%\")\nprint(f\"R-squared (R²): {ols_r2:.3f}\")\n\n\nMean Absolute Error (MAE): 2.54\nRoot Mean Squared Error (RMSE): 3.11\nMean Absolute Percentage Error (MAPE): 124.5%\nR-squared (R²): 0.484\n\n\nFrom the chart below, we can see that the model generally follows the same trend as the actual GDP growth, it gets the overall direction right most of the time.\nHowever, it misses the big drop and rebound in 2020, likely due to the unexpected impact of COVID-19. The model also tends to smooth out sharp changes, so it doesn’t fully capture sudden spikes or dips.\nThat said, during more stable periods (like 2016–2019), the predictions are closer to the actual values, showing the model performs better when the economy is more steady.\n\n# If needed, generate quarterly dates\ndate_index = pd.date_range(start='2014-01-01', periods=len(y_train), freq='Q')\n\n# Create figure\nfig = go.Figure()\n\n# Actual GDP\nfig.add_trace(go.Scatter(\n    x=date_index, y=y_train,\n    mode='lines+markers',\n    name='Actual GDP Growth',\n    line=dict(color='#FF4136', width=2),  # Bright red\n    marker=dict(color='#FF4136', size=5)\n))\n\n# Predicted GDP\nfig.add_trace(go.Scatter(\n    x=date_index, y=ols_predictions,\n    mode='lines+markers',\n    name='Predicted GDP Growth (OLS)',\n    line=dict(color='#0074D9', width=2),  # Bright blue\n    marker=dict(color='#0074D9', size=5)\n))\n\n# Shaded error area\nfig.add_trace(go.Scatter(\n    x=np.concatenate([date_index, date_index[::-1]]),\n    y=np.concatenate([y_train, ols_predictions[::-1]]),\n    fill='toself',\n    fillcolor='rgba(173, 216, 230, 0.3)',  # LightBlue with transparency\n    line=dict(color='rgba(255,255,255,0)'),\n    hoverinfo='skip',\n    name='Prediction Error',\n    showlegend=True,\n))\n\n# Layout styling (black background, white text)\nfig.update_layout(\n    height=500, width=900,\n    margin=dict(l=50, r=40, t=40, b=40),\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    xaxis_title='Date',\n    yaxis_title='QoQ Non-Oil GDP Growth (%)',\n    legend=dict(orientation='h', y=1.05, x=0.5, xanchor='center', yanchor='bottom')\n)\n\n# Axes\nfig.update_yaxes(showgrid=False, zeroline=False, gridcolor='lightgray', dtick=2)\nfig.update_xaxes(showgrid=False)\n\nfig.show()"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#stationarity-check",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#stationarity-check",
    "title": "Introduction",
    "section": "Stationarity Check",
    "text": "Stationarity Check\nIn the correlation and OLS regression analysis helped me uncover static relationships between GDP and other indicators. But from the results, it was clear that these methods don’t fully capture the dynamic and interconnected nature of macroeconomic variables over time. To add to that there was also presence of autocorrelation in the residuals.\nTo address this, I moved to a Vector Autoregression (VAR) model. Unlike OLS, VAR can account for lagged interactions between multiple variables and can also generate impulse response functions which very useful in measuring the economic shocks.\nBefore applying VAR or running Granger causality tests, it was important to check whether the variables were stationary, meaning their statistical properties (like mean and variance) don’t change over time. If they weren’t, I had to difference them to stabilize the series.\nThese steps are standard in macroeconomic modeling and are commonly used in analyses including those involving GCC economies, as shown in studies by Bentour & Fund (2022), Magazzino (2016), and Kireyev (2000)\nInitially, some variables such as PMI, residential prices, and international visitors were transformed when I retrieved them in the very first step. These transformations typically help stabilize the mean and variance over time and could potentially have made them stationary.\nHowever, to formally assess this, we can apply statistical tests such as the Augmented Dickey-Fuller (ADF) test, which helps determine the presence of a unit root and whether a series is stationary.\nAt its core, the ADF test checks whether the data has a kind of “memory.”\nThink of it like this: imagine a drunk man walking. If he stumbles randomly in any direction with no tendency to return to where he started, he’ll likely drift further and further away over time — just like a non-stationary series that doesn’t settle around a stable average. But if there’s a rope tied to his waist pulling him gently back toward a lamppost, he’ll wander, but not too far — this is like a stationary series that fluctuates around a long-term mean.\nThe ADF test essentially asks: Is there a “pull-back” force in the data, or is it just wandering off forever?\n(Credit to ChatGPT for making statistics feel like storytelling.)\n\nStationarity Test Results\nUsing the Augmented Dickey-Fuller (ADF) test, I found that: - Personal Lending, Business Lending, and Visitor Inflows were already stationary (p-values &lt; 0.05), so they didn’t require any transformation. - Non-Oil GDP, PMI, and Residential Sales Index were non-stationary in their level form.\n\npmi_quarterly.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 44 entries, 2014-03 to 2024-12\nFreq: M\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   PMI     44 non-null     float64\ndtypes: float64(1)\nmemory usage: 1.7 KB\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n# Created a dictionary mapping names the DataFrames\ndataframes = {\n    \"Non Oil GDP\": ngdp,\n    \"PMI\": pmi_quarterly,\n    \"Personal Lending\": personal,\n    \"Business Lending\": business,\n    \"Residential Sales Index\": residential_log,\n    \"Monthly Visitors\": visitors_log,        \n}\n\ndef adf_test(series, signif=0.05, name=''):\n    # Remove missing values\n    series = series.dropna()\n    \n    print(f\"ADF Test for {name}:\")\n    # Perform ADF test with automatic lag selection via AIC\n    result = adfuller(series, autolag='AIC')\n    \n    # Extract test statistic, p-value, and critical values\n    test_statistic, p_value, usedlag, n_obs, crit_values, icbest = result\n    \n    print(f\"  Test Statistic   : {test_statistic:.4f}\")\n    print(f\"  p-value          : {p_value:.4f}\")\n    print(f\"  # Lags Used     : {usedlag}\")\n    print(f\"  # Observations   : {n_obs}\")\n    \n    for key, value in crit_values.items():\n        print(f\"  Critical Value ({key}) : {value:.4f}\")\n    \n    # Interpretation: if p-value is less than significance, we reject H0\n    if p_value &lt;= signif:\n        print(\"  =&gt; Reject the null hypothesis. The series is stationary.\")\n    else:\n        print(\"  =&gt; Fail to reject the null hypothesis. The series is non-stationary.\")\n    print(\"-\" * 60)\n    \n# Loop through each series in the dictionary and apply the ADF test.\nfor name, df in dataframes.items():\n    # Assuming the first (and only) column is your time series data.\n    adf_test(df.iloc[:, 0], name=name)\n\nADF Test for Non Oil GDP:\n  Test Statistic   : -1.6482\n  p-value          : 0.4580\n  # Lags Used     : 4\n  # Observations   : 37\n  Critical Value (1%) : -3.6209\n  Critical Value (5%) : -2.9435\n  Critical Value (10%) : -2.6104\n  =&gt; Fail to reject the null hypothesis. The series is non-stationary.\n------------------------------------------------------------\nADF Test for PMI:\n  Test Statistic   : -2.1896\n  p-value          : 0.2100\n  # Lags Used     : 0\n  # Observations   : 43\n  Critical Value (1%) : -3.5925\n  Critical Value (5%) : -2.9315\n  Critical Value (10%) : -2.6041\n  =&gt; Fail to reject the null hypothesis. The series is non-stationary.\n------------------------------------------------------------\nADF Test for Personal Lending:\n  Test Statistic   : -3.2917\n  p-value          : 0.0153\n  # Lags Used     : 0\n  # Observations   : 43\n  Critical Value (1%) : -3.5925\n  Critical Value (5%) : -2.9315\n  Critical Value (10%) : -2.6041\n  =&gt; Reject the null hypothesis. The series is stationary.\n------------------------------------------------------------\nADF Test for Business Lending:\n  Test Statistic   : -2.9727\n  p-value          : 0.0375\n  # Lags Used     : 0\n  # Observations   : 43\n  Critical Value (1%) : -3.5925\n  Critical Value (5%) : -2.9315\n  Critical Value (10%) : -2.6041\n  =&gt; Reject the null hypothesis. The series is stationary.\n------------------------------------------------------------\nADF Test for Residential Sales Index:\n  Test Statistic   : -1.4082\n  p-value          : 0.5783\n  # Lags Used     : 4\n  # Observations   : 39\n  Critical Value (1%) : -3.6104\n  Critical Value (5%) : -2.9391\n  Critical Value (10%) : -2.6081\n  =&gt; Fail to reject the null hypothesis. The series is non-stationary.\n------------------------------------------------------------\nADF Test for Monthly Visitors:\n  Test Statistic   : -7.3270\n  p-value          : 0.0000\n  # Lags Used     : 1\n  # Observations   : 41\n  Critical Value (1%) : -3.6010\n  Critical Value (5%) : -2.9351\n  Critical Value (10%) : -2.6060\n  =&gt; Reject the null hypothesis. The series is stationary.\n------------------------------------------------------------\n\n\nTo determine how many times to difference these non-stationary series, I used the .ndiffs() function from the pmdarima package. This function estimates the minimum number of differences needed to make a series stationary, based on statistical unit root tests.\n\nimport pmdarima as pm\n\n# Create a dictionary for the series that need to be differenced\ndatasets_to_diff = {\n    \"Non-Oil GDP\": ngdp,\n   \"PMI\": pmi_quarterly,\n    \"Residential Sales Index\": residential_log,\n}\n\n# Loop through each dataset and determine the required number of differences\nfor name, df in datasets_to_diff.items():\n    # Assuming the series is in the first column\n    series = df.iloc[:, 0].dropna()\n    d = pm.arima.ndiffs(series, test='adf')  # 'adf' is the default test\n    print(f\"{name} requires {d} difference(s) to achieve stationarity based on the ADF test.\")\n\nNon-Oil GDP requires 1 difference(s) to achieve stationarity based on the ADF test.\nPMI requires 1 difference(s) to achieve stationarity based on the ADF test.\nResidential Sales Index requires 1 difference(s) to achieve stationarity based on the ADF test.\n\n\nBased on this method, a first order difference was found to be sufficient for the three nonstationary data.\nAfter applying the differencing, I reran the ADF test and confirmed that the series were now stationary.\n\n# Created a dictionary mapping the series name to the number of differences needed.\ndiff_order = {\n    \"Non-Oil GDP\": 1,\n    \"Residential Sales Index\": 1,\n    \"PMI\": 1,    \n}\n\n# Function to apply differencing d times on a Pandas Series\ndef difference_series(series, d):\n    diff_series = series.copy()\n    for i in range(d):\n        diff_series = diff_series.diff().dropna()\n    return diff_series\n\n# Dictionary to store the differenced series\ndifferenced_series = {}\n\n# Apply differencing based on the required order for each dataset\nfor name, df in datasets_to_diff.items():\n    # Extract the series from the first column and drop missing values\n    series = df.iloc[:, 0].dropna()\n    d = diff_order[name]\n    if d &gt; 0:\n        differenced_series[name] = difference_series(series, d)\n    else:\n        # If no differencing is needed, store the original series\n        differenced_series[name] = series\n\n\n\n\n# Function to run the ADF test and print results like before\ndef adf_test(series, signif=0.05, name=''):\n    series = series.dropna()\n    result = adfuller(series, autolag='AIC')\n    test_statistic, p_value, usedlag, n_obs, crit_values, icbest = result\n    \n    print(f\"ADF Test for {name}:\")\n    print(f\"  Test Statistic   : {test_statistic:.4f}\")\n    print(f\"  p-value          : {p_value:.4f}\")\n    print(f\"  # Lags Used      : {usedlag}\")\n    print(f\"  # Observations   : {n_obs}\")\n    for key, value in crit_values.items():\n        print(f\"  Critical Value ({key}) : {value:.4f}\")\n    if p_value &lt;= signif:\n        print(\"  =&gt; Reject the null hypothesis. The series is stationary.\")\n    else:\n        print(\"  =&gt; Fail to reject the null hypothesis. The series is non-stationary.\")\n    print(\"-\" * 60)\n\n# Run the ADF test on each differenced series\nfor name, series in differenced_series.items():\n    adf_test(series, name=name)\n\nADF Test for Non-Oil GDP:\n  Test Statistic   : -5.4970\n  p-value          : 0.0000\n  # Lags Used      : 3\n  # Observations   : 37\n  Critical Value (1%) : -3.6209\n  Critical Value (5%) : -2.9435\n  Critical Value (10%) : -2.6104\n  =&gt; Reject the null hypothesis. The series is stationary.\n------------------------------------------------------------\nADF Test for PMI:\n  Test Statistic   : -6.5476\n  p-value          : 0.0000\n  # Lags Used      : 0\n  # Observations   : 42\n  Critical Value (1%) : -3.5966\n  Critical Value (5%) : -2.9333\n  Critical Value (10%) : -2.6050\n  =&gt; Reject the null hypothesis. The series is stationary.\n------------------------------------------------------------\nADF Test for Residential Sales Index:\n  Test Statistic   : -5.0959\n  p-value          : 0.0000\n  # Lags Used      : 3\n  # Observations   : 39\n  Critical Value (1%) : -3.6104\n  Critical Value (5%) : -2.9391\n  Critical Value (10%) : -2.6081\n  =&gt; Reject the null hypothesis. The series is stationary.\n------------------------------------------------------------\n\n\n\n# Renaming the new differenced variables\nngdp_diff = differenced_series[\"Non-Oil GDP\"].to_frame(name=\"Non-Oil GDP\")\nresidential_diff = differenced_series[\"Residential Sales Index\"].to_frame(name=\"Residential Sales Index\")\npmi_diff = differenced_series[\"PMI\"].to_frame(name=\"PMI\")"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#granger-causality-analysis",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#granger-causality-analysis",
    "title": "Introduction",
    "section": "Granger Causality Analysis",
    "text": "Granger Causality Analysis\nAfter ensuring stationarity, I used the Granger causality test, a statistical method that determines whether past values of an independent variable help predict future values of a dependent variable beyond what is already explained by the dependent variable’s own past values (Granger, 1969).\nUsing an analogy again,\n\nImagine you’re trying to predict whether people will carry umbrellas tomorrow.\n\nYou have two pieces of historical information:\n\nPast Weather Data – Did it rain recently?\n\nPast Umbrella Usage – Did people carry umbrellas recently?\n\n\nIf knowing the history of rainfall improves your prediction of whether people will carry umbrellas tomorrow — beyond just looking at past umbrella usage — then we say that rain “Granger-causes” umbrella usage.\nIt’s important to note that Granger causality isn’t about real-world causation. It simply tests whether one variable’s history improves the prediction of another. In other words, it asks: “Does adding this information make my forecast better?”\nYou can have Granger causality without true cause-and-effect relationships in the real world.\nThanks to ChatGPT again\nFrom the results, looking at the ssr based F test p-value, it showed: - Residential Sales Price Index significantly Granger-causes GDP growth at shorter lags (1–3 quarters). - UAE PMI, tourism inflows, and personal lending demand were significant at lag 4, suggesting longer-term predictive power. - Business lending demand did not show statistically significant Granger causality at any lag.\nThese insights directly answered the question: “Does adding this information make my forecast better?” They helped identify which variables were worth including in the final VAR model, ensuring that only the most relevant predictors contributed to improving forecast accuracy.\n\nfrom statsmodels.tsa.stattools import grangercausalitytests\n\ndef merge_on_date(series1, series2, col1_name, col2_name):\n    \"\"\"\n    Convert two series to DataFrames (if necessary) and merge them on the index (date) using an inner join.\n    Returns a DataFrame with columns [col1_name, col2_name].\n    \"\"\"\n    # Convert to DataFrame if not already one.\n    df1 = series1.to_frame(name=col1_name) if not isinstance(series1, pd.DataFrame) else series1.copy()\n    df2 = series2.to_frame(name=col2_name) if not isinstance(series2, pd.DataFrame) else series2.copy()\n    \n    # Merge on the index (date) using an inner join.\n    merged_df = pd.merge(df1, df2, left_index=True, right_index=True, how='inner')\n    return merged_df\n\nmaxlag = 4  # Maximum lag to test\n\n# Dictionary of indicators (ensure these series are aligned appropriately)\nindicators = {\n    \"Residential Sales Index\": residential_diff,\n    \"Visitors\": visitors_log,\n    \"PMI\": pmi_diff,\n    \"Personal\": personal,\n    \"Business\": business,   \n}\n\nprint(\"Granger Causality Tests: GDP vs Indicators\")\nfor indicator_name, indicator_series in indicators.items():\n    # Merge GDP_diff and the indicator by date\n    df_test = merge_on_date(ngdp_diff, indicator_series, \"Non-Oil GDP\", indicator_name)\n    n_obs = df_test.shape[0]\n    required_obs = 3 * maxlag + 1  # Minimum required observations (for lag 4, 13 observations)\n    \n    print(f\"\\nTesting if {indicator_name} Granger-causes Non-Oil GDP:\")\n    print(f\"Number of observations: {n_obs} (required at least {required_obs})\")\n    \n    if n_obs &lt; required_obs:\n        print(f\"Skipping {indicator_name} due to insufficient observations.\")\n        continue\n\n    grangercausalitytests(df_test, maxlag=maxlag, verbose=True)\n\nGranger Causality Tests: GDP vs Indicators\n\nTesting if Residential Sales Index Granger-causes Non-Oil GDP:\nNumber of observations: 41 (required at least 13)\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=5.0774  , p=0.0303  , df_denom=37, df_num=1\nssr based chi2 test:   chi2=5.4890  , p=0.0191  , df=1\nlikelihood ratio test: chi2=5.1437  , p=0.0233  , df=1\nparameter F test:         F=5.0774  , p=0.0303  , df_denom=37, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=3.4687  , p=0.0426  , df_denom=34, df_num=2\nssr based chi2 test:   chi2=7.9577  , p=0.0187  , df=2\nlikelihood ratio test: chi2=7.2417  , p=0.0268  , df=2\nparameter F test:         F=3.4687  , p=0.0426  , df_denom=34, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=4.5753  , p=0.0091  , df_denom=31, df_num=3\nssr based chi2 test:   chi2=16.8254 , p=0.0008  , df=3\nlikelihood ratio test: chi2=13.9296 , p=0.0030  , df=3\nparameter F test:         F=4.5753  , p=0.0091  , df_denom=31, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=2.3178  , p=0.0818  , df_denom=28, df_num=4\nssr based chi2 test:   chi2=12.2514 , p=0.0156  , df=4\nlikelihood ratio test: chi2=10.5827 , p=0.0317  , df=4\nparameter F test:         F=2.3178  , p=0.0818  , df_denom=28, df_num=4\n\nTesting if Visitors Granger-causes Non-Oil GDP:\nNumber of observations: 41 (required at least 13)\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=0.2794  , p=0.6002  , df_denom=37, df_num=1\nssr based chi2 test:   chi2=0.3021  , p=0.5826  , df=1\nlikelihood ratio test: chi2=0.3010  , p=0.5833  , df=1\nparameter F test:         F=0.2794  , p=0.6002  , df_denom=37, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=0.5162  , p=0.6014  , df_denom=34, df_num=2\nssr based chi2 test:   chi2=1.1843  , p=0.5531  , df=2\nlikelihood ratio test: chi2=1.1666  , p=0.5580  , df=2\nparameter F test:         F=0.5162  , p=0.6014  , df_denom=34, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=2.5294  , p=0.0754  , df_denom=31, df_num=3\nssr based chi2 test:   chi2=9.3015  , p=0.0255  , df=3\nlikelihood ratio test: chi2=8.3203  , p=0.0398  , df=3\nparameter F test:         F=2.5294  , p=0.0754  , df_denom=31, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=3.3133  , p=0.0242  , df_denom=28, df_num=4\nssr based chi2 test:   chi2=17.5132 , p=0.0015  , df=4\nlikelihood ratio test: chi2=14.3384 , p=0.0063  , df=4\nparameter F test:         F=3.3133  , p=0.0242  , df_denom=28, df_num=4\n\nTesting if PMI Granger-causes Non-Oil GDP:\nNumber of observations: 41 (required at least 13)\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=0.5548  , p=0.4611  , df_denom=37, df_num=1\nssr based chi2 test:   chi2=0.5998  , p=0.4387  , df=1\nlikelihood ratio test: chi2=0.5953  , p=0.4404  , df=1\nparameter F test:         F=0.5548  , p=0.4611  , df_denom=37, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=0.8240  , p=0.4473  , df_denom=34, df_num=2\nssr based chi2 test:   chi2=1.8903  , p=0.3886  , df=2\nlikelihood ratio test: chi2=1.8459  , p=0.3974  , df=2\nparameter F test:         F=0.8240  , p=0.4473  , df_denom=34, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=2.4156  , p=0.0853  , df_denom=31, df_num=3\nssr based chi2 test:   chi2=8.8832  , p=0.0309  , df=3\nlikelihood ratio test: chi2=7.9828  , p=0.0464  , df=3\nparameter F test:         F=2.4156  , p=0.0853  , df_denom=31, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=5.8537  , p=0.0015  , df_denom=28, df_num=4\nssr based chi2 test:   chi2=30.9408 , p=0.0000  , df=4\nlikelihood ratio test: chi2=22.4856 , p=0.0002  , df=4\nparameter F test:         F=5.8537  , p=0.0015  , df_denom=28, df_num=4\n\nTesting if Personal Granger-causes Non-Oil GDP:\nNumber of observations: 41 (required at least 13)\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=1.5025  , p=0.2280  , df_denom=37, df_num=1\nssr based chi2 test:   chi2=1.6244  , p=0.2025  , df=1\nlikelihood ratio test: chi2=1.5923  , p=0.2070  , df=1\nparameter F test:         F=1.5025  , p=0.2280  , df_denom=37, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=2.7995  , p=0.0749  , df_denom=34, df_num=2\nssr based chi2 test:   chi2=6.4224  , p=0.0403  , df=2\nlikelihood ratio test: chi2=5.9453  , p=0.0512  , df=2\nparameter F test:         F=2.7995  , p=0.0749  , df_denom=34, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=2.4942  , p=0.0783  , df_denom=31, df_num=3\nssr based chi2 test:   chi2=9.1724  , p=0.0271  , df=3\nlikelihood ratio test: chi2=8.2164  , p=0.0417  , df=3\nparameter F test:         F=2.4942  , p=0.0783  , df_denom=31, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=3.2777  , p=0.0253  , df_denom=28, df_num=4\nssr based chi2 test:   chi2=17.3250 , p=0.0017  , df=4\nlikelihood ratio test: chi2=14.2105 , p=0.0067  , df=4\nparameter F test:         F=3.2777  , p=0.0253  , df_denom=28, df_num=4\n\nTesting if Business Granger-causes Non-Oil GDP:\nNumber of observations: 41 (required at least 13)\n\nGranger Causality\nnumber of lags (no zero) 1\nssr based F test:         F=0.3368  , p=0.5652  , df_denom=37, df_num=1\nssr based chi2 test:   chi2=0.3641  , p=0.5462  , df=1\nlikelihood ratio test: chi2=0.3625  , p=0.5471  , df=1\nparameter F test:         F=0.3368  , p=0.5652  , df_denom=37, df_num=1\n\nGranger Causality\nnumber of lags (no zero) 2\nssr based F test:         F=1.6563  , p=0.2059  , df_denom=34, df_num=2\nssr based chi2 test:   chi2=3.7998  , p=0.1496  , df=2\nlikelihood ratio test: chi2=3.6259  , p=0.1632  , df=2\nparameter F test:         F=1.6563  , p=0.2059  , df_denom=34, df_num=2\n\nGranger Causality\nnumber of lags (no zero) 3\nssr based F test:         F=1.2499  , p=0.3086  , df_denom=31, df_num=3\nssr based chi2 test:   chi2=4.5965  , p=0.2038  , df=3\nlikelihood ratio test: chi2=4.3390  , p=0.2271  , df=3\nparameter F test:         F=1.2499  , p=0.3086  , df_denom=31, df_num=3\n\nGranger Causality\nnumber of lags (no zero) 4\nssr based F test:         F=0.5135  , p=0.7263  , df_denom=28, df_num=4\nssr based chi2 test:   chi2=2.7143  , p=0.6067  , df=4\nlikelihood ratio test: chi2=2.6193  , p=0.6234  , df=4\nparameter F test:         F=0.5135  , p=0.7263  , df_denom=28, df_num=4"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#vector-autoregression-var",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#vector-autoregression-var",
    "title": "Introduction",
    "section": "Vector Autoregression (VAR)",
    "text": "Vector Autoregression (VAR)\nThe Vector Autoregression (VAR) framework allows all variables in the system to be treated as endogenous, capturing both direct and indirect effects across multiple time lags. This makes it particularly well-suited for analyzing the dynamic interactions among the indicators and non-oil GDP growth in complex economic systems (Sims, 1980).\nTo understand this better, imagine a group of friends who constantly influence each other’s decisions.\n\nIf one friend starts going to the gym, a few months later, another might pick up the habit.\n\nA third friend might start eating healthier in response to seeing both friends making positive lifestyle changes.\n\nIn this case, no one friend is the sole influencer they all react to and affect each other over time.\nSimilarly, in a VAR model, every variable is treated as both an influencer and a responder. It doesn’t assume that one variable is always the cause and the others are always the effect. Instead, it captures how they all interact and influence each other dynamically across different time lags.\nI have used a similar model before called the ARIMA model, which is designed to forecast how a single variable (for my case was the stock price of $PG) changes over time by learning from its own past behavior. In contrast, the VAR model looks at how multiple variables influence each other over time, treating all of them as both causes and effects in the system.\n\nLag Order Selection\nBefore fitting the VAR model, I needed to choose how many past quarters (lags) to use. This is important because too few lags might miss delayed effects, while too many can overfit the data.\nTo stay consistent, I set the maximum lag to 4 in the lag selection process since it’s the highest lag I’ve been using throughout my earlier analysis (e.g., Granger causality). I then used the .select_order() function from the statsmodels library, which evaluates different lag lengths (from 1 to 4 in this case) and identifies the best lag order based on statistical scoring rules.\nTo decide, I used the information criteria which are statistical tools used for model selection that balance model fit and complexity by penalizing excessive parameters, helping to prevent overfitting, I used four common ones:\n\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\nFinal Prediction Error (FPE)\nHannan-Quinn Criterion (HQIC)\n\nYou can think of it like four friends trying to pick a restaurant. They all have slightly different preferences, one wants cheap, another wants healthy, one likes variety, and another wants a short wait time. While they might not always agree, if all four pick the same place, it’s probably a solid choice.\nIn this case, all four criteria pointed to lag 4 as the best option. So I used 4 lags in the VAR model.\n\n# Join all variables based on the datetime index\nvar_df = ngdp_diff.join([\n    residential_diff,\n    visitors_log,\n    personal,\n    pmi_diff\n])\n\n# Drop any rows with missing values\nvar_df.dropna(inplace=True)\n\nfrom statsmodels.tsa.api import VAR\n\n# Create VAR model object\nmodel = VAR(var_df)\n\n# Select optimal lag length\nlag_selection = model.select_order(maxlags=4)\nlag_selection.summary()\n\n\nVAR Order Selection (* highlights the minimums)\n\n\n\nAIC\nBIC\nFPE\nHQIC\n\n\n0\n0.9565\n1.174\n2.603\n1.033\n\n\n1\n0.5895\n1.896\n1.829\n1.050\n\n\n2\n-0.3719\n2.023\n0.7563\n0.4723\n\n\n3\n-1.380\n2.103\n0.3410\n-0.1525\n\n\n4\n-3.401*\n1.171*\n0.07155*\n-1.789*\n\n\n\n\n\n\n\nVAR Results\nBased on the Granger causality results, only variables that showed statistically significant predictive power for non-oil GDP at any lag length were included in the VAR model hence, Using four lags, The model included five endogenous variables and was estimated over 37 quarterly observations.\nThe VAR model generates a system of six equations, one for each variable in the system, but for my analysis, the focus was specifically on the Non-Oil GDP equation, as it was the primary variable of interest.\n\\[\n\\text{GDP}_t = \\alpha + \\sum_{i=1}^{4} \\beta_i \\text{GDP}_{t-i} + \\sum_{i=1}^{4} \\gamma_i \\text{ResIndex}_{t-i} + \\sum_{i=1}^{4} \\delta_i \\text{Visitors}_{t-i} + \\sum_{i=1}^{4} \\phi_i \\text{Lending}_{t-i} + \\sum_{i=1}^{4} \\theta_i \\text{PMI}_{t-i} + \\varepsilon_t\n\\]\nWhere: - \\(\\text{GDP}_t\\) is Non-Oil GDP growth at time t\n- \\(\\text{RESI}\\) = Residential Sales Index\n- \\(\\text{VIS}\\) = Visitor Inflows\n- \\(\\text{PLD}\\) = Personal Lending Demand\n- \\(\\text{PMI}\\) = UAE Manufacturing PMI\n- \\(\\varepsilon_t\\) = error term\n\nUnderstanding the VAR Equation for Non-Oil GDP\nThis equation is one part of a system of five equations in the VAR model. It describes how the value of Non-Oil GDP at time ( t ) is influenced by:\n\nIts own past values : GDP from 1 to 4 periods ago\nThese are the terms like \\(GDP_{t-1},\\ GDP_{t-2},\\ \\dots\\)\nLagged values of the other variables:\n\nResidential Sales Index\nVisitors\nPersonal Lending Demand\nPMI\n\nEach of these variables enters the equation with 4 lags, meaning the model uses their values from the past four time periods to help predict current GDP.\nThe constant term ( \\(\\alpha\\)): This is the baseline level of GDP if all other influences were zero (the intercept).\nThe error term ( \\(\\varepsilon_t\\)): Captures any random shocks or unexplained changes in GDP.\n\nWe’re trying to predict today’s Non-Oil GDP using a combination of:\n\nWhat GDP was in the last 4 periods,\nWhat residential sales, visitors, lending, and PMI were in the last 4 periods,\n\nIn the Non-Oil GDP equation, most variables were not statistically significant at the 5% level, except for PMI at lag 4, which had a negative and significant effect (p = 0.023).\nOther variables such as the Residential Sales Index, Personal Lending, and Visitor Numbers showed some larger coefficients and borderline significance at longer lags, but none passed the usual 5% threshold in the GDP equation.\nOverall, the VAR results suggest that UAE Manufacturing PMI is the only reliable short-term predictor of non-oil GDP growth within this model. The other indicators may influence GDP more indirectly, with longer delays, or may be affected by structural factors outside the model’s scope.\n\\[\n\\text{GDP}_t = 0.027 - 0.002 \\cdot \\text{GDP}_{t-1} + 19.06 \\cdot \\text{ResIndex}_{t-1} + 0.17 \\cdot \\text{Visitors}_{t-1} + 0.10 \\cdot \\text{Lending}_{t-1} + 0.17 \\cdot \\text{PMI}_{t-1} + \\dots + \\varepsilon_t\n\\]\n(Full coefficients omitted for visual clarity. See table for all terms.)\n\n# Fit the model at lag 4\nresults = model.fit(4)\n\n# Print summary of the model\nresults.summary()\n\n  Summary of Regression Results   \n==================================\nModel:                         VAR\nMethod:                        OLS\nDate:           Sun, 11, May, 2025\nTime:                     17:03:12\n--------------------------------------------------------------------\nNo. of Equations:         5.00000    BIC:                    1.17060\nNobs:                     37.0000    HQIC:                  -1.78924\nLog likelihood:          -94.5866    FPE:                  0.0715525\nAIC:                     -3.40092    Det(Omega_mle):      0.00755951\n--------------------------------------------------------------------\nResults for equation Non-Oil GDP\n=============================================================================================\n                                coefficient       std. error           t-stat            prob\n---------------------------------------------------------------------------------------------\nconst                              0.027202         0.826879            0.033           0.974\nL1.Non-Oil GDP                    -0.002335         0.262790           -0.009           0.993\nL1.Residential Sales Index        19.063571        57.052020            0.334           0.738\nL1.Visitors                        0.172285         0.333261            0.517           0.605\nL1.Personal Lending Demand         0.097513         0.074224            1.314           0.189\nL1.PMI                             0.171908         0.252630            0.680           0.496\nL2.Non-Oil GDP                    -0.057310         0.172958           -0.331           0.740\nL2.Residential Sales Index       -68.315618        64.140023           -1.065           0.287\nL2.Visitors                        0.179809         0.428840            0.419           0.675\nL2.Personal Lending Demand        -0.040749         0.070511           -0.578           0.563\nL2.PMI                             0.375576         0.260185            1.443           0.149\nL3.Non-Oil GDP                     0.096099         0.179300            0.536           0.592\nL3.Residential Sales Index       -74.070909        51.241778           -1.446           0.148\nL3.Visitors                        0.314494         0.423607            0.742           0.458\nL3.Personal Lending Demand         0.043716         0.082972            0.527           0.598\nL3.PMI                             0.160643         0.341211            0.471           0.638\nL4.Non-Oil GDP                    -0.142234         0.177412           -0.802           0.423\nL4.Residential Sales Index        61.063097        43.303181            1.410           0.159\nL4.Visitors                       -0.018911         0.307582           -0.061           0.951\nL4.Personal Lending Demand        -0.116549         0.076646           -1.521           0.128\nL4.PMI                            -0.978764         0.431754           -2.267           0.023\n=============================================================================================\n\nResults for equation Residential Sales Index\n=============================================================================================\n                                coefficient       std. error           t-stat            prob\n---------------------------------------------------------------------------------------------\nconst                              0.001816         0.003014            0.603           0.547\nL1.Non-Oil GDP                     0.001864         0.000958            1.946           0.052\nL1.Residential Sales Index        -0.323274         0.207977           -1.554           0.120\nL1.Visitors                       -0.000000         0.001215           -0.000           1.000\nL1.Personal Lending Demand         0.000073         0.000271            0.270           0.787\nL1.PMI                             0.000650         0.000921            0.706           0.480\nL2.Non-Oil GDP                     0.001275         0.000631            2.022           0.043\nL2.Residential Sales Index         0.140876         0.233816            0.603           0.547\nL2.Visitors                        0.001011         0.001563            0.647           0.518\nL2.Personal Lending Demand        -0.000484         0.000257           -1.884           0.060\nL2.PMI                             0.000180         0.000948            0.190           0.850\nL3.Non-Oil GDP                     0.001302         0.000654            1.992           0.046\nL3.Residential Sales Index         0.164991         0.186797            0.883           0.377\nL3.Visitors                       -0.000285         0.001544           -0.184           0.854\nL3.Personal Lending Demand         0.000180         0.000302            0.596           0.551\nL3.PMI                            -0.002906         0.001244           -2.336           0.019\nL4.Non-Oil GDP                    -0.000702         0.000647           -1.085           0.278\nL4.Residential Sales Index        -0.420014         0.157857           -2.661           0.008\nL4.Visitors                       -0.000342         0.001121           -0.305           0.761\nL4.Personal Lending Demand         0.000126         0.000279            0.449           0.653\nL4.PMI                             0.000006         0.001574            0.004           0.997\n=============================================================================================\n\nResults for equation Visitors\n=============================================================================================\n                                coefficient       std. error           t-stat            prob\n---------------------------------------------------------------------------------------------\nconst                              0.148540         0.745333            0.199           0.842\nL1.Non-Oil GDP                     0.288083         0.236874            1.216           0.224\nL1.Residential Sales Index       -51.840243        51.425610           -1.008           0.313\nL1.Visitors                       -1.121010         0.300395           -3.732           0.000\nL1.Personal Lending Demand         0.055358         0.066904            0.827           0.408\nL1.PMI                             0.385779         0.227716            1.694           0.090\nL2.Non-Oil GDP                     0.103947         0.155901            0.667           0.505\nL2.Residential Sales Index         0.437067        57.814602            0.008           0.994\nL2.Visitors                       -0.878064         0.386548           -2.272           0.023\nL2.Personal Lending Demand        -0.067085         0.063558           -1.056           0.291\nL2.PMI                             0.612824         0.234526            2.613           0.009\nL3.Non-Oil GDP                    -0.076189         0.161617           -0.471           0.637\nL3.Residential Sales Index       -49.012700        46.188368           -1.061           0.289\nL3.Visitors                       -0.539115         0.381831           -1.412           0.158\nL3.Personal Lending Demand        -0.014622         0.074789           -0.196           0.845\nL3.PMI                             1.025898         0.307561            3.336           0.001\nL4.Non-Oil GDP                     0.028380         0.159916            0.177           0.859\nL4.Residential Sales Index        -5.494405        39.032667           -0.141           0.888\nL4.Visitors                       -0.230364         0.277249           -0.831           0.406\nL4.Personal Lending Demand         0.026006         0.069088            0.376           0.707\nL4.PMI                            -0.475725         0.389175           -1.222           0.222\n=============================================================================================\n\nResults for equation Personal Lending Demand\n=============================================================================================\n                                coefficient       std. error           t-stat            prob\n---------------------------------------------------------------------------------------------\nconst                              0.579003         2.556480            0.226           0.821\nL1.Non-Oil GDP                     0.549041         0.812472            0.676           0.499\nL1.Residential Sales Index       162.945998       176.389056            0.924           0.356\nL1.Visitors                        0.209060         1.030350            0.203           0.839\nL1.Personal Lending Demand         0.528909         0.229481            2.305           0.021\nL1.PMI                             0.701368         0.781063            0.898           0.369\nL2.Non-Oil GDP                    -0.673903         0.534738           -1.260           0.208\nL2.Residential Sales Index       -42.973955       198.303200           -0.217           0.828\nL2.Visitors                        0.291074         1.325854            0.220           0.826\nL2.Personal Lending Demand         0.071974         0.218002            0.330           0.741\nL2.PMI                             2.682019         0.804421            3.334           0.001\nL3.Non-Oil GDP                     0.444805         0.554345            0.802           0.422\nL3.Residential Sales Index       103.337438       158.425395            0.652           0.514\nL3.Visitors                        0.334854         1.309676            0.256           0.798\nL3.Personal Lending Demand         0.008650         0.256526            0.034           0.973\nL3.PMI                            -1.593701         1.054929           -1.511           0.131\nL4.Non-Oil GDP                    -1.252175         0.548509           -2.283           0.022\nL4.Residential Sales Index      -120.642043       133.881450           -0.901           0.368\nL4.Visitors                        0.922064         0.950959            0.970           0.332\nL4.Personal Lending Demand         0.416187         0.236969            1.756           0.079\nL4.PMI                            -1.760560         1.334865           -1.319           0.187\n=============================================================================================\n\nResults for equation PMI\n=============================================================================================\n                                coefficient       std. error           t-stat            prob\n---------------------------------------------------------------------------------------------\nconst                              0.136332         0.844292            0.161           0.872\nL1.Non-Oil GDP                    -0.012271         0.268324           -0.046           0.964\nL1.Residential Sales Index       -24.650173        58.253477           -0.423           0.672\nL1.Visitors                       -0.125821         0.340279           -0.370           0.712\nL1.Personal Lending Demand         0.116343         0.075788            1.535           0.125\nL1.PMI                             0.073901         0.257950            0.286           0.775\nL2.Non-Oil GDP                     0.038020         0.176600            0.215           0.830\nL2.Residential Sales Index       -34.755131        65.490746           -0.531           0.596\nL2.Visitors                       -0.196072         0.437871           -0.448           0.654\nL2.Personal Lending Demand        -0.065681         0.071996           -0.912           0.362\nL2.PMI                            -0.014077         0.265665           -0.053           0.958\nL3.Non-Oil GDP                     0.028025         0.183076            0.153           0.878\nL3.Residential Sales Index         4.168953        52.320877            0.080           0.936\nL3.Visitors                       -0.020181         0.432528           -0.047           0.963\nL3.Personal Lending Demand         0.068121         0.084719            0.804           0.421\nL3.PMI                            -0.138161         0.348396           -0.397           0.692\nL4.Non-Oil GDP                    -0.032776         0.181148           -0.181           0.856\nL4.Residential Sales Index        62.009461        44.215101            1.402           0.161\nL4.Visitors                        0.129284         0.314059            0.412           0.681\nL4.Personal Lending Demand        -0.144540         0.078260           -1.847           0.065\nL4.PMI                            -0.155714         0.440847           -0.353           0.724\n=============================================================================================\n\nCorrelation matrix of residuals\n                           Non-Oil GDP  Residential Sales Index  Visitors  Personal Lending Demand       PMI\nNon-Oil GDP                   1.000000                 0.060753  0.761382                 0.093986  0.211417\nResidential Sales Index       0.060753                 1.000000 -0.424514                 0.334433  0.383702\nVisitors                      0.761382                -0.424514  1.000000                -0.127675  0.110087\nPersonal Lending Demand       0.093986                 0.334433 -0.127675                 1.000000  0.138265\nPMI                           0.211417                 0.383702  0.110087                 0.138265  1.000000\n\n\n\n\nfrom statsmodels.stats.stattools import jarque_bera\n\n# DataFrame containing residuals for each VAR equation\nresiduals = results.resid\n\n# Run the Jarque-Bera normality test on the GDP residuals\njb_stat, jb_pvalue, skew, kurtosis = jarque_bera(residuals['Non-Oil GDP'])\n\n# Print results\nprint(f\"Jarque-Bera Statistic: {jb_stat:.3f}\")\nprint(f\"P-value: {jb_pvalue:.3f}\")\nprint(f\"Skewness: {skew:.3f}\")\nprint(f\"Kurtosis: {kurtosis:.3f}\")\n\n# Interpretation\nif jb_pvalue &gt; 0.05:\n    print(\"Residuals appear normally distributed (fail to reject H0).\")\nelse:\n    print(\"Residuals do not appear normally distributed (reject H0).\")\n\n\nJarque-Bera Statistic: 0.880\nP-value: 0.644\nSkewness: -0.264\nKurtosis: 2.459\nResiduals appear normally distributed (fail to reject H0).\n\n\n\n\nModel Evaluation\nTo fairly evaluate the VAR model’s forecasting performance and compare it with the earlier OLS model, I first needed to reverse the differencing process. Since the VAR was estimated on differenced data to ensure stationarity, its predictions are also in “changes” rather than actual GDP growth values.\nTo get meaningful evaluation metrics (like MAE or RMSE), I reconstructed the predicted levels of GDP by adding back the previous values essentially “undifferencing” the series. This allowed me to compare the predicted GDP growth values from the VAR model against the actual GDP growth rates in their original form. as shown in similar workflows like Kale (2020).\n\n\nEvaluation Metrics (VAR Model)\n\nMean Absolute Error (MAE): 2.68\n\nRoot Mean Squared Error (RMSE): 3.15\n\nMean Absolute Percentage Error (MAPE): 182.2%\n\nR-squared (R²): 0.480\n\nThe R² is comparable to the OLS model, and the slightly higher errors (MAE, RMSE, MAPE). The model didn’t perform strongly. It explained about 0.478 which is around 48% of the changes in non-oil GDP based on the R-squared value, the same as the linear model but with a much higher MAPE (182.2%) indicating limited accuracy.\nThe OLS model seemed to be the better option between the two.\n\nfrom sklearn.metrics import r2_score\n\noriginal_gdp = ngdp.iloc[:, 0].dropna().values\npredicted_diff = results.fittedvalues['Non-Oil GDP'].values\n\n# initial value: first value after the lag period\ninitial_value = original_gdp[results.k_ar]  # not results.k_ar - 1\n\n# Reconstruct original GDP values from predicted diffs\nreconstructed = [initial_value]\nfor diff in predicted_diff:\n    reconstructed.append(reconstructed[-1] + diff)\n\n# Drop first value to align with predicted_diff length\nreconstructed = reconstructed[1:]\n\n# Actual GDP aligned to match predicted diff length\nactual_aligned = original_gdp[results.k_ar + 1:]  # skip one more to match length\ntime_index = np.arange(len(actual_aligned))\n\n# Calculate evaluation metrics\nvar_mae = mean_absolute_error(actual_aligned, reconstructed)\nvar_rmse = mean_squared_error(actual_aligned, reconstructed, squared=False)\nvar_mape = np.mean(np.abs((actual_aligned - reconstructed) / actual_aligned)) * 100\nvar_r2 = r2_score(actual_aligned, reconstructed)\n\nprint(f\"Mean Absolute Error (MAE): {var_mae:.2f}\")\nprint(f\"Root Mean Squared Error (RMSE): {var_rmse:.2f}\")\nprint(f\"Mean Absolute Percentage Error (MAPE): {var_mape:.1f}%\")\nprint(f\"R-squared (R²): {var_r2:.3f}\")\n\nMean Absolute Error (MAE): 2.68\nRoot Mean Squared Error (RMSE): 3.15\nMean Absolute Percentage Error (MAPE): 182.2%\nR-squared (R²): 0.480\n\n\n\ndates = pd.date_range(start='2014-01-01', periods=len(actual_aligned), freq='Q')\n\n# Plot\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=dates, y=actual_aligned,\n    mode='lines+markers',\n    name='Actual GDP Growth',\n    line=dict(color='#FF4136', width=2),\n    marker=dict(color='#FF4136', size=5)\n))\n\nfig.add_trace(go.Scatter(\n    x=dates, y=reconstructed,\n    mode='lines+markers',\n    name='Predicted GDP Growth (VAR)',\n    line=dict(color='green', width=2),\n    marker=dict(size=6)\n))\n\n# Shaded area between actual and predicted\nfig.add_trace(go.Scatter(\n    x=np.concatenate([dates, dates[::-1]]),\n    y=np.concatenate([actual_aligned, reconstructed[::-1]]),\n    fill='toself',\n    fillcolor='rgba(144, 238, 144, 0.3)',  # LightGreen with transparency\n    line=dict(color='rgba(255,255,255,0)'),\n    hoverinfo='skip',\n    name='Prediction Error',\n    showlegend=True,\n))\n\n\n# Layout styling (black background, white text)\nfig.update_layout(\n    height=500, width=900,\n    margin=dict(l=50, r=40, t=40, b=40),\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),\n    xaxis_title='Date',\n    yaxis_title='QoQ Non-Oil GDP Growth (%)',\n    legend=dict(orientation='h', y=1.05, x=0.5, xanchor='center', yanchor='bottom')\n)\n\n# Axes\nfig.update_yaxes(showgrid=False, zeroline=False, gridcolor='lightgray', dtick=2)\nfig.update_xaxes(showgrid=False)\n\nfig.show()"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#impulse-response-analysis",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#impulse-response-analysis",
    "title": "Introduction",
    "section": "Impulse Response Analysis",
    "text": "Impulse Response Analysis\nEven though the VAR model had its limitations its still provided useful tools like the Impulse Response Functions (IRFs).which is a great way to see what happens to one variable when another suddenly changes and watching how it reacts over time.\nThink of it like tapping a glass of water. If you give it a quick tap, ripples form and gradually settle. The size of the ripples and how long they last tell you something about the stability and sensitivity of the water.\nIn the same way, an Impulse Response Function (IRF) shows how one variable reacts over time when another variable experiences a sudden “shock.” It helps visualize both the immediate impact and how long the effects persist before the system returns to normal.\nIn this case, I wanted to see how non-oil GDP responds to sudden changes in PMI, lending demand, real estate, and tourism while holding everything else constant.\nThis is especially useful in real-world scenarios, like when a new policy, interest rate shift, or external event causes a sudden movement in one sector. IRFs help reveal: - How strong the reaction is - Whether the effect is positive or negative - How long the impact lasts\nFor a country like the UAE, where the non-oil economy is growing fast but influenced by many moving parts, this kind of analysis helps us understand which sectors have the biggest short-term impact on growth.\n\nResults\nThe impulse response results showed that shocks to the UAE Manufacturing PMI had the strongest and most immediate impact on non-oil GDP, peaking around the second and third quarters.\nThis suggests that rising business confidence leads to short-term economic expansion, but the effect reverses later, possibly due to over-optimism\nReal estate prices, personal lending demand, and tourism inflows also produced early positive effects on GDP. Although not as prominent as the PMI, However, these impacts were also shortlived, generally fading by the fourth quarter. This means that while sectors like real estate and tourism can boost the economy temporarily, they do not sustain momentum beyond one year.\nOverall, these responses confirm that these indicators influence GDP within the first two to three quarters, which supports the idea that they could potentially be useful for short-term economic forecasting.\n\n# IRF setup\nirf = results.irf(4)  # 4 periods ahead\nirf_data = irf.orth_irfs  \nvariables = results.names\nresponse_var = 'Non-Oil GDP'\nresponse_idx = variables.index(response_var)\nn_steps = irf_data.shape[0]\nlags = list(range(n_steps))\n\n# Shock variables (exclude GDP itself)\nshock_vars = [v for v in variables if v != response_var]\ncolors = px.colors.qualitative.Bold[:len(shock_vars)]\n\n# Create figure\nfig = go.Figure()\n\n# Add line for each shock variable\nfor i, shock in enumerate(shock_vars):\n    shock_idx = variables.index(shock)\n    response = irf_data[:, response_idx, shock_idx]\n    fig.add_trace(go.Scatter(\n        x=lags,\n        y=response,\n        mode='lines+markers',\n        name=f'Shock in {shock}',\n        line=dict(color=colors[i], width=2),\n        marker=dict(size=5)\n    ))\n\n# Add zero line\nfig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\")\n\n# Styling\nfig.update_layout(\n    xaxis_title=\"Quarters After Shock\",\n    yaxis_title=\"non Oil GDP Response\",\n    plot_bgcolor='black',\n    paper_bgcolor='black',\n    font=dict(color='white'),    \n    showlegend=True,\n    legend=dict(\n        orientation=\"h\",\n        yanchor=\"bottom\",\n        y=1.05,\n        xanchor=\"center\",\n        x=0.5\n    ),\n    height=500,\n    width=800,\n    margin=dict(t=60)\n)\n\n\nfig.update_xaxes(showgrid=False, zeroline=False, showline=False, linecolor='black', tickcolor='black',dtick=1)\nfig.update_yaxes(showgrid=False,gridcolor='lightgrey', zeroline=False, showline=False, linecolor='black', tickcolor='black', dtick=0.2 )\n\nfig.show()"
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#conclusion",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#conclusion",
    "title": "Introduction",
    "section": "Conclusion",
    "text": "Conclusion\nThe research highlighted the predictive potential of these indicators.\nFuture research could benefit from integrating these localized variables with conventional macroeconomic indicators to develop more comprehensive forecasting models. This blended approach may improve accuracy by capturing both global trends and UAE-specific dynamics.\nMachine learning techniques also present an exciting opportunity, for now there isn’t much data but as datasets grow larger and more detailed over time, models like random forests, could potentially outperform traditional econometric methods by learning complex, non-linear patterns from the data. Longer historical time series would only improve their performance and robustness.\nFinally, there is significant potential to develop a UAE-specific Leading Economic Index built on this set of localized indicators. Such an index could serve as an early warning system for policymakers, investors, and businesses providing near real-time insights for informed decision-making. Similar to the effort made by El Mahmah (2017) using conventional indicators. (Central Bank of UAE, 2017)."
  },
  {
    "objectID": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#conclusion-1",
    "href": "Quant/2025-05-05-UAE-non-Oil-GDP-Forecast.html#conclusion-1",
    "title": "Introduction",
    "section": "Conclusion",
    "text": "Conclusion\nWhile the forecasting models used in this study weren’t perfect, tools like correlation analysis and Impulse Response Functions (IRFs) still offered meaningful insights. They highlighted some early signals of predictive potential in these indicators even if the overall model accuracy was limited.\nLooking ahead, future research could explore combining these variables with more traditional macroeconomic indicators. A mixed approach like this could help build more complete and accurate forecasting models.\nMachine learning techniques also present an exciting opportunity. Although the current dataset is relatively small, as more data becomes available over time, models like random forests could uncover, non-linear relationships that most econometric models might miss. More historical data generally makes these models smarter and more reliable.\nFinally, there is significant potential to develop a UAE-specific Leading Economic Index built on these kind of indicators. Such an index could serve as an early warning system for policymakers, investors, and businesses providing near real-time insights for informed decision-making. Similar to the effort made by El Mahmah (2017) using conventional indicators. (Central Bank of UAE, 2017).\nReferences\nMcCloskey, J., & Remor, A., 2025.\nBentour, Y., & Fund, R., 2022. Macroeconomic Forecasting in Oil-Exporting Economies: The Case of UAE. ResearchGate. Link\nCherif, R., Hasanov, F., & Zhu, M., 2011. Breaking the Oil Spell: The Gulf Falcons’ Path to Diversification. IMF. Link\nEl Mahmah, M.A., 2017. CONSTRUCTING AN ECONOMIC COMPOSITE INDICATOR FOR THE UAE. Central Bank of the United Arab Emirates. Available at: https://www.centralbank.ae/media/sznddngl/wp19062017.pdf\nHann, R.N., Li, C., and Ogneva, M., 2017. Aggregate Earnings and Their Relation to Macroeconomic Activity: A Labor Market Perspective. SSRN Working Paper. Available at: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2993654.\nWooldridge, J.M., 2013. Introductory Econometrics: A Modern Approach. 5th ed. Mason, OH: South-Western Cengage Learning.\nKale, A., 2020. Vector Autoregression (VAR) – Comprehensive Guide with Examples in Python. [online] MachineLearningPlus. Available at: https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/\nBentour, E.M. & Fund, A.M., 2022. The Role of Oil Prices in Forecasting Economic Growth in Oil Exporting Countries: Evidence from the Kingdom of Saudi Arabia and the United Arab Emirates. Available at: https://www.researchgate.net/publication/358039803\nMagazzino, C., 2016. The relationship between real GDP, CO₂ emissions, and energy use in the GCC countries: A time series approach. Cogent Economics & Finance, 4(1). Available at: https://doi.org/10.1080/23322039.2016.1152729\nKireyev, A., 2000. Comparative Macroeconomic Dynamics in the Arab World: A Panel VAR Approach. SSRN. Available at: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=879461\nGranger, C.W.J., 1969. Investigating causal relations by econometric models and cross-spectral methods. Econometrica, 37(3), pp.424-438. Available at: http://www.econ.uiuc.edu/~econ536/Papers/granger69.pdf.\nSims, C.A., 1980. Macroeconomics and reality. Econometrica, 48(1), pp.1-48. Available at: https://www.jstor.org/stable/1912017.\nMcCloskey, P.J. & Remor, R.M.,2025. Comparative Analysis of ARIMA, VAR, and Linear Regression Models for UAE GDP Forecasting. Emirati Journal of Business, Economics & Social Studies, 4(1), pp. 23–33. Available at: https://www.emiratesscholar.com/system/publish/070325040313116.pdf"
  },
  {
    "objectID": "Quant/2022-06-01-btc-usd-price-prediction.html",
    "href": "Quant/2022-06-01-btc-usd-price-prediction.html",
    "title": "BTC/USD Price Prediction Using Linear Regression",
    "section": "",
    "text": "In this notebook I’m going to try and predict the BTC/USD close price(target/dependant variable) for the month of June 2022 using the linear regression as the chosen model, I will also use a set of relevant independent variables like moving averages, volume, etc. I’ll follow a basic data science workflow from importing data to model development and evaluation. Lastly, assuming the model passes the validation stage, I will bactest and measure its performance as a trading strategy to further gain insights on how reliable it can be before placing any trade based on the prediction made\n\n\ntoc: true\nbadges: true\ncategories:[Quantitative Research]\nimage:images/lr.png\nuse_plotly: true\n\n\nDISCLAIMER!\nBefore proceeding, please make sure that you note the following important information:\nNOT FINANCIAL ADVICE!\nMy content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice, None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument to make any investment or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur\n\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom scipy import stats\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom datetime import datetime\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nfrom IPython.display import HTML\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n\nImport Data\n\n#Downloading all relevant data\n\n#BTC price and volume\n\n#you will import Gold price when you start creating the indipendant variables\n\n# Read data\nDf = yf.download('BTC-USD', '2012-01-01', '2022-05-31', interval= '1mo', auto_adjust=True)\n\n# Only keep close columns\nDf = Df[['Close','Volume']]\n\n# Drop rows with missing values\nDf = Df.dropna()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nData Cleaning and Wrangling\nBefore any modeling can be done, there are a few steps needed to prepare the data before feeding it to the model, at least by arranging the data set in a way it makes sense\n\n#Reset date index and arrange rows in same format\ndf = Df.reset_index()\nfor i in ['Close', 'Volume']: \n      df[i]  =  df[i].astype('float64')\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\n\n\n\n\n0\n2014-10-01\n338.321014\n9.029944e+08\n\n\n1\n2014-11-01\n378.046997\n6.597334e+08\n\n\n2\n2014-12-01\n320.192993\n5.531023e+08\n\n\n3\n2015-01-01\n217.464005\n1.098812e+09\n\n\n4\n2015-02-01\n254.263000\n7.115187e+08\n\n\n...\n...\n...\n...\n\n\n87\n2022-01-01\n38483.125000\n9.239790e+11\n\n\n88\n2022-02-01\n43193.234375\n6.713360e+11\n\n\n89\n2022-03-01\n45538.675781\n8.309438e+11\n\n\n90\n2022-04-01\n37714.875000\n8.301159e+11\n\n\n91\n2022-05-01\n31792.310547\n1.105689e+12\n\n\n\n\n92 rows × 3 columns\n\n\n\nBecause I’m working with monthly data, I’ll drop the days in the date to avoid confusion\n\ndate_format = \"%Y/%m\"\ndf['Date'] = df['Date'].dt.strftime(date_format)\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\n\n\n\n\n0\n2014/10\n338.321014\n9.029944e+08\n\n\n1\n2014/11\n378.046997\n6.597334e+08\n\n\n2\n2014/12\n320.192993\n5.531023e+08\n\n\n3\n2015/01\n217.464005\n1.098812e+09\n\n\n4\n2015/02\n254.263000\n7.115187e+08\n\n\n...\n...\n...\n...\n\n\n87\n2022/01\n38483.125000\n9.239790e+11\n\n\n88\n2022/02\n43193.234375\n6.713360e+11\n\n\n89\n2022/03\n45538.675781\n8.309438e+11\n\n\n90\n2022/04\n37714.875000\n8.301159e+11\n\n\n91\n2022/05\n31792.310547\n1.105689e+12\n\n\n\n\n92 rows × 3 columns\n\n\n\nBitcoin is often referred to as “digital gold” by its backers hence I’ll add the gold price and volume data as potential independent variables and I’ll explore further to see its relationship and whether or not it will be a good predictor\n\n# Import gold\ngold = yf.download('GLD', '2014-10-01', '2022-05-31', interval= '1mo', auto_adjust=True)\ngold = gold[['Close','Volume']]\ngld = gold.reset_index()\nfor i in ['Close', 'Volume']: \n      gld[i]  =  gld[i].astype('float64')\ngld\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\n\n\n\n\n0\n2014-10-01\n112.660004\n155183900.0\n\n\n1\n2014-11-01\n112.110001\n147594200.0\n\n\n2\n2014-12-01\n113.580002\n153722200.0\n\n\n3\n2015-01-01\n123.449997\n198034100.0\n\n\n4\n2015-02-01\n116.160004\n125686200.0\n\n\n...\n...\n...\n...\n\n\n87\n2022-01-01\n168.089996\n211125100.0\n\n\n88\n2022-02-01\n178.380005\n254601300.0\n\n\n89\n2022-03-01\n180.649994\n377087100.0\n\n\n90\n2022-04-01\n176.910004\n195346400.0\n\n\n91\n2022-05-01\n171.139999\n179902200.0\n\n\n\n\n92 rows × 3 columns\n\n\n\nThe other two independent variables will be the moving averages and volume\nmoving averages are often used by technical analysts to keep track of price trends for specific securities. I’ll use the 3 and 6 month exponential moving averages but whether it’s simple, weighted, or exponential in general it doesn’t really make much of a difference (but this could be a good hypothesis to test)\nVolume is also a well-known indicator of price movement, Trading volume is the total number of shares/units of a security traded during a given period of time.\n\n#add the first two indipendant variables: 3ema, 6ema\ndf['ema3'] = df['Close'].ewm(span=3, adjust=False).mean()\ndf['ema6'] = df['Close'].ewm(span=6, adjust=False).mean()\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\nema3\nema6\n\n\n\n\n0\n2014/10\n338.321014\n9.029944e+08\n338.321014\n338.321014\n\n\n1\n2014/11\n378.046997\n6.597334e+08\n358.184006\n349.671295\n\n\n2\n2014/12\n320.192993\n5.531023e+08\n339.188499\n341.248923\n\n\n3\n2015/01\n217.464005\n1.098812e+09\n278.326252\n305.881804\n\n\n4\n2015/02\n254.263000\n7.115187e+08\n266.294626\n291.133574\n\n\n...\n...\n...\n...\n...\n...\n\n\n87\n2022/01\n38483.125000\n9.239790e+11\n44519.703176\n46247.967191\n\n\n88\n2022/02\n43193.234375\n6.713360e+11\n43856.468776\n45375.186387\n\n\n89\n2022/03\n45538.675781\n8.309438e+11\n44697.572278\n45421.897642\n\n\n90\n2022/04\n37714.875000\n8.301159e+11\n41206.223639\n43219.891173\n\n\n91\n2022/05\n31792.310547\n1.105689e+12\n36499.267093\n39954.868137\n\n\n\n\n92 rows × 5 columns\n\n\n\n\n# adding the other two indipendant variables download gold price and godl volume (btc volume already there)\ndf['Gold Close'] = gld['Close']\ndf['Gold Volume'] = gld['Volume']\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\nema3\nema6\nGold Close\nGold Volume\n\n\n\n\n0\n2014/10\n338.321014\n9.029944e+08\n338.321014\n338.321014\n112.660004\n155183900.0\n\n\n1\n2014/11\n378.046997\n6.597334e+08\n358.184006\n349.671295\n112.110001\n147594200.0\n\n\n2\n2014/12\n320.192993\n5.531023e+08\n339.188499\n341.248923\n113.580002\n153722200.0\n\n\n3\n2015/01\n217.464005\n1.098812e+09\n278.326252\n305.881804\n123.449997\n198034100.0\n\n\n4\n2015/02\n254.263000\n7.115187e+08\n266.294626\n291.133574\n116.160004\n125686200.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n87\n2022/01\n38483.125000\n9.239790e+11\n44519.703176\n46247.967191\n168.089996\n211125100.0\n\n\n88\n2022/02\n43193.234375\n6.713360e+11\n43856.468776\n45375.186387\n178.380005\n254601300.0\n\n\n89\n2022/03\n45538.675781\n8.309438e+11\n44697.572278\n45421.897642\n180.649994\n377087100.0\n\n\n90\n2022/04\n37714.875000\n8.301159e+11\n41206.223639\n43219.891173\n176.910004\n195346400.0\n\n\n91\n2022/05\n31792.310547\n1.105689e+12\n36499.267093\n39954.868137\n171.139999\n179902200.0\n\n\n\n\n92 rows × 7 columns\n\n\n\nNow i’m going to generate the dependant/target variable that i’m going to try and predict\n\n#Creating the dependant variable which is next month close price and adding it to the dataframe \ndf['Next Month Close'] = df['Close'].shift(-1)\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\nema3\nema6\nGold Close\nGold Volume\nNext Month Close\n\n\n\n\n0\n2014/10\n338.321014\n9.029944e+08\n338.321014\n338.321014\n112.660004\n155183900.0\n378.046997\n\n\n1\n2014/11\n378.046997\n6.597334e+08\n358.184006\n349.671295\n112.110001\n147594200.0\n320.192993\n\n\n2\n2014/12\n320.192993\n5.531023e+08\n339.188499\n341.248923\n113.580002\n153722200.0\n217.464005\n\n\n3\n2015/01\n217.464005\n1.098812e+09\n278.326252\n305.881804\n123.449997\n198034100.0\n254.263000\n\n\n4\n2015/02\n254.263000\n7.115187e+08\n266.294626\n291.133574\n116.160004\n125686200.0\n244.223999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n87\n2022/01\n38483.125000\n9.239790e+11\n44519.703176\n46247.967191\n168.089996\n211125100.0\n43193.234375\n\n\n88\n2022/02\n43193.234375\n6.713360e+11\n43856.468776\n45375.186387\n178.380005\n254601300.0\n45538.675781\n\n\n89\n2022/03\n45538.675781\n8.309438e+11\n44697.572278\n45421.897642\n180.649994\n377087100.0\n37714.875000\n\n\n90\n2022/04\n37714.875000\n8.301159e+11\n41206.223639\n43219.891173\n176.910004\n195346400.0\n31792.310547\n\n\n91\n2022/05\n31792.310547\n1.105689e+12\n36499.267093\n39954.868137\n171.139999\n179902200.0\nNaN\n\n\n\n\n92 rows × 8 columns\n\n\n\n\n#We don't need the current close price hence i'll remove it from the dataframe\nbtc_close = df['Close'] #But i'll save the close price just incase i need it \n\n#Then i'll remove the previous month btc close price so that i'm left with only the relevant data that i need\ndf.drop(columns='Close', inplace=True)\n\n\n#i'll drop all NA values in the data frame\nDf = df.dropna()\n\n#Now i should have a good clean dataframe ready for some EDA\nDf\n\n\n\n\n\n\n\n\nDate\nVolume\nema3\nema6\nGold Close\nGold Volume\nNext Month Close\n\n\n\n\n0\n2014/10\n9.029944e+08\n338.321014\n338.321014\n112.660004\n155183900.0\n378.046997\n\n\n1\n2014/11\n6.597334e+08\n358.184006\n349.671295\n112.110001\n147594200.0\n320.192993\n\n\n2\n2014/12\n5.531023e+08\n339.188499\n341.248923\n113.580002\n153722200.0\n217.464005\n\n\n3\n2015/01\n1.098812e+09\n278.326252\n305.881804\n123.449997\n198034100.0\n254.263000\n\n\n4\n2015/02\n7.115187e+08\n266.294626\n291.133574\n116.160004\n125686200.0\n244.223999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n86\n2021/12\n9.570472e+11\n50556.281353\n49353.904068\n170.960007\n151214100.0\n38483.125000\n\n\n87\n2022/01\n9.239790e+11\n44519.703176\n46247.967191\n168.089996\n211125100.0\n43193.234375\n\n\n88\n2022/02\n6.713360e+11\n43856.468776\n45375.186387\n178.380005\n254601300.0\n45538.675781\n\n\n89\n2022/03\n8.309438e+11\n44697.572278\n45421.897642\n180.649994\n377087100.0\n37714.875000\n\n\n90\n2022/04\n8.301159e+11\n41206.223639\n43219.891173\n176.910004\n195346400.0\n31792.310547\n\n\n\n\n91 rows × 7 columns\n\n\n\n\n\nExplanatory Data Analysis\nIntuitley I know that traders like to use the ema lines and volume to predict BTC price. But as I mentioned before here I get the chance to explore whether gold price and its volume can help predict BTC price.\n\n\nThe Pearson correlation coefficient and p value\n\nPearson Correlation:\nCorrelation between sets of data is a measure of how well they are related. The most common measure of correlation in stats is the Pearson Correlation.The full name is the Pearson Product Moment Correlation (PPMC). It shows the linear relationship between two sets of data. In simple terms, it answers the question, Can I draw a line graph to represent the data?\nIt is a number between –1 and 1 that measures the strength and direction of the relationship between two variables, where:\n\n1: Perfect positive linear correlation.\n\n&lt;li&gt;&lt;b&gt;0&lt;/b&gt;: No linear correlation, the two variables most likely do not affect each other.&lt;/li&gt;\n&lt;li&gt;&lt;b&gt;-1&lt;/b&gt;: Perfect negative linear correlation.&lt;/li&gt;\n\n\nP-Value:\nA p-value measures the probability of obtaining the observed results, assuming that the null hypothesis is true. The lower the p-value, the greater the statistical significance of the observed difference. A p-value of 0.05 or lower is generally considered statistically significant which means that we are 95% confident that the correlation between the variables is significant.\nBy convention, when the\n\n\np-value is &lt; 0.001: we say there is strong evidence that the correlation is significant.\n\n\nthe p-value is &lt; 0.05: there is moderate evidence that the correlation is significant.\n\n\nthe p-value is &lt; 0.1: there is weak evidence that the correlation is significant.\n\n\nthe p-value is &gt; 0.1: there is no evidence that the correlation is significant.\n\n\nTwo things keeps to keep in mind when interprating the results:\n\n\nThe null hypothesis is that the two variables are uncorrelated .\n\n\nThe p value is in scientific notation, it’s decimal form is e.g 4.2e-7 = 0.00000042.\n\n\n\n#checking pearson correlation for gold price and volume\n#I will start form the second last row to avoid errors bcz of nan value \npearson_coef, p_value = stats.pearsonr(df['Gold Close'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nThe Pearson Correlation Coefficient is 0.7706711768570538  with a P-value of P = 6.555477090301474e-19\n\n\nIn this case,\nThe p-value is &lt; 0.001 hence, there is strong evidence that the correlation between gold price and BTC price is statistically significant, and the linear relationship is quite strong (0.77, close to 1)\n\npearson_coef, p_value = stats.pearsonr(df['Gold Volume'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nThe Pearson Correlation Coefficient is 0.08476385450593574  with a P-value of P = 0.42700282567060693\n\n\nThe p-value is &lt; 0.001 hence, there is moderate evidence that the correlation between gold volume and BTC price is statistically significant, and there is no linear relationship (0.08, almost 0)\nVisually we can see that there is almost no linear relationship between gold volume and btc price\n\n# make linear regression chart of btc next month price and gold price and do for volume\nfig = px.scatter(\n    df, x='Gold Volume', y='Next Month Close', opacity=0.65,\n    trendline='ols', trendline_color_override='firebrick'\n)\nfig.update_layout(template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nSo we now know that we can use gold price but not its volume, it would have destroyed value and it wouldn’t have added anything to the model if anything it would have probably ruined our prediction\n\n#Dropping gold volume\ndf.drop(columns='Gold Volume', inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nDate\nVolume\nema3\nema6\nGold Close\nNext Month Close\n\n\n\n\n0\n2014/10\n9.029944e+08\n338.321014\n338.321014\n112.660004\n378.046997\n\n\n1\n2014/11\n6.597334e+08\n358.184006\n349.671295\n112.110001\n320.192993\n\n\n2\n2014/12\n5.531023e+08\n339.188499\n341.248923\n113.580002\n217.464005\n\n\n3\n2015/01\n1.098812e+09\n278.326252\n305.881804\n123.449997\n254.263000\n\n\n4\n2015/02\n7.115187e+08\n266.294626\n291.133574\n116.160004\n244.223999\n\n\n\n\n\n\n\nWhat about the the other indipendant variables?\n\nprint('EMA 3')\npearson_coef, p_value = stats.pearsonr(df['ema3'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nprint('\\nEMA 6')\npearson_coef, p_value = stats.pearsonr(df['ema6'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nprint('\\nVolume')\npearson_coef, p_value = stats.pearsonr(df['Volume'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nEMA 3\nThe Pearson Correlation Coefficient is 0.9518278081533298  with a P-value of P = 5.913915911148407e-47\n\nEMA 6\nThe Pearson Correlation Coefficient is 0.9357719658423489  with a P-value of P = 1.3124438502636034e-41\n\nVolume\nThe Pearson Correlation Coefficient is 0.7995337431531997  with a P-value of P = 3.465784507142274e-21\n\n\nThe other indipendant variables are all statistically significant, and their linear relationship are very strong with p-values of &lt; 0.001\n\n\nNOTE:\nCausation is the relationship between cause and effect between two variables.\nIt is important to know the difference between correlation and causation. Correlation does not imply causation. Determining correlation is much simpler than determining causation as causation may require independent experimentation.\n\n\n\nModel Development\nBefore we continue let’s clarify the objective again: I’m using the 3 and 6 month ema, BTC previous month volume, and Gold close price of the preceding month to predict what BTC close price of the impending month\n\n#Define the indipendant variables\nx = Df[['ema3','ema6','Volume','Gold Close']]\n\n# Define the dependent variable\ny = Df['Next Month Close']\n\nI am going to spilt the data, 80% of the data will be used to train the model and 20% will be used to test the prediction made from that 80%\n\n# Split the data into train and test dataset\nt = .8\nt = int(t*len(Df))\n\n# Train dataset\nx_train = x[:t]\ny_train = y[:t]\n\n# Test dataset\nx_test = x[t:]\ny_test = y[t:]\n\n\n#print the lengths\nprint(\"number of test samples :\", y_test.shape[0])\nprint(\"number of training samples:\",y_train.shape[0])\n\nnumber of test samples : 19\nnumber of training samples: 72\n\n\n\n#fit the regression\nreg = linear_model.LinearRegression()\nreg.fit(x_train,y_train)\n\nLinearRegression()\n\n\nThe constant came back negative which is confusing but i’ll get back to this later\n\n#Intercept value\nreg.intercept_ \n\n-1262.8899087871669\n\n\n\n#make coefficient table\ncoeff_df = pd.DataFrame(reg.coef_.T, x.columns, columns=['Coefficient']) \ncoeff_df \n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nema3\n1.777304e+00\n\n\nema6\n-9.488261e-01\n\n\nVolume\n9.555041e-10\n\n\nGold Close\n1.521999e+01\n\n\n\n\n\n\n\nThe Multiple linear regression formula:\n\n\\[ y = β_{0} \\;+ \\;β_{1} * \\;X_{1} + \\;β_{2} * \\;X_{2} + \\;β_{3} * \\;X_{3} + \\;β_{4} * \\;X_{4}\\]\n\nprint(\"Linear Regression model\")\nprint(\"BTC Price (y) = %.2f (constant) + %.2f * EM3 (x1) + %.2f * EMA6 (x2) + %.4f * Volume (x3) + %.2f * Gold Close (x4)\" % (reg.intercept_,reg.coef_[0], reg.coef_[1],reg.coef_[2],reg.coef_[3]))\n\nLinear Regression model\nBTC Price (y) = -1262.89 (constant) + 1.78 * EM3 (x1) + -0.95 * EMA6 (x2) + 0.0000 * Volume (x3) + 15.22 * Gold Close (x4)\n\n\n\n\n\nModel Evaluation\nIn this step, I will evaluate the model’s accuracy but before that happens I’m going to make the predictions first\n\n#make prediction of test data\npredicted_price = reg.predict(x_test)\n\nThe R square is 0.05 which means the model’s predicitive power is poor in fact it is worse than what I expected it predicts little to nothing of the target variable\n\n#check r2 and MSE's of testing data\ntest_r2_score = r2_score(y_test,predicted_price)\nprint('The test R-square is: ', test_r2_score)\n\nThe test R-square is:  0.053315838215583056\n\n\nI will also look at other evaluation methods\n\n#Test r2\ntest_r2_score = r2_score(y_test,predicted_price)\nprint('The test R-square is: ', test_r2_score)\n\ntest_mse = mean_squared_error(y_test, predicted_price)\nprint('The test mean square error of target variable and predicted value is: ', test_mse)\n\ntest_mae = mean_absolute_error(y_test, predicted_price)\nprint('The test mean absolute error of target variable and predicted value is: ', test_mae)\n\ntest_rmse=np.sqrt(test_mse)\nprint('The test root mean square error of target variable and predicted value is: ', test_rmse)\n\nThe test R-square is:  0.053315838215583056\nThe test mean square error of target variable and predicted value is:  107191024.05803142\nThe test mean absolute error of target variable and predicted value is:  8821.373009960726\nThe test root mean square error of target variable and predicted value is:  10353.309811747711\n\n\nMean Square Error (MSE) is the average difference of actual values and predicted values There is no correct value for MSE. Simply put, the lower the value the better, and 0 means the model is perfect.\nMean Absolute Error (MAE) is the sum of the absolute difference between actual and predicted values in this case the average difference is $8821\nI’m going to evaluate further and try to see what other insights I can gather from the predicted price I’ll start by creating a data frame and add the predicted price and actual price so that I can plot the prices side by side\n\nbtc = pd.DataFrame()\n\n#btc['Close Previous Month'] = btc_close[t:]\nbtc['Date'] = Df['Date'][t:]\nbtc['Predicted Close'] = predicted_price\nbtc['Actual Close'] = btc_close[t:].shift(-1)#btc['Close Previous Month'].shift(-1)\nbtc\n\n\n\n\n\n\n\n\nDate\nPredicted Close\nActual Close\n\n\n\n\n72\n2020/10\n13607.631908\n19625.835938\n\n\n73\n2020/11\n17737.624104\n29001.720703\n\n\n74\n2020/12\n25451.933856\n33114.359375\n\n\n75\n2021/01\n31623.812022\n45137.769531\n\n\n76\n2021/02\n40797.895059\n58918.832031\n\n\n77\n2021/03\n52011.667637\n57750.175781\n\n\n78\n2021/04\n55687.583652\n37332.855469\n\n\n79\n2021/05\n43935.783612\n35040.835938\n\n\n80\n2021/06\n35882.237824\n41626.195312\n\n\n81\n2021/07\n36483.673993\n47166.687500\n\n\n82\n2021/08\n40423.918840\n43790.894531\n\n\n83\n2021/09\n39815.250546\n61318.957031\n\n\n84\n2021/10\n50330.042450\n57005.425781\n\n\n85\n2021/11\n51684.668062\n46306.445312\n\n\n86\n2021/12\n45279.181353\n38483.125000\n\n\n87\n2022/01\n37422.063673\n43193.234375\n\n\n88\n2022/02\n36986.624235\n45538.675781\n\n\n89\n2022/03\n38624.254997\n37714.875000\n\n\n90\n2022/04\n34450.675253\n31792.310547\n\n\n\n\n\n\n\n\n# make a chart of predicted next month close VS actual next month close\nfig = px.line(btc, x=\"Date\", y=btc.columns,              \n              title='Predicted Close Vs Actual Close')\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.update_layout(template = \"plotly_dark\")\n\n                                                \n\n\nWe have already seen the mean of these residuals (mean squared error), now I’ll look at the residuals of each month in absolute and in % to see how far off are the predictions for each month (Remember, the residual is the difference between the observed value and the estimated value)\n\nbtc['Residual'] = btc_close[t:] - btc['Actual Close'] #The difference in absolute $ terms\nbtc['Residual in %'] = np.absolute(btc['Residual']/btc['Actual Close']*100) #The difference in % \nbtc\n\n\n\n\n\n\n\n\nDate\nPredicted Close\nActual Close\nResidual\nResidual in %\n\n\n\n\n72\n2020/10\n13607.631908\n19625.835938\n-5844.840820\n29.781360\n\n\n73\n2020/11\n17737.624104\n29001.720703\n-9375.884766\n32.328719\n\n\n74\n2020/12\n25451.933856\n33114.359375\n-4112.638672\n12.419502\n\n\n75\n2021/01\n31623.812022\n45137.769531\n-12023.410156\n26.637138\n\n\n76\n2021/02\n40797.895059\n58918.832031\n-13781.062500\n23.389911\n\n\n77\n2021/03\n52011.667637\n57750.175781\n1168.656250\n2.023641\n\n\n78\n2021/04\n55687.583652\n37332.855469\n20417.320312\n54.689951\n\n\n79\n2021/05\n43935.783612\n35040.835938\n2292.019531\n6.540996\n\n\n80\n2021/06\n35882.237824\n41626.195312\n-6585.359375\n15.820229\n\n\n81\n2021/07\n36483.673993\n47166.687500\n-5540.492188\n11.746621\n\n\n82\n2021/08\n40423.918840\n43790.894531\n3375.792969\n7.708892\n\n\n83\n2021/09\n39815.250546\n61318.957031\n-17528.062500\n28.585063\n\n\n84\n2021/10\n50330.042450\n57005.425781\n4313.531250\n7.566878\n\n\n85\n2021/11\n51684.668062\n46306.445312\n10698.980469\n23.104733\n\n\n86\n2021/12\n45279.181353\n38483.125000\n7823.320312\n20.329223\n\n\n87\n2022/01\n37422.063673\n43193.234375\n-4710.109375\n10.904739\n\n\n88\n2022/02\n36986.624235\n45538.675781\n-2345.441406\n5.150438\n\n\n89\n2022/03\n38624.254997\n37714.875000\n7823.800781\n20.744602\n\n\n90\n2022/04\n34450.675253\n31792.310547\n5922.564453\n18.628921\n\n\n\n\n\n\n\nAs you can see from the residual, the difference is pretty large but this is due to BTC being very volatile hence anything between 7-10% difference (this is subjective based on my observations from BTC trading) could be good but to expect a residual of less than 5% consistently would be very unlikely from an asset class this volatile\n\n\nConclusion\nIf the model had been at least 50-60% accurate (have an R square of 0.50-0.60), I would have proceeded with backtesting and then take the model live by predicting the close price of this month (June 2022)\nThe linear regression is not a good model to use when predicting BTC/USD prices, maybe it would’ve been more efficient in predicting the returns instead. There were many red flags and based on the R square alone I would never take this model live and risk real money on it\nThe MSE was way too high and very far from 0 since MSE is a measure of how close a fitted line is to data points\nAnother red flag was the constant being negative, This means when the independent variables are 0 the mean price of BTC will be -1262. A negative constant doesn’t mean the regression made a mistake but rather it’s the data being modeled, realistically any security price can go to 0 but no security price can fall below 0 and turn negative, The your position value of that asset can turn negative but not the actual asset price. which is also why I think predicting returns instead of price would have been more accurate and much more realistic\nThis is also a good example to showcase how a machine learning model is only as useful as the features selected and in-order to select the right features depends on the knowledge one has of that data set!"
  },
  {
    "objectID": "my-site.html",
    "href": "my-site.html",
    "title": "my-site",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "my-site.html#quarto",
    "href": "my-site.html#quarto",
    "title": "my-site",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Finance/index.html",
    "href": "Finance/index.html",
    "title": "Financial Analysis",
    "section": "",
    "text": "Amdocs Limited Stock Analysis and Valuation\n\n\nAmdocs LTD To Unlock Value From 5G IoT Connectivity Market.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Corporation Stock Analysis and Valuation\n\n\nMicrosoft’s Growth Potential In The Cloud Industry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFootlocker Inc Stock Analysis and Valuation\n\n\nFoot Locker, A good business at a cheap price?.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hershey Company Stock Analysis and Valuation\n\n\nThe Hershey Company Potential For Global Dominance.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Finance/2021-01-21-Footlocker-Inc.html",
    "href": "Finance/2021-01-21-Footlocker-Inc.html",
    "title": "Footlocker Inc Stock Analysis and Valuation",
    "section": "",
    "text": "Summary\n\nThe Stock Is Already Up 6% This Year.\nThe Sneaker Culture Is Not An Ephemeral Trend.\nA Low P/E and High Shareholder Yield Makes It An Attractive Investment.\n\n\n\nRead the full analysis on seeking alpha\n\n\nDownload the full excel model\n\n\n\nfootlocker"
  },
  {
    "objectID": "Finance/2020-12-21-Amdocs-LTD.html",
    "href": "Finance/2020-12-21-Amdocs-LTD.html",
    "title": "Amdocs Limited Stock Analysis and Valuation",
    "section": "",
    "text": "Summary\n\nAmdocs role as a vendor is critical for CSP’s to exploit new 5G and IoT opportunities.\nB2C and B2B markets are the key areas for 5G IoT connectivity growth.\nCOVID-19 makes IoT a necessity to the most impacted industries.\nAmdocs top line to reach $8 Billion by 2030 with rich margins to create a huge influx of cash flow.\n\n\n\nRead the full analysis on seeking alpha\n\n\nDownload the full excel model\n\n\n\namdocs-logo"
  },
  {
    "objectID": "Finance/2020-12-25-Microsoft-Corp.html",
    "href": "Finance/2020-12-25-Microsoft-Corp.html",
    "title": "Microsoft Corporation Stock Analysis and Valuation",
    "section": "",
    "text": "Summary\n\nAZURE growing faster than AWS and GCP.\nSaas is at the center of the digital economy.\nThe Stock Price could potential grow by more than 40%.\n\n\n\nRead the full analysis on seeking alpha\n\n\nDownload the full excel model\n\n\n\nmicrosoft logo"
  },
  {
    "objectID": "Finance/2024-10-12-HSY.html",
    "href": "Finance/2024-10-12-HSY.html",
    "title": "The Hershey Company Stock Analysis and Valuation",
    "section": "",
    "text": "Read the full analysis on my LinkedIn\n\n\n\nhsy-logo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monewmetrics",
    "section": "",
    "text": "Monewmetrics is centered on three core areas:\nQuantitative Research: I analyze financial data and apply advanced statistical models such as linear regression and autoregressive time series models, to generate predictive insights. Python is my primary tool, given its robust libraries for data analysis, machine learning, and statistical modeling.\nStatistics & Probability: I explore a wide range of statistical concepts, I cover foundational topics like data location and dispersion measures and will cover more advanced principles, such as regression assumptions and their applications.\nFinancial Analysis: Through my LinkedIn or SeekingAlpha,I post comprehensive financial analysis on companies, both quantitatively and qualitatively. This includes 3-statement financial modeling, scenario and sensitivity analysis, DCF valuation, and deeper insights into business models, competitive advantages, industry trends, market positioning and more.\nDISCLAIMER! Before proceeding, please make sure that you note the following important information:\nNOT FINANCIAL ADVICE! My content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog and wish to rely upon in making any investment decision or otherwise.\nI accept no liability whatsoever for any loss or gain you may incur."
  },
  {
    "objectID": "Quant/2022-05-07-Tech-Stocks-Portfolio.html",
    "href": "Quant/2022-05-07-Tech-Stocks-Portfolio.html",
    "title": "Portfolio Analysis, Efficient Frontier & Monte Carlo",
    "section": "",
    "text": "In this notebook, I’ll be constructing and analyzing an equal-weighted portfolio which will constitute a list of my favorite large-cap tech stocks that I had already screened using various metrics like debt to asset ratio, investor yield and other metrics. I will then use a monte carlo simulation to find the optimal weightings of the tech stocks which i will compare against certain benchmarks and pick the best one out of the lot\n\n\ntoc: true\nbadges: true\ncategories:[Quantitative Research]\nimage:images/pic.png\nuse_plotly: true\n\n\nDISCLAIMER!\nBefore proceeding, please make sure that you note the following important information:\nNOT FINANCIAL ADVICE!\nMy content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument, to make any investment, or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur\n\n#Importing all the relevant librabries and modules\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom pandas_datareader import data as wb\nfrom chart_studio import plotly as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom IPython.display import HTML\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)  \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-darkgrid')\n\n\n\nPortfolio construction and analysis:\n\n# importing the data from yahoo finance and exploring the data\nassets = ['TXN','CSCO','INTC','AAPL','MSFT',\n          'NVDA','INFY','INTU','SAP','ADI',\n          'ANSS','CRM','ADBE','FB','AMD',\n          'AMZN','MA','VMW','GOOG','SNPS']\npf_data = pd.DataFrame()\nfor a in assets:\n    pf_data[a] = yf.download(a, start=\"2012-05-20\", end=\"2021-12-31\", index_col = 'Date', parse_dates=True)['Adj Close'] \n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\npf_data.head()\n\n\n\n\n\n\n\n\nTXN\nCSCO\nINTC\nAAPL\nMSFT\nNVDA\nINFY\nINTU\nSAP\nADI\nANSS\nCRM\nADBE\nFB\nAMD\nAMZN\nMA\nVMW\nGOOG\nSNPS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012-05-21\n22.610977\n12.278630\n19.480406\n17.139450\n24.290533\n2.821950\n4.160825\n50.348274\n49.764915\n28.088888\n61.000000\n37.262501\n32.009998\n34.029999\n6.30\n218.110001\n38.773224\n69.075218\n305.908386\n28.040001\n\n\n2012-05-22\n22.518942\n12.322823\n19.391010\n17.007828\n24.298697\n2.787507\n4.215657\n51.022617\n49.680264\n28.034071\n61.750000\n37.362499\n32.009998\n31.000000\n6.16\n215.330002\n39.043331\n68.748131\n299.278229\n28.040001\n\n\n2012-05-23\n22.350197\n12.293363\n18.951487\n17.422827\n23.767975\n2.856392\n4.168488\n51.460045\n49.773380\n27.940096\n62.119999\n37.652500\n32.180000\n32.000000\n6.08\n217.279999\n39.388008\n69.177422\n303.592072\n28.200001\n\n\n2012-05-24\n22.158449\n12.072392\n19.107927\n17.262810\n23.735321\n2.780620\n4.243171\n51.387135\n48.842247\n28.190681\n61.689999\n36.552502\n31.540001\n33.029999\n6.02\n215.240005\n39.646744\n65.027603\n300.702881\n29.850000\n\n\n2012-05-25\n22.196800\n12.028200\n19.174969\n17.170284\n23.727154\n2.847208\n4.172420\n51.441814\n48.308960\n28.339464\n62.090000\n36.750000\n31.600000\n31.910000\n6.22\n212.889999\n39.092426\n64.066803\n294.660553\n29.889999\n\n\n\n\n\n\n\n\n#Check to see if there are any missing values\npf_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2420 entries, 2012-05-21 to 2021-12-30\nData columns (total 20 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   TXN     2420 non-null   float64\n 1   CSCO    2420 non-null   float64\n 2   INTC    2420 non-null   float64\n 3   AAPL    2420 non-null   float64\n 4   MSFT    2420 non-null   float64\n 5   NVDA    2420 non-null   float64\n 6   INFY    2420 non-null   float64\n 7   INTU    2420 non-null   float64\n 8   SAP     2420 non-null   float64\n 9   ADI     2420 non-null   float64\n 10  ANSS    2420 non-null   float64\n 11  CRM     2420 non-null   float64\n 12  ADBE    2420 non-null   float64\n 13  FB      2420 non-null   float64\n 14  AMD     2420 non-null   float64\n 15  AMZN    2420 non-null   float64\n 16  MA      2420 non-null   float64\n 17  VMW     2420 non-null   float64\n 18  GOOG    2420 non-null   float64\n 19  SNPS    2420 non-null   float64\ndtypes: float64(20)\nmemory usage: 397.0 KB\n\n\nI’ll use a built-in method in DataFrame that computes the percent change from one row to another\n\n#Calculate the daily returns of the stocks in the portfolio\nreturns = pf_data.pct_change(1).dropna()\nreturns\n\n\n\n\n\n\n\n\nTXN\nCSCO\nINTC\nAAPL\nMSFT\nNVDA\nINFY\nINTU\nSAP\nADI\nANSS\nCRM\nADBE\nFB\nAMD\nAMZN\nMA\nVMW\nGOOG\nSNPS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012-05-22\n-0.004070\n0.003599\n-0.004589\n-0.007679\n0.000336\n-0.012206\n0.013178\n0.013394\n-0.001701\n-0.001952\n0.012295\n0.002684\n0.000000\n-0.089039\n-0.022222\n-0.012746\n0.006966\n-0.004735\n-0.021674\n0.000000\n\n\n2012-05-23\n-0.007493\n-0.002391\n-0.022666\n0.024400\n-0.021842\n0.024712\n-0.011189\n0.008573\n0.001874\n-0.003352\n0.005992\n0.007762\n0.005311\n0.032258\n-0.012987\n0.009056\n0.008828\n0.006244\n0.014414\n0.005706\n\n\n2012-05-24\n-0.008579\n-0.017975\n0.008255\n-0.009184\n-0.001374\n-0.026527\n0.017916\n-0.001417\n-0.018707\n0.008969\n-0.006922\n-0.029214\n-0.019888\n0.032187\n-0.009868\n-0.009389\n0.006569\n-0.059988\n-0.009517\n0.058511\n\n\n2012-05-25\n0.001731\n-0.003661\n0.003509\n-0.005360\n-0.000344\n0.023947\n-0.016674\n0.001064\n-0.010919\n0.005278\n0.006484\n0.005403\n0.001902\n-0.033909\n0.033223\n-0.010918\n-0.013981\n-0.014775\n-0.020094\n0.001340\n\n\n2012-05-29\n0.014513\n0.015922\n0.013598\n0.017749\n0.017206\n0.025806\n0.023316\n0.003189\n0.020151\n0.019619\n0.015623\n-0.008912\n0.014240\n-0.096208\n0.038585\n0.008737\n0.005048\n0.009253\n0.004750\n0.009368\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-12-23\n0.002514\n0.012189\n0.006671\n0.003644\n0.004472\n0.008163\n0.008153\n0.006412\n0.004878\n0.002148\n0.007363\n0.001345\n0.010000\n0.014495\n0.015707\n0.000184\n0.008672\n0.012912\n0.001317\n0.003618\n\n\n2021-12-27\n0.023693\n0.018304\n0.012278\n0.022975\n0.023186\n0.044028\n0.024262\n0.026836\n0.008638\n0.016275\n0.018905\n0.020384\n0.014150\n0.032633\n0.056247\n-0.008178\n-0.000749\n-0.005219\n0.006263\n0.025782\n\n\n2021-12-28\n-0.003857\n0.001734\n-0.003466\n-0.005767\n-0.003504\n-0.020133\n-0.001184\n-0.004580\n0.001557\n-0.006212\n-0.003161\n-0.011034\n-0.014402\n0.000116\n-0.007839\n0.005844\n0.001304\n0.010320\n-0.010914\n-0.009159\n\n\n2021-12-29\n-0.001518\n0.006768\n0.001353\n0.000502\n0.002051\n-0.010586\n0.003162\n-0.002693\n-0.010388\n0.006537\n-0.006977\n-0.003562\n-0.000123\n-0.009474\n-0.031929\n-0.008555\n0.001414\n0.003405\n0.000386\n0.003466\n\n\n2021-12-30\n-0.007337\n-0.005316\n-0.001736\n-0.006578\n-0.007691\n-0.013833\n0.001182\n-0.007206\n0.002571\n-0.004216\n-0.003390\n0.003104\n0.002178\n0.004141\n-0.020977\n-0.003289\n-0.000830\n-0.005260\n-0.003427\n-0.007043\n\n\n\n\n2419 rows × 20 columns\n\n\n\nI expected the stocks in this portfolio to have a large number of positive correlations considering they are all part of the same sector in fact no pair is negatively correlated\nThe top 3 most correlated stocks are: Analog devices and Texas Instruments which have a strong positive correlation with 0.80 both companies are in the business of designing and fabrication of semiconductors and semiconductor devices which is a sub-industry of the overall technology sector,\nThe other two sets of stocks have a moderate positive correlation which is Ansys Inc, and Synopsys inc with 0.69 and Synopsys inc again but this time with financial software company Intuit inc with 0.66\n\ncorr = returns.corr()\nfig = px.imshow(corr)\nfig.update_layout(width=1000, height=800)\nfig.update_layout(template = \"plotly_dark\", title = 'The Correlation coefficient of the Assets in the Portfolio') \nfig.show()\n\n                                                \n\n\n\ncorr.unstack().sort_values().drop_duplicates()\n\nFB    AMD     0.234715\nAMD   INFY    0.235012\nFB    INFY    0.236786\nINFY  AMZN    0.248299\nVMW   AMD     0.264527\n                ...   \nMSFT  ADBE    0.662720\nSNPS  INTU    0.672901\nANSS  SNPS    0.698040\nADI   TXN     0.803991\nTXN   TXN     1.000000\nLength: 191, dtype: float64\n\n\n\n\nCreating an equal weight (EW) portfolio:\nEqual weight is a type of proportional measuring method that gives the same importance to each stock in a portfolio, index, or index fund. So stocks of the smallest companies are given equal statistical significance, or weight, to the largest companies when it comes to evaluating the overall group’s performance.\n\n#Equal weighted portfolio\nN = len(returns.columns)\nequal_weights = N * [1/N] # Shows 1/20, 20 times. Its not multiplication, but repetition! 20*[\"A\"]\nequal_weights\n\n[0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05]\n\n\n\n#Calculate portfolio daily return\nportfolio_return = returns.dot(equal_weights)\nportfolio_return\n\nDate\n2012-05-22   -0.006508\n2012-05-23    0.003661\n2012-05-24   -0.004807\n2012-05-25   -0.002338\n2012-05-29    0.008578\n                ...   \n2021-12-23    0.006743\n2021-12-27    0.019035\n2021-12-28   -0.004217\n2021-12-29   -0.002838\n2021-12-30   -0.004248\nLength: 2419, dtype: float64\n\n\n\n#Formatting the dates to column and not index\npf_data.index\ndates = pf_data.index.to_frame().reset_index(drop=True)\n\nThe returns were noticeably volatile in 2018 November as that year a lot was happening like the federal reserve interest hike but the most notable event was a lot of the big tech were under scrutiny at the time and considering this is a tech portfolio the volatility shouldn’t be surprising\nAnother noticeable moment here is the pandemic in 2020, volatility was extremely high, in fact, On March 16, 2020, the VIX closed at a record high of 82.69 The markets were tumbling and a lot of trades were being made, some were covering short positions while others buying “the dip” and last but not least you have countless algorithims and retail traders day trading and taking advantage of the high volatility\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=dates['Date'], y=portfolio_return,\n                    mode='lines',\n                    line=dict(color='firebrick',width=2),\n                    name='lines'))\nfig.update_layout(template = \"plotly_dark\")\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));                    \n\n                                                \n\n\n\n#calculating the cummulative retrun of the equal weight portfolio\ncum_equal_returns =   (1 + portfolio_return).cumprod() - 1\ncum_equal_returns_perc = pd.Series(100 * cum_equal_returns)\n\nThe EW has done pretty well returning more than 1000%!\n\nfig = go.Figure([go.Scatter(x=dates['Date'], y=cum_equal_returns_perc)])\nfig.update_layout(template = \"plotly_dark\", title = 'Cummulative % Return') \nfig.show()\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\n                                                \n\n\n\n\nSharpe Ratio\nIn the next step i am going to calcluate sharpe ratio, but first we need the annual volatility, annual return and risk free rate\n\n#Calculating the mean return and volatility\nER = portfolio_return.mean()\nSTD = portfolio_return.std()\n\n\n#Annualzing the volatility\nASTD = STD * 252 ** 0.5\nASTD\n\n0.21152541130551866\n\n\n\n#Annualzing the expected return \nAER = ER * 252 \nAER\n\n0.2784416783450839\n\n\n\n#Annual sharpe ratio\nrf = 0.03 #risk free rate is the 10 year trasury bond as of april 2022\nexcess_return = AER - rf\nSR = excess_return/ASTD\nSR\n\n1.1745240291070507\n\n\nA Sharpe ratio of 1.17 is not the best but also considering that tech stocks are very volatile maybe there is a weight combination that would have a higher Sharpe ratio and/or lower volatility or even a higher expected return and that is what I’ll try to uncover in the next section\n\n\nModern Portfolio Theory & Monte Carlo simulation:\nMonte Carlo simulations are used to model the probability of different outcomes in a process that cannot easily be predicted due to the intervention of random variables. It is a technique used to understand the impact of risk and uncertainty in prediction and forecasting models.\nModern portfolio theory refers to the quantitative practice of asset allocation that maximizes projected (ex ante) return for a portfolio while holding constant its overall exposure to risk. Or, inversely, minimizing overall risk for a given target portfolio return. The theory considers the covariance of constituent assets or asset classes within a portfolio, and the impact of an asset allocation change on the overall expected risk/return profile of the portfolio.\nThe theory was originally proposed by nobel-winning economist Harry Markowitz in the 1952 Journal of Finance, and is now a cornerstone of portfolio management practice. Modern portfolio theory generally supports a practice of diversifying toward a mix of assets and asset classes with a low degree of mutual correlation.\nHence, I’m going to find the optimal portfolio using Monte Carlo simulations by building thousands of portfolios, using randomly assigned weights, and visualizing the results.\n\n#Saving a variable which will have the number of stocks in my portfolio which i will use later\nnum_assets = len(pf_data.columns)\nnum_assets\n\n20\n\n\n\n#calculating the log returns of the stocks in the portfolio\nlog_returns = np.log(pf_data/pf_data.shift(1))\nlog_returns.head()\n\n\n\n\n\n\n\n\nTXN\nCSCO\nINTC\nAAPL\nMSFT\nNVDA\nINFY\nINTU\nSAP\nADI\nANSS\nCRM\nADBE\nFB\nAMD\nAMZN\nMA\nVMW\nGOOG\nSNPS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012-05-21\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2012-05-22\n-0.004079\n0.003593\n-0.004599\n-0.007709\n0.000336\n-0.012280\n0.013093\n0.013305\n-0.001702\n-0.001953\n0.012220\n0.002680\n0.000000\n-0.093255\n-0.022473\n-0.012828\n0.006942\n-0.004746\n-0.021912\n0.000000\n\n\n2012-05-23\n-0.007521\n-0.002394\n-0.022927\n0.024107\n-0.022083\n0.024411\n-0.011252\n0.008536\n0.001873\n-0.003358\n0.005974\n0.007732\n0.005297\n0.031749\n-0.013072\n0.009015\n0.008790\n0.006225\n0.014311\n0.005690\n\n\n2012-05-24\n-0.008616\n-0.018139\n0.008221\n-0.009227\n-0.001375\n-0.026885\n0.017758\n-0.001418\n-0.018885\n0.008929\n-0.006946\n-0.029650\n-0.020089\n0.031680\n-0.009917\n-0.009433\n0.006548\n-0.061863\n-0.009562\n0.056863\n\n\n2012-05-25\n0.001729\n-0.003668\n0.003502\n-0.005374\n-0.000344\n0.023665\n-0.016815\n0.001063\n-0.010979\n0.005264\n0.006463\n0.005389\n0.001901\n-0.034497\n0.032683\n-0.010978\n-0.014080\n-0.014886\n-0.020299\n0.001339\n\n\n\n\n\n\n\n\n#the covariance matrix of the stocks\ncov = log_returns.cov()\ncov\n\n\n\n\n\n\n\n\nTXN\nCSCO\nINTC\nAAPL\nMSFT\nNVDA\nINFY\nINTU\nSAP\nADI\nANSS\nCRM\nADBE\nFB\nAMD\nAMZN\nMA\nVMW\nGOOG\nSNPS\n\n\n\n\nTXN\n0.000283\n0.000149\n0.000206\n0.000162\n0.000165\n0.000267\n0.000120\n0.000163\n0.000135\n0.000244\n0.000171\n0.000170\n0.000175\n0.000147\n0.000268\n0.000141\n0.000157\n0.000150\n0.000140\n0.000161\n\n\nCSCO\n0.000149\n0.000248\n0.000156\n0.000132\n0.000144\n0.000181\n0.000104\n0.000137\n0.000117\n0.000147\n0.000131\n0.000150\n0.000143\n0.000112\n0.000170\n0.000115\n0.000141\n0.000148\n0.000117\n0.000125\n\n\nINTC\n0.000206\n0.000156\n0.000349\n0.000157\n0.000176\n0.000243\n0.000121\n0.000161\n0.000134\n0.000202\n0.000161\n0.000155\n0.000166\n0.000145\n0.000229\n0.000131\n0.000152\n0.000150\n0.000138\n0.000152\n\n\nAAPL\n0.000162\n0.000132\n0.000157\n0.000317\n0.000167\n0.000223\n0.000107\n0.000154\n0.000123\n0.000165\n0.000157\n0.000161\n0.000167\n0.000169\n0.000218\n0.000154\n0.000150\n0.000128\n0.000148\n0.000145\n\n\nMSFT\n0.000165\n0.000144\n0.000176\n0.000167\n0.000262\n0.000233\n0.000109\n0.000178\n0.000135\n0.000164\n0.000170\n0.000189\n0.000199\n0.000160\n0.000212\n0.000168\n0.000162\n0.000147\n0.000168\n0.000161\n\n\nNVDA\n0.000267\n0.000181\n0.000243\n0.000223\n0.000233\n0.000664\n0.000142\n0.000232\n0.000170\n0.000276\n0.000236\n0.000260\n0.000262\n0.000227\n0.000473\n0.000216\n0.000208\n0.000203\n0.000201\n0.000231\n\n\nINFY\n0.000120\n0.000104\n0.000121\n0.000107\n0.000109\n0.000142\n0.000334\n0.000124\n0.000107\n0.000121\n0.000116\n0.000124\n0.000119\n0.000100\n0.000159\n0.000085\n0.000124\n0.000108\n0.000096\n0.000107\n\n\nINTU\n0.000163\n0.000137\n0.000161\n0.000154\n0.000178\n0.000232\n0.000124\n0.000282\n0.000132\n0.000166\n0.000182\n0.000196\n0.000202\n0.000159\n0.000213\n0.000146\n0.000178\n0.000143\n0.000149\n0.000176\n\n\nSAP\n0.000135\n0.000117\n0.000134\n0.000123\n0.000135\n0.000170\n0.000107\n0.000132\n0.000257\n0.000138\n0.000136\n0.000152\n0.000150\n0.000122\n0.000181\n0.000119\n0.000139\n0.000134\n0.000118\n0.000124\n\n\nADI\n0.000244\n0.000147\n0.000202\n0.000165\n0.000164\n0.000276\n0.000121\n0.000166\n0.000138\n0.000325\n0.000178\n0.000179\n0.000183\n0.000151\n0.000277\n0.000135\n0.000167\n0.000151\n0.000142\n0.000168\n\n\nANSS\n0.000171\n0.000131\n0.000161\n0.000157\n0.000170\n0.000236\n0.000116\n0.000182\n0.000136\n0.000178\n0.000286\n0.000198\n0.000202\n0.000150\n0.000226\n0.000154\n0.000166\n0.000150\n0.000147\n0.000184\n\n\nCRM\n0.000170\n0.000150\n0.000155\n0.000161\n0.000189\n0.000260\n0.000124\n0.000196\n0.000152\n0.000179\n0.000198\n0.000434\n0.000248\n0.000214\n0.000246\n0.000192\n0.000187\n0.000192\n0.000169\n0.000179\n\n\nADBE\n0.000175\n0.000143\n0.000166\n0.000167\n0.000199\n0.000262\n0.000119\n0.000202\n0.000150\n0.000183\n0.000202\n0.000248\n0.000340\n0.000192\n0.000249\n0.000190\n0.000177\n0.000170\n0.000171\n0.000189\n\n\nFB\n0.000147\n0.000112\n0.000145\n0.000169\n0.000160\n0.000227\n0.000100\n0.000159\n0.000122\n0.000151\n0.000150\n0.000214\n0.000192\n0.000521\n0.000201\n0.000199\n0.000161\n0.000141\n0.000186\n0.000149\n\n\nAMD\n0.000268\n0.000170\n0.000229\n0.000218\n0.000212\n0.000473\n0.000159\n0.000213\n0.000181\n0.000277\n0.000226\n0.000246\n0.000249\n0.000201\n0.001315\n0.000217\n0.000204\n0.000211\n0.000167\n0.000230\n\n\nAMZN\n0.000141\n0.000115\n0.000131\n0.000154\n0.000168\n0.000216\n0.000085\n0.000146\n0.000119\n0.000135\n0.000154\n0.000192\n0.000190\n0.000199\n0.000217\n0.000350\n0.000152\n0.000132\n0.000178\n0.000141\n\n\nMA\n0.000157\n0.000141\n0.000152\n0.000150\n0.000162\n0.000208\n0.000124\n0.000178\n0.000139\n0.000167\n0.000166\n0.000187\n0.000177\n0.000161\n0.000204\n0.000152\n0.000276\n0.000147\n0.000154\n0.000154\n\n\nVMW\n0.000150\n0.000148\n0.000150\n0.000128\n0.000147\n0.000203\n0.000108\n0.000143\n0.000134\n0.000151\n0.000150\n0.000192\n0.000170\n0.000141\n0.000211\n0.000132\n0.000147\n0.000471\n0.000127\n0.000134\n\n\nGOOG\n0.000140\n0.000117\n0.000138\n0.000148\n0.000168\n0.000201\n0.000096\n0.000149\n0.000118\n0.000142\n0.000147\n0.000169\n0.000171\n0.000186\n0.000167\n0.000178\n0.000154\n0.000127\n0.000250\n0.000135\n\n\nSNPS\n0.000161\n0.000125\n0.000152\n0.000145\n0.000161\n0.000231\n0.000107\n0.000176\n0.000124\n0.000168\n0.000184\n0.000179\n0.000189\n0.000149\n0.000230\n0.000141\n0.000154\n0.000134\n0.000135\n0.000241\n\n\n\n\n\n\n\n\nnum_ports = 20000 #the number of trials i will run\nall_weights = np.zeros((num_ports,num_assets))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    #weigths\n    weights = np.array(np.random.random(num_assets))\n    weights = weights/np.sum(weights)\n\n    #save weigths\n    all_weights[ind,:] = weights\n\n    #expected return\n    ret_arr[ind] = np.sum((log_returns.mean() * weights) * 250)\n\n    #expected volatility\n    vol_arr[ind] = np.sqrt(np.dot(weights.T,np.dot(log_returns.cov()*250,weights)))\n\n    #sharpe ratio\n    sharpe_arr[ind] = (ret_arr[ind] - rf)/vol_arr[ind]\n\nAfter the monte carlo is done it’s time to inspect and locate the results and look at the weightings of the portfolios we need a portfolio that might be better than my initial equal weighted portfolio, rememebr returns alone are not the objective but also volatility, we want the highest return for the lowest volatility possible hence the highest sharpe ratio\nIn the next step i will be creating a data frame that will contain not just the weigthings but also the expected return, volatility and even the sharpe ratio of all the portfolios generated which will help me locate where the optimal or tangency portfolio, the portfolio with minimum volatility and the portfolio with maximum expected return are at in the weightings that were generated\n\n#Each row is a portfolio with the coresponding details \ndata = pd.DataFrame({'Return': ret_arr, 'Volatility': vol_arr, 'Sharpe Ratio': sharpe_arr})#(ret_arr - rf) /vol_arr})\nfor counter, symbol in enumerate(pf_data.columns.tolist()):\n    data[symbol + 'weight'] = [w[counter]for w in all_weights]\n\nportfolios = pd.DataFrame(data)\nportfolios\n\n\n\n\n\n\n\n\nReturn\nVolatility\nSharpe Ratio\nTXNweight\nCSCOweight\nINTCweight\nAAPLweight\nMSFTweight\nNVDAweight\nINFYweight\n...\nANSSweight\nCRMweight\nADBEweight\nFBweight\nAMDweight\nAMZNweight\nMAweight\nVMWweight\nGOOGweight\nSNPSweight\n\n\n\n\n0\n0.235060\n0.213741\n0.959388\n0.061536\n0.091934\n0.040965\n0.067888\n0.076020\n0.054105\n0.049063\n...\n0.003004\n0.092469\n0.040359\n0.081557\n0.032928\n0.003992\n0.066299\n0.014090\n0.053930\n0.019123\n\n\n1\n0.229757\n0.219309\n0.910850\n0.061311\n0.031462\n0.050643\n0.049502\n0.001270\n0.077914\n0.029312\n...\n0.033376\n0.040043\n0.078451\n0.053226\n0.073149\n0.071961\n0.062006\n0.077717\n0.030547\n0.007948\n\n\n2\n0.239854\n0.218571\n0.960119\n0.022931\n0.034186\n0.000097\n0.095811\n0.055255\n0.098486\n0.032107\n...\n0.039784\n0.038417\n0.094715\n0.038167\n0.052099\n0.029348\n0.002343\n0.069246\n0.053960\n0.017094\n\n\n3\n0.223821\n0.206283\n0.939588\n0.021978\n0.088269\n0.028764\n0.031951\n0.023287\n0.044314\n0.061802\n...\n0.084773\n0.025944\n0.023086\n0.070774\n0.019001\n0.004489\n0.067659\n0.011433\n0.067834\n0.080970\n\n\n4\n0.252654\n0.220076\n1.011718\n0.032829\n0.038141\n0.031621\n0.038156\n0.032453\n0.071135\n0.023667\n...\n0.049051\n0.001107\n0.104740\n0.069692\n0.071773\n0.051470\n0.017169\n0.034197\n0.086193\n0.106864\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19995\n0.236976\n0.217382\n0.952134\n0.065015\n0.062946\n0.031630\n0.024278\n0.054483\n0.060273\n0.038499\n...\n0.071690\n0.029728\n0.083362\n0.078441\n0.073044\n0.005775\n0.036472\n0.059114\n0.075507\n0.040374\n\n\n19996\n0.211920\n0.207550\n0.876511\n0.059174\n0.066972\n0.080858\n0.028952\n0.050625\n0.050136\n0.074528\n...\n0.066560\n0.097057\n0.004428\n0.037434\n0.029613\n0.009752\n0.050356\n0.032781\n0.063095\n0.061517\n\n\n19997\n0.218922\n0.215106\n0.878273\n0.020343\n0.000088\n0.041725\n0.001719\n0.004495\n0.014127\n0.094459\n...\n0.033807\n0.010043\n0.056615\n0.023852\n0.106904\n0.110114\n0.094245\n0.093785\n0.106817\n0.022204\n\n\n19998\n0.231003\n0.213849\n0.939929\n0.070349\n0.038615\n0.025927\n0.055969\n0.053822\n0.084375\n0.079402\n...\n0.067857\n0.047777\n0.030630\n0.029430\n0.042781\n0.011784\n0.023835\n0.074925\n0.056096\n0.032982\n\n\n19999\n0.214351\n0.207301\n0.889289\n0.038387\n0.083181\n0.062824\n0.052065\n0.058076\n0.032348\n0.044598\n...\n0.071942\n0.011572\n0.003619\n0.072106\n0.048186\n0.023791\n0.094538\n0.051598\n0.099134\n0.007197\n\n\n\n\n20000 rows × 23 columns\n\n\n\nThe tangency portfolio:\nThe tangency or maximum Sharpe ratio portfolio in the Markowitz procedure possesses the highest potential return-for-risk tradeoff.\n\n#optimal risky portfolio/ Tangency Portfolio / Highest sharpe\noptimal_risky_portfolio = portfolios.iloc[portfolios['Sharpe Ratio'].idxmax()]\noptimal_risky_portfolio\n\nReturn          0.263745\nVolatility      0.215477\nSharpe Ratio    1.084784\nTXNweight       0.041522\nCSCOweight      0.060248\nINTCweight      0.001923\nAAPLweight      0.089765\nMSFTweight      0.099979\nNVDAweight      0.083038\nINFYweight      0.066468\nINTUweight      0.103850\nSAPweight       0.026235\nADIweight       0.014330\nANSSweight      0.024099\nCRMweight       0.008061\nADBEweight      0.076826\nFBweight        0.000352\nAMDweight       0.056392\nAMZNweight      0.091473\nMAweight        0.010688\nVMWweight       0.009695\nGOOGweight      0.061586\nSNPSweight      0.073470\nName: 13834, dtype: float64\n\n\nMinim vol is also known as the minimum variance portfolio:\nThe minimum variance portfolio (mvp) is the portfolio that provides the lowest variance (standard deviation) among all possible portfolios of risky assets.\n\n#Portfolio with minimum volatility\nmin_vol_port = portfolios.iloc[portfolios['Volatility'].idxmin()]\nmin_vol_port\n\nReturn          0.202356\nVolatility      0.197514\nSharpe Ratio    0.872627\nTXNweight       0.042340\nCSCOweight      0.105379\nINTCweight      0.077549\nAAPLweight      0.026698\nMSFTweight      0.002516\nNVDAweight      0.014645\nINFYweight      0.102012\nINTUweight      0.034337\nSAPweight       0.058483\nADIweight       0.076789\nANSSweight      0.010081\nCRMweight       0.010085\nADBEweight      0.006261\nFBweight        0.039513\nAMDweight       0.017248\nAMZNweight      0.033863\nMAweight        0.028866\nVMWweight       0.053479\nGOOGweight      0.132173\nSNPSweight      0.127684\nName: 5947, dtype: float64\n\n\nMax return portfolio: The portfolio with the highest return regardless of risk\n\n#Portfolio with the maximum expected return\nmax_er_port = portfolios.iloc[portfolios['Return'].idxmax()]\nmax_er_port\n\nReturn          0.267209\nVolatility      0.228768\nSharpe Ratio    1.036900\nTXNweight       0.086701\nCSCOweight      0.024911\nINTCweight      0.005748\nAAPLweight      0.085139\nMSFTweight      0.021823\nNVDAweight      0.135536\nINFYweight      0.039539\nINTUweight      0.023250\nSAPweight       0.006508\nADIweight       0.000708\nANSSweight      0.064148\nCRMweight       0.091271\nADBEweight      0.014365\nFBweight        0.006770\nAMDweight       0.076369\nAMZNweight      0.109782\nMAweight        0.038671\nVMWweight       0.031825\nGOOGweight      0.061691\nSNPSweight      0.075245\nName: 6471, dtype: float64\n\n\nWith a scatter plot I’ll be able to visually see the portfolios and where they lay on the frontier, but remember the correlation of the stocks, there was virtually no negative correlation and modern portfolio theory is about diversifying with UNCORELATED assets, hence I do not expect the plot to form the usual bullet like shape, but i will still be able to see the optimal portfolios across the edges of the fronteir\n\n# creating a scatter plot and pinpointing the location of the above portfolios\nplt.figure(figsize=(20,10))\nplt.scatter(portfolios['Volatility'],portfolios['Return'],c=sharpe_arr,cmap='RdBu')#ret_arr,vol_arr\nplt.colorbar(label='Sharpe Ratio')\nplt.xlabel('Risk (Volatility)')\nplt.ylabel('Expected Returns')\n\nplt.scatter(optimal_risky_portfolio[1],optimal_risky_portfolio[0], c='green', s=80)\nplt.scatter(min_vol_port[1],min_vol_port[0], c='purple', s=80)#\nplt.scatter(max_er_port[1],max_er_port[0], c='yellow', s=80)\nplt.style.use('dark_background')\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\nC:\\Users\\one\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:4: MatplotlibDeprecationWarning:\n\nAuto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n\n\n\n\n\n\n\n\n\n\nNow that we know the weights of each portfolio, I’ll Assign the weights to the stocks and check the cumulative returns of each of the portfolios\nBut, NOTE: You’ve might have noticed from the observations produced by the simulation, the tangency portfolio has a lower sharp than my initial equal weight portfolio which now means based on ALL the observations I have, the EW portfolio is my tangency/Optimal portfolio and i will treat it as my optimal portfolio going forward\n\n#min volatility portfolio weights\nmin_vol_weights = all_weights[5947,:]\nmin_vol_weights\n\narray([0.04234009, 0.10537944, 0.07754858, 0.02669783, 0.00251591,\n       0.0146454 , 0.10201163, 0.03433662, 0.0584827 , 0.07678887,\n       0.0100807 , 0.0100849 , 0.00626119, 0.03951333, 0.01724793,\n       0.03386276, 0.02886554, 0.05347914, 0.13217338, 0.12768405])\n\n\n\nmin_vol_port_return = returns.dot(min_vol_weights)\ncum_minvol_returns =   (1 + min_vol_port_return).cumprod() - 1\ncum_minvol_returns_perc = pd.Series(100 * cum_minvol_returns)\n\n#Plot\nfig = go.Figure([go.Scatter(x=dates['Date'], y=cum_minvol_returns_perc)])\nfig.update_layout(template = \"plotly_dark\", title = 'Cummulative % Return of the minimum variance portfolio') \nfig.show()\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\n                                                \n\n\n\n#max volatility portfolio weights\nmax_er_weights = all_weights[6471,:]\nmax_er_weights\n\narray([0.08670101, 0.02491136, 0.00574761, 0.08513945, 0.02182338,\n       0.13553581, 0.03953875, 0.02325034, 0.00650826, 0.00070845,\n       0.06414761, 0.09127111, 0.01436485, 0.00676964, 0.07636874,\n       0.1097819 , 0.03867063, 0.03182504, 0.06169064, 0.07524542])\n\n\n\nmax_er_port_return = returns.dot(max_er_weights)\ncum_maxer_returns =   (1 + max_er_port_return).cumprod() - 1\ncum_maxer_returns_perc = pd.Series(100 * cum_maxer_returns)\n\n#Plot\nfig = go.Figure([go.Scatter(x=dates['Date'], y=cum_maxer_returns_perc)])\nfig.update_layout(template = \"plotly_dark\", title = 'Cummulative % Return of the maximum expected return portfolio') \nfig.show()\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\n                                                \n\n\n\n\nPortfolio vs benchmarks:\nIt’s time to compare the all the portfolios against certain benchmarks which are going to be the Invesco QQQ fund which is a technology ETF, I chose this particular tech ETF as a benchmark because it has the highest NAV of 135 billion as of April 2022\nI’ll also include the NASDAQ composite as a benchmark as it is widely followed and considered as a benchmark by investors, the Nasdaq composite is even more relevant here because more than 50% of the stocks in the index are technology companies\n\n#Importing the benchmarks data\nQQQ = pd.DataFrame(yf.download('QQQ', start=\"2012-05-20\", end=\"2021-12-31\", index_col = 'Date', parse_dates=True)['Adj Close']) \nNASDAQ = pd.DataFrame(yf.download('^IXIC', start=\"2012-05-20\", end=\"2021-12-31\", index_col = 'Date', parse_dates=True)['Adj Close']) \n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n#calculate daily return, cumm return for each bechmark and place them in their data frames\nfor funds in (QQQ,NASDAQ):\n    funds['Daily Return'] = funds.pct_change(1).dropna()\n    funds['Cumulative Return'] = (1 + funds['Daily Return']).cumprod() - 1\n    funds['Cumulative % Return'] = funds['Cumulative Return'] * 100\n\n#creating a data frame that has all the portfolios and benchmarks cummulative % return\ndata = {'QQQ':QQQ['Cumulative % Return'],\n       'NASDAQ':NASDAQ['Cumulative % Return'],\n       'Optimal Port':cum_equal_returns_perc,\n       'Min Variance':cum_minvol_returns_perc,\n       'Max ER Port':cum_maxer_returns_perc}\nfunds_cumm = pd.DataFrame(data)\nfunds_cumm.reset_index(drop=True, inplace=True)\nfunds_cumm.insert(loc=0, column=\"Dates\", value=dates)\nfunds_cumm.tail()\n\n\n\n\n\n\n\n\nDates\nQQQ\nNASDAQ\nOptimal Port\nMin Variance\nMax ER Port\n\n\n\n\n2415\n2021-12-23\n598.161713\n449.779269\n1057.179336\n765.628694\n1652.657742\n\n\n2416\n2021-12-27\n609.700431\n457.432012\n1079.205886\n780.031449\n1690.668661\n\n\n2417\n2021-12-28\n606.411117\n454.287177\n1074.233314\n776.898982\n1679.724874\n\n\n2418\n2021-12-29\n606.305615\n453.742785\n1070.900768\n777.018843\n1670.698787\n\n\n2419\n2021-12-30\n604.194880\n452.876668\n1065.927132\n773.809387\n1659.601642\n\n\n\n\n\n\n\nBoth benchmarks performed poorly compared to the portfolios but this is largely due to concentration, as the portfolios have only 20 stocks while the benchmarks usually have hundreds of stocks in them, but the portfolio’s strongest point is also its weakest as a concentrated portfolio will probably have a much larger drawdown even during corrections let alone recessions\nThe max expected return portfolio outperformed all the other portfolios and the benchmarks as expected\nWhile my initial EW portfolio now turned into my tangency optimal portfolio faired well by beating both the benchmarks by a mile!\n\nfig = px.line(funds_cumm, x=\"Dates\", y=funds_cumm.columns,\n              hover_data={\"Dates\": \"|%B %d, %Y\"},\n              title='Commulative % Return')\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.update_layout(template = \"plotly_dark\", title = '10 years Cummulative % Return of all tech portfolios and benchmarks')\nfig.show()\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\n                                                \n\n\n\n\nConclusion:\nAssuming I started with a $5,000 in 2012 and invested in the ideal two best portfolios Max return portfolio and Optimal portfolio, by comparison, how much would my investment be by the end of 2021? (without rebalancing)\nThe max expected return portfolio seems to have a better outcome and seems very attractive especially considering the Sharpe ratio difference isn’t that big but this portfolio was picked because of its high expected return unfortunately, it’s not practical due to the presence of large estimation errors in those expected return estimate. As I have estimated them using historical data and have assumed past performance will be the same in the future which is unlikely as businesses change ESPECIALLY in the ever-changing technology industry\nHence the more reliable portfolio based on this research would either be the min vol or EW/Tangency portfolio\n\nInitial_Investment = 5000\n\n\n#Minimum Variance\nMin_ASRr = str(round(portfolios['Sharpe Ratio'][5947],2))\nMin_AERr = str(round(portfolios['Return'][5947]* 100,2)) + '%' \nMin_ASTDr = str(round(portfolios['Volatility'].min()*100,2)) + '%'\ncumm = str(round(cum_minvol_returns_perc[2418],2)) + '%'\nEW_Value = Initial_Investment * (cum_minvol_returns_perc[2418]/100)\nAbsolute_Value = '$' + str(round(EW_Value,2))\nprint('THE MINIMUM VARIANCE PORTFOLIO:')\nprint(f'The annual sharpe ratio of the minimum variance portfolio is {Min_ASRr}')\nprint(f'The annual Volatility of the minimum variance portfolio is {Min_ASTDr}')\nprint(f'The annual Expected Return of the minimum variance portfolio is {Min_AERr}')\nprint(f'The 10 yr cummulative return of the minimum variance portfolio is {cumm}')\nprint(f'A ${Initial_Investment} investment in minimum variance portfolio in 2012 would be worth {Absolute_Value} by the end of 2021')\n\n#Tangency/Optimal Portfolio\nASRr = str(round(SR,2))\nAERr = str(round(AER* 100,2)) + '%' \nASTDr = str(round(ASTD*100,2)) + '%'\ncumm3 = str(round(cum_equal_returns_perc[2418],2)) + '%'\nEW_Value3 = Initial_Investment * (cum_equal_returns_perc[2418]/100)\nAbsolute_Value3 = '$' + str(round(EW_Value3,2))\nprint('\\nTHE OPTIMAL PORTFOLIO:')\nprint(f'The annual sharpe ratio of the optimal portfolio is {ASRr}')\nprint(f'The annual Volatility of the optimal portfolio is {ASTDr}')\nprint(f'The annual Expected Return of the optimal portfolio is {AERr}')\nprint(f'The 10 yr cummulative return of the optimal portfolio is {cumm3}')\nprint(f'A ${Initial_Investment} investment in the optimal portfolio in 2012 would be worth {Absolute_Value3} by the end of 2021')\n\nTHE MINIMUM VARIANCE PORTFOLIO:\nThe annual sharpe ratio of the minimum variance portfolio is 0.87\nThe annual Volatility of the minimum variance portfolio is 19.75%\nThe annual Expected Return of the minimum variance portfolio is 20.24%\nThe 10 yr cummulative return of the minimum variance portfolio is 773.81%\nA $5000 investment in minimum variance portfolio in 2012 would be worth $38690.47 by the end of 2021\n\nTHE OPTIMAL PORTFOLIO:\nThe annual sharpe ratio of the optimal portfolio is 1.17\nThe annual Volatility of the optimal portfolio is 21.15%\nThe annual Expected Return of the optimal portfolio is 27.84%\nThe 10 yr cummulative return of the optimal portfolio is 1065.93%\nA $5000 investment in the optimal portfolio in 2012 would be worth $53296.36 by the end of 2021"
  },
  {
    "objectID": "Quant/2022-07-03-ARIMA-prediction.html",
    "href": "Quant/2022-07-03-ARIMA-prediction.html",
    "title": "Stock Price Forecast Using ARIMA Model",
    "section": "",
    "text": "In this notebook i’m going to forecast the stock price of The Procter & Gamble Company (Ticker PG) using the Auto Regressive Integrated Moving Average (ARIMA) model\n\n\ntoc: true\nbadges: true\ncategories:[Quantitative Research]\nimage:images/time series.png\n\n\nDISCLAIMER!\nBefore proceeding, please make sure that you note the following important information:\nNOT FINANCIAL ADVICE!\nMy content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice, None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument to make any investment or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur\n\n\nIntro on time series:\nA time series is a sequence of data that attaches a time period to each value, the value can be anything measurable that depends on time in some way like rainfall measurements, heart rate measurements, annual retail sales, monthly subscription, trading volumes, security prices, exchange rates and so on.\nAll we need for a time series is a starting point and an ending point(a time period), all time periods must be equal and clearly defined which would result in a constant frequency. The frequency is how often the values of the data set are recorded which could be hourly, daily, monthly, quarterly, etc. There are also no limitations regarding the total time span of our time series.\nTime series analysis comprises methods for analyzing time series data in order to extract meaningful statistics like observing patterns in the data while Time series forecasting is the use of a model to predict future values based on previously observed patterns.\nData can be univariate or multivariate, univariate time series is when we are forecasting the value of a single variable based on patterns observed in its own past history. For example, predicting the closing stock price of apple tomorrow using its own closing price in the past 7 trading days. While multivariate time series is when we are forecasting the value of a single variable considering patterns in parallel time series. For example, predicting the closing stock price of apple tomorrow using the closing price of Microsoft and Nvidia in the past 7 trading days.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom pmdarima.arima.utils import ndiffs\nimport datetime\nfrom pandas_datareader import data as wb\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(\"ignore\")\n%matplotlib inline\n\n\n\nObjective:\nThe American economy contracted an annualized 1.6% in Q1 2022. It was the first contraction since the pandemic-induced recession in 2020 as record trade deficits, supply constraints, worker shortages, and high inflation weigh.\nYear to date, the S&P500 is down approximately 20% this officially puts it in a bear market territory. As fear of recession weighs in, historically the consumer staple sector usually fare well during these downturns as the idea is that companies in these sectors produce goods & services that consumers will buy regardless of the economic conditions.\nHence, the objective of today’s blog is to forecast the stock price of The Procter & Gamble Company (Ticker PG) using the Auto Regressive Integrated Moving Average (ARIMA) model.\nThe reasons why I picked PG is because they have a diverse pool of products mainly 5 segments, Beauty, Grooming, Health Care, Fabric &Home Care, and Baby, Feminine & Family Care, they also operate in a lot of regions with consumers from Latin America, Europe, the Asia Pacific, Greater China, India, the Middle East, and Africa.\nNOTE: depending on the level of risk one is willing to take with his/her portfolio, a safer bet would be to trade the overall sector using an ETF like the Consumer Staples Select Sector SPDR Fund(Ticker XLP) or Vanguard Consumer Staples ETF(Ticker VDC) in order to get exposure of the sector in one’s portfolio.\n\n\nWhat is an ARIMA model:\nARIMA stands for Autoregressive Integrated Moving Average. ARIMA is also known as the Box-Jenkins approach. it is a combination of an AR model, MA model, and differencing (Integration).\n\nThe components of an ARIMA model:\nIn an ARIMA(p,d,q) model, p indicates the number or order of AR terms, d indicates the number or order of differences, and q indicates the number or order of MA terms. The p, d, and q parameters are integers equal to or greater than 0\n\n\nAR:\nA correlation of a variable with itself at different time periods in the past is known as “Autocorrelation”. AR model uses past values to make future predictions. It is indicated by the “p” value in the ARIMA model. The lag “p” term signifies how many prior time periods that each observation is highly correlated to, Increasing “p” (longer lag) would increase the dependency on previous values further.\nif \\(Y\\) is a time series variable and AR(2) then the equation would look like this:\n\n\n\\(Y_{t} = c + \\varphi_{1}Y_{t−1} + \\varphi_{2}Y_{t−2} + ϵ_{t}\\)\nWhere: - \\(c\\) is a constant - \\(Y_{t−1}\\) is the values of \\(Y\\) during the previous period - \\(\\varphi\\) is the magnitude of the autocorrelation - \\(ϵ_{t}\\) is the residuals for the current period (the difference between our prediction for period \\(_{t}\\) and the correct value)\n\n\nMA:\nMA model uses past errors to make a prediction. The “q” term is the number of lagged values of the error term it\nif \\(Y\\) is a time series variable and MA(2) then the equation would look like this:\n\n\n\\(Y_{t} = c + θ_{1}ϵ_{t−1} + θ_{2}ϵ_{t−2} + ϵ_{t}\\)\nWhere:\n\n\\(c\\) is a constant\n\\(θ\\) is the value of the autocorrelation of the error\n\\(ϵ_{t}\\) is the residuals for the current period\n\\(ϵ_{t−1}\\) is the residuals for the past period\n\n\n\nI:\nThe I stands for Integrated and is used to difference the time series data to remove the trend and convert a non-stationary time series to a stationary one. This is indicated by the “d” value in the ARIMA model. Hence we are combining AR and MA techniques into a single integrated model in order to achieve weak stationarity\nif \\(Y\\) is a time series variable, then first order differencing equation would look like:\n\n\n\\(Y_{t}^′ = Y_{t} − Y_{t−1}\\)\nWhere:\n\n\\(Y_{t}^′\\) is the difference between adjacent observation\n\n\n\nARIMA equation:\nThus if \\(Y\\) is a time series variable and ARIMA(2,1,2) then the equation would be:\n\n\n\\(Y_{t}^′ = c + \\varphi_{1}Y_{t−1}^′ + \\varphi_{2}Y_{t−2}^′ + θ_{1}ϵ_{t−1} + θ_{2}ϵ_{t−2} + ϵ_{t}\\)\nAn ARIMA model with 0 degrees of integration is simply an ARMA model, and so any ARIMA (p, 0, q) model is equivalent to an ARMA (p,q). likewise, any ARIMA(p, 0, 0) is equivalent to an AR(p) model and any ARIMA(0, 0, q) is equivalent to an MA(q) model\n\n\n\nEDA:\nThe objective now is to perform an Exploratory Data Analysis(EDA) on the dataset (Closing prices of PG) to find the values of p, q, and d\nBefore performing EDA, I’m going to split the data first and work only with the train data set which I will use to perform the EDA on, while the test data will be used to evaluate the prediction\n\n# Read data\ndf = yf.download('PG', '2018-06-01', '2022-06-30', interval= '1d', auto_adjust=True)\n\n# Only keep close column\ndf = df[['Close']]\n\n# Drop rows with missing values\ndf = df.dropna()\n\n#make a copy of my data just incase i need the original data\ndf_copy = df.copy()\n\n#Setting frequency to days\ndf.index = pd.DatetimeIndex(df.index).to_period('D')\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 1028 entries, 2018-05-31 to 2022-06-29\nFreq: D\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Close   1028 non-null   float64\ndtypes: float64(1)\nmemory usage: 16.1 KB\n\n\n90/10 split, 90% of data will be used to train and 10% to test\n\nt = .9\nt = int(t*len(df))\n\n# Train dataset\ntrain = df[:t]\n\n# Test dataset\ntest = df[t:]\nprint(\"number of test samples :\", test.shape[0])\nprint(\"number of training samples:\",train.shape[0])\n\nnumber of test samples : 103\nnumber of training samples: 925\n\n\n\n\nTest for stationarity and finding the “d” value:\nThe primary purpose of differencing in the ARIMA model is to make the Time Series stationary. But if the time series is already stationary then we would use the ARMA model instead because the “d” value will be 0.\nA stationary time series means a time series without a trend, one having a constant mean and variance over time,a stationary series (also called a “white noise process”) is easier to analyse as it can be modelled with fewer parameters. While it may fluctuate, it will always revert to a constant mean and is thus easier to predict.\nNOTE: A time series is considered strictly stationary if the probability distribution of a sequence of observations is unchanged by shifts in time. Strictly stationary series are rare, and it is often enough to assume weak stationarity.\nbut how would we know whether our data is stationary or not?\nWe can check for stationarity visually and statistically\n\nVisual test:\n\ntrain['rolmean'] = train['Close'].rolling(20).mean()\ntrain['rolstd'] = train['Close'].rolling(20).std()\n\n\nplt.style.use('dark_background')\nplt.figure(figsize=(12, 6))\norig = plt.plot(df_copy.index[:925], train['Close'], color='red')\nplt.title('Price')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\nplt.style.use('dark_background')\nplt.figure(figsize=(12, 6))\nmean = plt.plot(df_copy.index[:925],train['rolmean'], color='red')\nplt.title('Rolling Mean')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\nplt.style.use('dark_background')\nplt.figure(figsize=(12, 6))\nstd = plt.plot(df_copy.index[:925],train['rolstd'], color='red')\nplt.title('Rolling Standard Deviation')\nplt.grid(False)\n\n\n\n\n\n\n\n\nVisually we can see clearly that PG has been trending higher for the past 5 years, it’s been making higher highs and higher lows, and on a monthly bases, the mean is also trending higher and the standard deviation is also not constant and fluctuates over time\n\n\nStatistical test:\nTo check for stationary statistically we use the Augmented Dickey-Fuller(ADF) which is more accurate than making a visual observation.\nIn statistics and econometrics, an augmented Dickey–Fuller test is used to test whether a given time series is stationary or not.\nThe p-value resulting from the ADF test has to be less than 0.05 or 5% for a time series to be stationary. If the p-value is greater than 0.05 or 5%, you conclude that the time series has a unit root which means that it is a non-stationary process.\n\nresult = adfuller(df['Close'])\nprint('Augmented Dickey-Fuller Test:')\nlabels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\nfor value,label in zip(result,labels):\n    print(label+' : '+str(value) )\n    \nif result[1] &lt;= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -1.7582424618957933\np-value : 0.4013812122884916\n#Lags Used : 9\nNumber of Observations Used : 1018\nweak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \n\n\nWe have made a mathematical and empirical observation that our time series is not stationary hence we rule out the idea of using an ARMA model because we in fact do need to difference our time series inorder to achieve (weak) stationarity\n\n\nDifferencing:\nDifferencing is a method of making a times series dataset stationary, by subtracting the observation in the previous time step from the current observation. This process can be repeated more than once, and the number of times differencing is performed is called the difference order.\nLet’s look at the equation again:\n\n\n\\(Y_{t}^′ = Y_{t} − Y_{t−1}\\)\nSo now the question is at what difference order? Once, twice?. The aim is to avoid over-differencing because it can lead to loss of valuable information about the time series and this often affects the construction of the model\nWe could repeat the visual and statistical test every time we difference our time series but fortunately, there is a helpful package we can use called pmdarima and it will tell us at what order should we difference our time series (under the hood it repeats our statistical test and constantly checks at what order will the null hypothesis be rejected)\n\n# from pmdarima.arima.utils import ndiffs\nndiffs(train['Close'], test=\"adf\")\n\n1\n\n\nSo in order to make the series stationary we only need to difference it once!\n\ndiff = train['Close'].diff().dropna()\ndiff_rolmean = diff.rolling(20).mean()\ndiff_rolstd = diff.rolling(20).std()\n\nplt.figure(figsize=(12, 6))\nplt.plot(df_copy.index[:924],diff, color='red', label='Original')\nplt.plot(diff_rolmean, color='green', label='Rolling Mean')\nplt.plot(diff_rolstd, color='blue', label = 'Rolling Std Deviation')\nplt.title('1st order difference')\nplt.legend(loc='best')\nplt.grid(False)\n\nresult2 = adfuller(diff)\nprint('Augmented Dickey-Fuller Test:')\n\nfor value,label in zip(result2,labels):\n    print(label+' : '+str(value) )\n    \nif result2[1] &lt;= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -10.202850337835578\np-value : 5.90798356244438e-18\n#Lags Used : 8\nNumber of Observations Used : 915\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\n\n\n\n\n\n\nNote: after differencing we can see it does look stationary but it is obvious that there are huge fluctuations in mid-2020 this was due to volatility being extremely high, in fact, On March 16, 2020, the VIX closed at a record high of 82.69 as investors and traders reacted to the pandemic.\nSo now we have the “d” value in the ARIMA which is 1\nARIMA(p,1,q)\n\n\n\nFinding the “p” and “q” value using PACF and ACF:\nThe next step is to determine the appropriate order of AR (p) and MA(q) processes by using the Partial Autocorrelation function (PACF)and Autocorrelation function (ACF)\n\nPACF for AR(p):\nThe order, p, of the autoregressive model can be determined by looking at the partial autocorrelation function (PACF) plot. Partial autocorrelation can be imagined as the correlation between the series and its lag, after excluding the contributions from the intermediate lags. In other words, partial autocorrelation is the relation between observed at two-time spots given that we consider both observations are correlated to the observations at other time spots. For example, today’s stock price can be correlated to the day before yesterday, and yesterday can also be correlated to the day before yesterday… day 1 with day 3, day 2 with day 3\n\n\nACF for MA(q):\nThe order q can be determined by the Autocorrelation Function plot (ACF) which checks for correlation between two different data points of a Time Series separated by a lag “h”. For example, for lag 1, today’s stock price can be correlated with yesterday’s and yesterday’s stock price and can be correlated with the day before yesterday… day 1 with day 2, day 2 with day 3, etc, etc\n\nplt.style.use('dark_background')\n\n#PACF to find p\nfig, ax = plt.subplots(figsize=(12,5))\nplot_pacf(diff, lags=20, ax=ax,color='red')\nplt.grid(False)\n\n#ACF to find q\nfig, ax = plt.subplots(figsize=(12,5))\nplot_acf(diff, lags=20, ax=ax, color='red') \nplt.grid(False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPACF and ACF Plot:\nThe light/shaded area shows the significant threshold value, and every vertical line indicates the PACF and ACF values at each time spot. So in the plot, only the vertical lines that exceed the light/shaded red area are considered significant.\nFor PACF we can see that the PACF lag 1,4,7,9 and 15 are significant, but I will pick lag 7 since it is well above the significance line compared to the rest. So, we will set the AR(p) value equal to 7.\nWhile for the ACF, the lags 1,4,7 and 9 are significant, but lag 4 seems to be well above the significance line compared to the other lags. So, we will set the MA(q) value equal to 4.\nNOTE: Before we continue something useful to remember is that the ACF can also be used to check for stationarity, for a stationary series, the autocorrelation in the ACF plot should decay quickly; with a non-stationary series, the ACF will decay slowly as we’ve already seen\n\n#ACF example for checking stationarity\nplt.style.use('dark_background')\nfig, ax = plt.subplots(figsize=(12,3))\nplot_acf(train['Close'], lags=50, ax=ax,color='red')\nplt.grid(False)\n\n\n\n\n\n\n\n\nSo we have our “p” and “q” values, which are 7 and 4.\nARIMA(7,1,4)\n\n\n\nModel development & Evaluation:\nNow that we have determined the parameters (p,d,q), we will estimate the accuracy of the ARIMA model on a training data set and then use the fitted model to forecast the values of the test data set using a forecasting function. In the end, we cross-check whether our forecasted values are in line with the actual values.\n\n# ARIMA Model\nmodel = ARIMA(train['Close'], order=(7,1,4))\nresult = model.fit()\n\n\n#Histogram to plot residuals\nresiduals = pd.DataFrame({'residuals':result.resid})\nplt.style.use('dark_background')\nresiduals[1:].plot(kind='hist',bins=50,figsize=(12,5),color='red')\nplt.title('Residuals')\nplt.grid(False)\n\n\n\n\n\n\n\n\nThe residuals are the error based of what it would predict for the train data, the closer the residuals are to zero the better\n\npred = result.get_forecast('2022-06-29').summary_frame()\npred = pred['2022-02-01':]\n\nplt.style.use('dark_background')\npred['mean']['2022-02-01':].plot(figsize=(12,8),color='black',label='Forecast')\ntest['Close'].plot(figsize=(12,8),color='red',label='Test Price')\ntrain['Close'].plot(figsize=(12,8),color='blue',label='Train Price')\n\n#plt.fill_between(pred.index, pred['mean_ci_lower'], pred['mean_ci_upper'], color='k', alpha=0.1);\nplt.fill_between(pred.index, pred['mean_ci_lower'], pred['mean_ci_upper'], \n                 color='gray')\nplt.title('ARIMA Price Forecast and Confidence Interval')\nplt.legend(loc='best')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\npred['mean']['2022-02-01':]\n\n2022-02-01    159.739671\n2022-02-02    159.739671\n2022-02-03    159.739671\n2022-02-04    159.739671\n2022-02-05    159.739671\n                 ...    \n2022-06-25    159.739671\n2022-06-26    159.739671\n2022-06-27    159.739671\n2022-06-28    159.739671\n2022-06-29    159.739671\nFreq: D, Name: mean, Length: 149, dtype: float64\n\n\nThe values seem to be flat, hence the prediction is far from accurate at the very least I was expecting the model to predict a trend in either direction\nA possible solution to get a more accurate result using the ARIMA would be to test with different lags, remember we got several lags that we could’ve used, we can test a combination of all of them to find the one which can offer better result\nBut obviously, this will take time, a solution to that is to use the same package that we used to get the number of differencing needed to make our time series stationary.\n\n\nAUTO ARIMA:\nWe can implement the Auto ARIMA model using the pmdarima time-series library which provides the auto_arima() function that automatically generates the optimal parameter values for all 3 p,d,q values\nThe auto_arima() function can take a number of paramters but to keep it simple i’ll just run the function and get the default best model, all we need to do is just pass in our time series data.\n\nfrom pmdarima.arima import auto_arima\nauto_arima = auto_arima(train[\"Close\"])\nauto_arima\n\nARIMA(order=(4, 1, 5), scoring_args={}, suppress_warnings=True)\n\n\n\nprint(f'The default best value is {auto_arima.fit(train[\"Close\"])}')\n\nThe default best value is  ARIMA(4,1,5)(0,0,0)[0] intercept\n\n\n\n# ARIMA Model\nmodel2 = ARIMA(train['Close'], order=(4,1,5))\nresult2 = model2.fit()\n\n\n#Histogram\nresiduals2 = pd.DataFrame({'residuals':result2.resid})\nplt.style.use('dark_background')\nresiduals2[1:].plot(kind='hist',bins=50,figsize=(12,5),color='red')\nplt.title('Residuals')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\npred2 = result2.get_forecast('2022-06-29').summary_frame()\npred2 = pred2['2022-02-01':] \n\nplt.style.use('dark_background')\npred2['mean']['2022-02-01':].plot(figsize=(12,8),color='black',label='Forecast')\ntest['Close'].plot(figsize=(12,8),color='red',label='Test Price')\ntrain['Close'].plot(figsize=(12,8),color='blue',label='Train Price')\n\nplt.fill_between(pred2.index, pred2['mean_ci_lower'], pred2['mean_ci_upper'], \n                 color='gray')\nplt.title('ARIMA Price Forecast and Confidence Interval')\nplt.legend(loc='best')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\npred2['mean']['2022-02-01':]\n\n2022-02-01    159.472243\n2022-02-02    159.472243\n2022-02-03    159.472243\n2022-02-04    159.472243\n2022-02-05    159.472243\n                 ...    \n2022-06-25    159.472243\n2022-06-26    159.472243\n2022-06-27    159.472243\n2022-06-28    159.472243\n2022-06-29    159.472243\nFreq: D, Name: mean, Length: 149, dtype: float64\n\n\n\n\nConclusion:\nEven after the default best model, the values haven’t changed much as they look constant from afar.\nStock data is complex and are usually composed of linear and non-linear components. So just using a linear model to make predictions will not be efficient, especially using ARIMA to predict prices as we saw didn’t provide a realistic prediction, they may however be useful to predict the overall trend of a time series.\nAlternatively, we could try and predict stock returns instead because returns are considered stationary(at least visually) or we could use the SARIMA (Seasonal Auto-Regressive Integrated Moving Average) model which is an extension of the ARIMA, it adds seasonal components of the time series that can help improve prediction.\nBut again something to remember is that even if we continue to fine-tune the ARIMA and SARIMA models, the prediction could still be inaccurate because we will still be using univariate data, in order to get the best predictions it’s better to use multivariate data like using the closing prices or stock return of other correlated consumer staple stocks like Colgate-Palmolive Company (TICKER: CL) or The Clorox Company (TICKER: CLX) using models like the ARIMAX or SARIMAX which are just an extension of the ARIMA and SARIMA models, The ‘X’ stands for an exogenous variable(or variables) and in this example, CL and CLX could be those exogenous variables.\nFor now, my knowledge is still limited, more time is needed to research and learn about other models as well learning in-depth analysis and forecasting techniques, hence, In future blogs, I will try and incorporate all relevant models and leverage the pmdarima package even further by adding extra parameters to try to find the most accurate model"
  },
  {
    "objectID": "Quant/index.html",
    "href": "Quant/index.html",
    "title": "Quantitative Research",
    "section": "",
    "text": "Portfolio Analysis, Efficient Frontier & Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBTC/USD Price Prediction Using Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStock Price Forecast Using ARIMA Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Stats/2022-10-07-Stats2.html",
    "href": "Stats/2022-10-07-Stats2.html",
    "title": "EDA PART 2, Graphical Analysis",
    "section": "",
    "text": "In this we’ll go through some visualization techniques performed during explanatory data analysis\n\n\ntoc: true\nbadges: true\ncategories:[Statistics & Probability]\nimage:images/plots.jpg\nuse_plotly: true\n\nWhile non-graphical analysis provides us with valuable insights, graphical techniques provide a visual representation that enhances our understanding of the data. graphical analysis serves as a vital tool for exploring and understanding data.\nGraphical analysis allows us to examine the distribution of data, providing insights into the patterns, shape, and spread of values within a dataset. Histograms, for example, display the frequency or proportion of data points within specific intervals, enabling us to identify peaks, gaps, or skewed distributions. Box plots provide a visual representation of the minimum, maximum, median, and quartiles, helping us understand the central tendencies and variability in the data. By exploring the distribution of data graphically, we can gain a deeper understanding of its characteristics and identify any outliers or anomalies.\nFurthermore, Graphical analysis enables us to explore the relationships between different variables in a dataset. Scatter plots, for instance, plot data points as dots on a graph, allowing us to observe the correlation or association between two variables. This helps us identify trends, patterns, or potential dependencies. Additionally, line plots or time series plots provide a visual representation of how variables change over time, highlighting any trends or seasonal patterns. By examining these graphical representations, we can uncover valuable insights into the relationships and dependencies between variables, enabling us to make informed decisions or predictions.\nLastly, Graphical analysis facilitates the comparison of different datasets or categories, allowing us to identify similarities, differences, or trends. Bar charts or column charts, for example, provide a visual representation of categorical data, making it easy to compare the frequency or proportions of different categories. Grouped bar charts or stacked bar charts can be used to compare multiple categories simultaneously. By visually comparing data, we can identify variations, spot outliers, or detect patterns across different groups or time periods. This helps us make data-driven decisions and identify areas of improvement.\nBy leveraging visual representations, we gain valuable insights into the underlying patterns, trends, and characteristics of the data, empowering us to make informed decisions and draw meaningful conclusions.\nWe will explore some of the most commonly used visualization tools that I frequently use when performing graphical analysis\n\n#importing relevant libraries\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom yahoofinancials import YahooFinancials as yfin\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom IPython.display import HTML\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)  \nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\nHEATMAP\nThe correlation heatmap allows for a quick visual assessment of the relationships between variables in a dataset. It helps identify strong positive or negative correlations, patterns, and clusters among variables. This graphical representation provides a more intuitive and comprehensive understanding of the interdependencies among multiple variables compared to a tabular format.\nUsing the same portfolio of stocks from the previous part, when we examine the return correlation between the stocks, we can quickly observe which stocks move together. As suspected, there is no pair with a negative correlation, considering that all the stocks share common traits. They are all part of the S&P 500, and they all belong to the technology sector, except for Tesla. This explains why Tesla is the only stock that exhibits a less positive correlation with every other stock. However, since Tesla utilizes technology heavily in its electric vehicles, it still considered a tech company by some despite its primary focus on automotive production. Therefore, it exhibits similar characteristics to other tech stocks, which may explain why the correlation is not negative but still not as strong as the correlations among the purely technology-focused stocks.\n\nassets = ['META','AMZN','NFLX','GOOG','MSFT','NVDA','TSLA']\npf_data = pd.DataFrame()\nfor a in assets:\n    pf_data[a] = yf.download(a, start=\"2021-10-01\", end=\"2021-12-31\")['Adj Close']\n\nreturns = pf_data.pct_change(1).dropna()\ncov = returns.cov()\ncorr = returns.corr()\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n(NOTE: REFRESH PAGE IF GRAPHS ARE NOT LOADING)\n\nfig = px.imshow(corr)\nfig.update_layout(width=1000, height=800)\nfig.update_layout(template = \"plotly_dark\", title = 'A heat Map Of Stock Returns In a Portfolio') \nfig.show()\n\n                                                \n\n\n\n\nSCATTER MATRIX & SCATTER PLOT\nAside from a heatmap we can also use a scatter matrix to check the relationships between the stocks.\nA scatter matrix, also known as a scatterplot matrix or pair plot, is a graphical tool used to explore the relationships between multiple variables in a dataset. It consists of a grid of scatterplots, where each scatterplot represents the relationship between two variables.\nA scatter plot is a type of graph that displays the relationship between two variables. It consists of a horizontal x-axis and a vertical y-axis, where each data point is represented by a dot or marker on the plot. The position of each dot on the scatter plot corresponds to the values of the two variables being analyzed.\nScatter plots are particularly useful for visualizing the correlation or relationship between two continuous variables. By plotting the data points on the scatter plot, it becomes easier to observe any patterns, trends, or associations between the variables. The general shape or direction of the points on the scatter plot can provide insights into the strength and direction of the relationship.\n\nfig = px.scatter_matrix(returns, title='A Scatter Matrix Of Stock Returns In A Portfolio', color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1200, height=800)\nfig.update_layout(template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nWhen we examine the relationships from both the heatmap and scatter matrix, the pair that exhibits the highest correlation is Microsoft and Google. This can be explained by the fact that these two companies are often mentioned in the same context, as they share similarities in their business models. For instance, Microsoft has Bing while Google dominates the search engine market, both companies have cloud platforms (Azure and Google Cloud Platform), and they offer productivity suites such as Excel and Google Sheets, Word and Google Docs, Gmail and Outlook, among others. To gain a deeper understanding of the relationship between these two stocks, we can visualize their relationship separately and analyze it more closely.\n\nfig = px.scatter(returns, x='MSFT', y='GOOG', title='Scatter Plot Of MSFT Return and GOOG Return',color=\"MSFT\")#,trendline='ols', trendline_color_override='firebrick')#color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1000, height=800)\nfig.update_layout(template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nWe can definitely see a linear relationship here, when one does well the other does well and vice versa, this should be taken into consideration whenever one includes both of them in a portfolio\nWe can also plot a trendline, which is also known as an Ordinary Least Squares (OLS) line. it is often used in scatter plots to depict the overall trend or relationship between two variables.\nBy using a trendline or OLS line in a scatter plot, we can visually observe the direction and strength of the relationship between the variables. The line is positioned to best represent the general pattern of the data points, whether it be a positive or negative correlation, or even no apparent correlation.\n\nfig = px.scatter(returns, x='MSFT', y='GOOG', title='Scatter Plot and Trendline Of MSFT Return and GOOG Return',color=\"MSFT\",trendline='ols', trendline_color_override='firebrick')\nfig.update_layout(width=1000, height=800)\nfig.update_layout(template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nIn addition to representing the overall trend, a trendline or OLS line in a scatter plot can also be utilized for prediction purposes using linear regression. By fitting a linear regression model to the data, we can establish a mathematical relationship between the variables, enabling us to make predictions or estimates based on this model.\nLinear regression allows us to determine the equation of the line that best fits the data points, providing us with a predictive model. This model can then be used to forecast or estimate the value of one variable based on the value of the other variable. By leveraging the linear regression analysis, we can make informed predictions and gain insights into how changes in one variable may impact the other.\nBut again, caution should be exercised when interpreting the relationship between variables based on a scatter plot and linear regression analysis. Although changes in one variable may be associated with changes in the other variable, it does not necessarily imply causation!\n\n\nBAR CHART\nA bar chart, also known as a bar graph, is a popular visualization tool that presents data using rectangular bars of varying heights. Each bar represents a category or group, and the height of the bar corresponds to the value or frequency of that category. Bar charts are effective in comparing different categories or groups and visually displaying patterns or trends in the data.\nFor this particular case, I wanted to examine the volume of trades for each stock in the 4th quarter of 2021. As anticipated, Tesla emerged as the leader of the pack, with approximately 5 billion shares traded. Amazon closely followed with 4 billion shares traded. Surprisingly, Netflix has not been receiving the level of trading activity that I would have expected, considering the company’s size and prominence\n\npf_data2 = pd.DataFrame()\nfor b in assets:\n    pf_data2[b] = yf.download(b, start=\"2021-01-01\", end=\"2021-12-31\", interval='3mo')['Volume']\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\nfig = px.bar(pf_data2.loc['2021-10-01'],color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nSTACKED BAR CHART\nA stacked bar chart is a visualization tool that represents different categories or groups of data as stacked bars, where each bar segment corresponds to a subcategory or a portion of the whole\nUsing the same example, instead of focusing on the volume traded in a single quarter, I now intend to analyze the volume traded in each of the four quarters of 2021. The verdict remains consistent, with Tesla consistently trading the highest volume. Additionally, another noteworthy observation is that the volume for the tech stocks in the portfolio decreased in the 3rd quarter, the volume had been in a downward trend decreasing QoQ since early 2021.\n\nfig = px.bar(pf_data2)\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nAlternatively, we can plot a side-by-side bar graph for each stock. While a stacked bar chart provides information on the total volume, a side-by-side bar graph presents a clearer depiction of how the volume for each stock has evolved throughout the year.\n\nfig = px.bar(pf_data2,barmode='group')\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nLINE CHART\nLine charts, also known as line graphs or time series plots, are effective visual representations for displaying trends and patterns over time. They are particularly useful when analyzing data that is continuous or sequential in nature, such as stock prices, temperature fluctuations, or population growth.\nIf we plot the stock prices, we can observe the progression of each stock throughout 2021.\n\nfig = px.line(pf_data)\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nHowever, I typically examine how each stock has performed relative to one another. To achieve this, I normalize the data using cumulative returns, making it much easier to identify outperforming and underperforming stocks. By doing so, we can see that Nvidia is leading the pack, followed by Tesla, while Netflix and Meta have significantly underperformed in comparison.\n\ncum_returns =   (1 + returns).cumprod() - 1\nfig = px.line(cum_returns)\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nPIE CHART\nA pie chart is a circular graphical representation that is divided into slices to illustrate the proportional composition of different categories or parts of a whole. Each slice represents a specific category, and its size is determined by the proportion or percentage it contributes to the total. Pie charts are effective in visualizing categorical data and showing the relative sizes or distributions of different categories. They are particularly useful for displaying data that can be grouped into distinct categories and highlighting the relative importance or contribution of each category. By examining a pie chart, we can quickly grasp the overall composition and relative significance of different components within the data.\nIf we want to determine the sectors that contribute the most or are most represented in the S&P 500, we can use a pie chart for this analysis. From the pie chart, we can observe that there are five dominant sectors, with four of them being cyclical. This observation may explain why the S&P 500 tends to underperform during economic downturns. Additionally, the telecommunication sector has the fewest number of companies in the index, indicating that it is the least represented sector\nData Source: https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information?resource=download\n\nSnP_500 = pd.read_csv('/home/mj22/data/financials.csv')\n\nfig = px.pie(SnP_500, names='Sector')\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nBOX PLOT\nA box plot is a graphical representation that provides a visual summary of the distribution of a dataset. It is particularly useful for comparing the distribution of multiple variables or groups. The plot consists of a box that represents the interquartile range (IQR), which contains the middle 50% of the data, with a line inside the box indicating the median. The whiskers extend from the box to the minimum and maximum values, excluding any outliers, which are represented as individual points. By examining the box plot, we can identify the central tendency, spread, skewness, and potential outliers in the data, making it a powerful tool for exploratory data analysis.\nWhen we examine the box plot for Price/Sales for all the stocks in the index, we quickly notice the presence of outliers. The maximum value is 20, the minimum value is 0.15, and the median is 2.89. The first quartile is 1.62, and the third quartile is 4.71.\n\nfig = px.box(SnP_500, y='Price/Sales', color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nWe can also analyze the Price/Sales ratio for each sector in the index, providing us with a more insightful observation. From this analysis, we can observe that the real estate sector has a significantly higher P/S ratio compared to other sectors, while the telecommunications sector appears to have the lowest. However, it’s important to note that this is an inductive observation, as the index only includes the 500 largest companies, which may not present the complete picture.\n\nfig = px.box(SnP_500, y='Price/Sales', x='Sector', color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nVIOLIN PLOT\nAlternatively we can use a violin plot, a violin plot combines a box plot with a kernel density plot. It displays the same summary statistics as a box plot, but also provides a more detailed view of the distribution. The plot is symmetrical and resembles a violin or a mirrored density plot. The width of the violin at each point represents the density of data points, with wider areas indicating higher density.\n\nfig = px.violin(SnP_500, y='Earnings/Share', color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nCompared to box plots, violin plots offer additional insights into the shape and multimodality of the distribution. They provide a more comprehensive visualization of the data, allowing for a better understanding of its characteristics. However, box plots are more compact and straightforward, making them useful for quick comparisons between multiple groups or variables.\nAdditionally, with a violin plot, we can plot scatter dots alongside to better visualize the concentration of data.\nWe can see that most of the companies in the index have earnings per share between 12 and -3, but the data is more concentrated around the range of 3 to 1. Additionally, the majority of companies have a positive earnings per share.\n\nfig = px.violin(SnP_500, y='Earnings/Share', color_discrete_sequence=['firebrick'], points=\"all\")\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nAgain, we could also observe the earnings per share (EPS) for all the sectors. It appears that almost all of the sectors have a higher number of companies with positive earnings, except for the energy sector. This discrepancy raises further questions and warrants investigation to understand why the energy sector has more companies with negative earnings compared to the rest.\n\nfig = px.violin(SnP_500, y='Earnings/Share', x='Sector', color_discrete_sequence=['firebrick'],points=\"all\")\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nBoth violin plots and box plots serve as valuable tools in exploratory data analysis, helping to identify central tendencies, dispersion, skewness, and potential outliers in a dataset. The choice between the two depends on the specific requirements and the level of detail desired in visualizing the data distribution.\n\n\nHistogram\nA histogram is a graphical representation that displays the distribution of a dataset. It consists of a series of bars, where each bar represents a range of values and the height of the bar corresponds to the frequency or count of observations falling within that range. Histograms provide a visual depiction of the data’s frequency distribution, allowing us to identify patterns, skewness, and central tendencies. They are particularly useful for understanding the shape of the data and detecting any outliers or unusual patterns. By examining the histogram, we can gain insights into the underlying characteristics and distribution of the variable being analyzed\nWhen we plot the weekly distribution of returns for the MSCI World Index, we can observe that most of the data points are situated around -1.5% to 2.5%. Additionally, we can identify outliers scattered along the distribution, indicating extreme or unusual returns compared to the majority of data points.\n\nworld = yf.download('IXUS', \"2013-01-01\", \"2022-06-01\",interval=\"1wk\")['Adj Close']\nworld_returns = world.pct_change(1).dropna()   \n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n# Create histogram using Plotly Express\nfig = px.histogram(world_returns, x=world_returns, nbins=80, color_discrete_sequence=['firebrick'])\n\n# Customize the layout if needed\nfig.update_layout(title=\"Distribution of Returns\", xaxis_title=\"Returns\", yaxis_title=\"Count\",width=1500, height=900,template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nBut the way I always use the distribution of returns is by analyzing the percentage of occurrence for each bin, which provides insights into the potential expectations for the asset going forward. Interpreting the data, we can derive the following conclusions:\n\nIn approximately 66% of the time, the MSCI returns anywhere from -1.5% to +2.5% in a given week.\nIf you were looking to go long on the index with a target return of 3%, the probability of achieving that would be 5.4%.\nConversely, if you were aiming to go short and expecting at least a 2% return, the probability of achieving that return is approximately 9%. However, both probabilities are statistically insignificant.\n\nFrom this analysis, we can conclude that expecting a 3% return or a 2% return is unrealistic, and it may be necessary to adjust your target returns accordingly. Let’s consider targeting a 1% return instead:\n\nIf you were to go long with a minimum target of a 1% return, the probability of that happening would be 33%.\nOn the other hand, if you were to go short with a minimum target of a 1% return, the probability of achieving that desired return would be 23%.\n\nAlthough the odds appear to be better when adjusting our expectations, it is still statistically insignificant. Comparing it to a skydiving scenario where the parachute only works 33% of the time, taking such a risk would not be advisable.\n\n# Create histogram using Plotly Express\nfig = px.histogram(world_returns, nbins=80, color_discrete_sequence=['firebrick'], text_auto=True, histnorm='percent')\n\n\n# Customize the layout if needed\nfig.update_layout(title=\"Distribution of Returns\", xaxis_title=\"Returns\", yaxis_title=\"Count\",width=1500, height=900,template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nHistograms are a great tool for exploring data. However, there is still more to uncover. As you may have noticed, we have begun delving into the realm of probability. In our next blog, we will continue this exploration so that we can gain a deeper understanding of distributions and appreciate concepts such as skewness, fat tails, probability of events, and more. By incorporating these theories, we will further enhance our data analysis capabilities.\n\nReferences\n\n“Quantitative Investment Analysis”, by DeFusco, McLeavey, Pinto, and Runkle\n“Introduction to Modern Statistics”, by Mine Çetinkaya-Rundel and Johanna Hardin"
  }
]