[
  {
    "objectID": "Stats/index.html",
    "href": "Stats/index.html",
    "title": "Statistics & Probability",
    "section": "",
    "text": "EDA PART 1, Summary Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEDA PART 2, Graphical Analysis\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Stats/2022-08-21-Stats1.html",
    "href": "Stats/2022-08-21-Stats1.html",
    "title": "EDA PART 1, Summary Statistics",
    "section": "",
    "text": "In this notebook we’ll go through some basic concepts that most if not all statisticians and data analysts look at when they first encouter a data set\nExploratory Data Analysis (EDA) is a systematic and unbiased approach to analyze and understand a dataset by employing statistical techniques and visualizations. It involves thoroughly examining the dataset from various perspectives, summarizing its characteristics, and identifying key patterns, trends, or anomalies without making any prior assumptions about the data’s nature or underlying relationships. EDA aims to provide meaningful insights and uncover relevant features of the data that can guide further analysis and decision-making processes.\nThere are two types of data, categorical and numerical data.\nCategorical data, also known as qualitative data, is a type of data that represents characteristics or attributes that belong to a specific category or group\nWhile, Numerical data, also known as quantitative data, is a type of data that consists of numeric values representing measurable quantities or variables.\nUnivariate Analysis: Univariate analysis is a type of exploratory data analysis that focuses on analyzing or dealing with only one variable at a time. It involves examining and describing the data of a single variable, aiming to identify patterns and characteristics within that variable. Univariate analysis does not consider causes or relationships and primarily serves the purpose of providing insights and summarizing data.\nBivariate Analysis: Bivariate analysis is a type of exploratory data analysis that involves the analysis of two different variables simultaneously. It explores the relationship between these two variables, aiming to understand how changes in one variable may impact the other. Bivariate analysis delves into causality and relationships, seeking to identify associations and dependencies between the two variables under investigation.\nMultivariate Analysis: Multivariate analysis is a type of exploratory data analysis that deals with datasets containing three or more variables. It examines the relationships and patterns between multiple variables, allowing for a more comprehensive analysis of the data. Multivariate analysis employs various statistical techniques and graphical representations to uncover complex relationships and interactions among the variables, facilitating a deeper understanding of the dataset as a whole.\nEDA consists of two parts,\nNon-graphical Analysis and Graphical Analysis\nNon-graphical Analysis: Non-graphical analysis in exploratory data analysis involves examining and analyzing data using statistical tools and measures such summary statistic that quantitatively describes or summarizes features of a dataset. It focuses on understanding the characteristics and patterns for mostly one variable but can also be used for two or more variables.\nGraphical Analysis: Graphical analysis is the most common part of exploratory data analysis that utilizes visualizations and charts to analyze and interpret data. It involves representing data in graphical forms to visually identify trends, patterns, distributions, relationships between variables or even compare different variables. Graphical analysis provides a comprehensive view of the data, allowing for a better understanding of the underlying structure and facilitating the exploration of multivariate relationships.\nWe’ll start off by first performing the non graphical part then we will finish with graphical analysis in the second part"
  },
  {
    "objectID": "Stats/2022-08-21-Stats1.html#measures-of-central-tendency",
    "href": "Stats/2022-08-21-Stats1.html#measures-of-central-tendency",
    "title": "EDA PART 1, Summary Statistics",
    "section": "Measures of central tendency:",
    "text": "Measures of central tendency:\nA measure of central tendency (also referred to as measures of center or central location) is a summary measure that attempts to describe a whole set of data with a single value that represents the middle or center of its distribution.\n\nMean:\nThe arithmetic mean is the sum of the observations divided by the number of observations. The arithmetic mean is by far the most frequently used measure of the middle or center of data. The mean is also referred to as the average The population mean, μ, is the arithmetic mean value of a population. For a finite population, the population mean is:\n\\[\\mu = \\dfrac{\\sum_{i=1}^N X_i}{N} \\]\nwhere \\(N\\) is the number of observations in the entire population and \\(X_i\\) is the \\(i\\)th observation.\nThe sample mean is the arithmetic mean computed for a sample. A sample is a percentage of the total population in statistics. You can use the data from a sample to make inferences about a population as a whole. The concept of the mean can be applied to the observations in a sample with a slight change in notation.\n\\[\\bar{x} = \\dfrac{\\sum_{i=1}^n X_i}{n} \\]\nwhere \\(n\\) is the number of observations in the sample.\n\n#importing relevant libraries\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nimport scipy.stats as stats\nimport statistics\n\n\n#Downloading data\nmsft_daily = yf.download('MSFT', '2016-01-01', '2021-12-31', interval= '1d')['Adj Close']\n\n\n#Calculating average return\nmsft_returns = msft_daily.pct_change(1).dropna()\naverage_return = str(round((np.mean(msft_returns) * 100),2)) + '%'\nprint(f'The average daily return for Microsoft stock is: {average_return}')\n\nThe average daily return for Microsoft stock is: 0.14%\n\n\n\n\nWeighted mean:\nThe ordinary arithmetic mean is where all sample observations are equally weighted by the factor 1/n (each of the data points contributes equally to the final average).\nBut with the weighted mean, some data points contribute more than others based on their weighting, the higher the weighting, the more it influences the final average. The weighted mean is also referred to as the weighted average\n\\[\\bar{X}_w = {\\sum_{i=1}^n w_i X_i} \\]\nwhere the sum of the weights equals 1; that is,\\(\\sum_{i} w_i = 1\\)\n\n#Downloading data\naapl = yf.download('AAPL', '2021-10-01', '2021-12-31', interval= '1d')['Adj Close']\nnvdia = yf.download('NVDA', '2021-10-01', '2021-12-31', interval= '1d')['Adj Close']\nmsft = yf.download('MSFT', '2021-10-01', '2021-12-31', interval= '1d')['Adj Close']\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n#Calculating 2021 Q4 return\nmsft_ret = (msft[-1] - msft[0])/msft[0]\naapl_ret = (aapl[-1] - aapl[0])/aapl[0]\nnvda_ret = (nvdia[-1] - nvdia[0])/aapl[0]\n\n#portfolio return if 50% of capital was deplyoed in microsoft,30% in apple and 20% in nvidia\nWavg = (msft_ret * .50 + aapl_ret + .30 + nvda_ret * .20)/3\navg = (msft_ret + aapl_ret + nvda_ret)/3\n\nweighted = str(round(Wavg,2)) + '%'\narith = str(round(avg,2)) + '%'\n\nprint(f\"The Weighted mean return of the portfolio assuming a 50/30/20 split is: {weighted}\")\nprint(f\"The Arithmetic mean return of the portfolio assuming no split is:\", arith)\n\nThe Weighted mean return of the portfolio assuming a 50/30/20 split is: 0.25%\nThe Arithmetic mean return of the portfolio assuming no split is: 0.35%\n\n\nThe weighted mean is also very useful when calculating a theoretically expected outcome where each outcome has a different probability of occurring (more on this in probability concepts)\n\n\nHarmonic mean:\nThe harmonic mean is a type of numerical average. It is calculated by dividing the number of observations by the reciprocal of each number in the series. Thus, the harmonic mean is the reciprocal of the arithmetic mean of the reciprocals.\n\\[\\bar{X}_h = \\dfrac{n}{\\sum_{i=1}^n \\dfrac1X_i} \\]\n\n\nGeometric mean:\nThe geometric mean is most frequently used to average rates of change over time or to compute the growth rate of a variable.\nFor volatile numbers like stock returns, the geometric average provides a far more accurate measurement of the true return by taking into account year-over-year compounding that smooths the average.\n\\[ G = \\sqrt[n]{X_1,X_2,X_3....X_n} \\]\nData Source: https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information?resource=download\n\nSnP_500 = pd.read_csv('/home/mj22/data/financials.csv')\nSnP_500\n\n\n\n\n\n\n\n\nSymbol\nName\nSector\nPrice\nPrice/Earnings\nDividend Yield\nEarnings/Share\n52 Week Low\n52 Week High\nMarket Cap\nEBITDA\nPrice/Sales\nPrice/Book\nSEC Filings\n\n\n\n\n0\nMMM\n3M Company\nIndustrials\n222.89\n24.31\n2.332862\n7.92\n259.77\n175.490\n1.387211e+11\n9.048000e+09\n4.390271\n11.34\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n1\nAOS\nA.O. Smith Corp\nIndustrials\n60.24\n27.76\n1.147959\n1.70\n68.39\n48.925\n1.078342e+10\n6.010000e+08\n3.575483\n6.35\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n2\nABT\nAbbott Laboratories\nHealth Care\n56.27\n22.51\n1.908982\n0.26\n64.60\n42.280\n1.021210e+11\n5.744000e+09\n3.740480\n3.19\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n3\nABBV\nAbbVie Inc.\nHealth Care\n108.48\n19.41\n2.499560\n3.29\n125.86\n60.050\n1.813863e+11\n1.031000e+10\n6.291571\n26.14\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n4\nACN\nAccenture plc\nInformation Technology\n150.51\n25.47\n1.714470\n5.44\n162.60\n114.820\n9.876586e+10\n5.643228e+09\n2.604117\n10.62\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n500\nXYL\nXylem Inc.\nIndustrials\n70.24\n30.94\n1.170079\n1.83\n76.81\n46.860\n1.291502e+10\n7.220000e+08\n2.726209\n5.31\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n501\nYUM\nYum! Brands Inc\nConsumer Discretionary\n76.30\n27.25\n1.797080\n4.07\n86.93\n62.850\n2.700330e+10\n2.289000e+09\n6.313636\n212.08\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n502\nZBH\nZimmer Biomet Holdings\nHealth Care\n115.53\n14.32\n0.794834\n9.01\n133.49\n108.170\n2.445470e+10\n2.007400e+09\n3.164895\n2.39\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n503\nZION\nZions Bancorp\nFinancials\n50.71\n17.73\n1.480933\n2.60\n55.61\n38.430\n1.067068e+10\n0.000000e+00\n3.794579\n1.42\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n504\nZTS\nZoetis\nHealth Care\n71.51\n32.80\n0.682372\n1.65\n80.13\n52.000\n3.599111e+10\n1.734000e+09\n9.280896\n18.09\nhttp://www.sec.gov/cgi-bin/browse-edgar?action...\n\n\n\n\n505 rows × 14 columns\n\n\n\n\n# Use scipy's gmean&hmean function to compute the geometric mean and harmonic mean\nprint(\"The Harmonic mean of Price to Sales Ratio for companies in the S&P 500 is:\", round(stats.hmean(SnP_500['Price/Sales']),2))\nprint(\"The Geometric mean of  Price to Sales Ratio  for companies in the S&P 500 is:\", round(stats.gmean(SnP_500['Price/Sales']),2))\nprint(\"The Arithmetic mean of  Price to Sales Ratio  for companies in the S&P 500 is:\", round(np.mean(SnP_500['Price/Sales']),2))\n\nThe Harmonic mean of Price to Sales Ratio for companies in the S&P 500 is: 1.95\nThe Geometric mean of  Price to Sales Ratio  for companies in the S&P 500 is: 2.83\nThe Arithmetic mean of  Price to Sales Ratio  for companies in the S&P 500 is: 3.94\n\n\n\n“A mathematical fact concerning the harmonic, geometric, and arithmetic means is that unless all the observations in a data set have the same value, the harmonic mean is less than the geometric mean, which in turn is less than the arithmetic mean” – Quantitative Investment Analysis, by DeFusco, McLeavey, Pinto, and Runkle\n\n\n\nTrimmed mean:\nA trimmed mean is a method of averaging that removes a small designated percentage of the largest and smallest values before calculating the mean. After removing the specified outlier observations, the trimmed mean is found using a standard arithmetic averaging formula. The use of a trimmed mean helps eliminate the influence of outliers or data points on the tails that may unfairly affect the traditional or arithmetic mean.\nTo trim the mean by a total of 40%, we remove the lowest 20% and the highest 20% of values, eliminating the scores of 8 and 9\n\ndata = [8, 2, 3, 4, 9]\nmean = np.mean(data)\n\ntrimmed_data = [2, 3, 4]\ntrimmed_mean = np.mean(trimmed_data)\n\nprint(f\"Hence, a mean trimmed at 40% would equal {trimmed_mean} versus {mean}\")\n\nHence, a mean trimmed at 40% would equal 3.0 versus 5.2\n\n\n\n\nMedian:\nThe median is the middle number in a sorted, ascending, or descending list of numbers and can be more descriptive of that data set than the average. It is the point above and below which half (50%) of the observed data falls, and so represents the midpoint of the data.\n\nprint(f'The Median Price to Sales Ratio for Companies in the S&P 500 is:', round(np.median(SnP_500['Price/Sales']),2))\n\nThe Median Price to Sales Ratio for Companies in the S&P 500 is: 2.9\n\n\n\n\nMode:\nThe mode is the value that appears most frequently in a data set A distribution can have more than one mode or even no mode. When a distribution has one most frequently occurring value, the distribution is said to be unimodal. If a distribution has two most frequently occurring values, then it has two modes and we say it is bimodal. If the distribution has three most frequently occurring values, then it is trimodal. When all the values in a data set are different, the distribution has no mode because no value occurs more frequently than any other value.\n\nmode =  round(statistics.mode(SnP_500['Price/Sales']),2)\nprint(f'The Mode of the Price to Sales ratio for Companies in the S&P 500 is {mode}, indicating that it is the most commonly occurring value among the dataset')\n\nThe Mode of the Price to Sales ratio for Companies in the S&P 500 is 4.39, indicating that it is the most commonly occurring value among the dataset"
  },
  {
    "objectID": "Stats/2022-08-21-Stats1.html#measures-of-dispersion",
    "href": "Stats/2022-08-21-Stats1.html#measures-of-dispersion",
    "title": "EDA PART 1, Summary Statistics",
    "section": "Measures of dispersion",
    "text": "Measures of dispersion\nLocation is just one dimension in describing data. A second dimension, variability, also referred to as dispersion, measures whether the data values are tightly clustered or spread out.\n\nRange:\nRange, the range of a set of data is the difference between the largest and smallest values, the result of subtracting the sample maximum and minimum. It is expressed in the same units as the data.\n\nmaX = np.max(dividends)\nmiN = np.min(dividends)\nRange= np.ptp(dividends)\n\nprint(f'The maximum dividend per share paid by Microsoft is {maX}$') \nprint(f'The minimum dividend per share paid by Microsoft is {miN}$')\nprint(f'Hence, the range is {Range}$')\n\nThe maximum dividend per share paid by Microsoft is 3.08$\nThe minimum dividend per share paid by Microsoft is 0.08$\nHence, the range is 3.0$\n\n\n\n\nVariance sample and population:\nThe variance and standard deviation are the two most widely used measures of dispersion Variance is defined as the average of the squared deviations around the mean. Population variance is a measure of the spread of population data. Hence, population variance can be defined as the average of the distances from each data point in a particular population to the mean squared, and it indicates how data points are spread out in the population. we can compute the population variance.Denoted by the symbol σ2\nPopulation formula:\n\\[\\sigma^2 = \\dfrac{\\sum_{i=1}^N(x_i - \\mu)^2}N \\]\nWhile sample formula is:\n\\[s^2 = \\dfrac{\\sum_{i=1}^n(x_i - \\bar x)^2}{n-1} \\]\n\n\nStandard deviation sample and population:\nStandard Deviation (SD) is the positive square root of the variance. It is represented by the Greek letter ‘σ’ and is used to measure the amount of variation or dispersion of a set of data values relative to its mean (average), thus interpret the reliability of the data. If it is smaller then the data points lie close to the mean value, thus showing reliability. But if it is larger then data points spread far from the mean.\nPopulation formula:\n\\[\\sigma^2 = \\sqrt{\\dfrac{\\sum_{i=1}^N(x_i - \\mu)^2}N} \\]\nWhile sample formula is:\n\\[s^2 = \\sqrt{\\dfrac{\\sum_{i=1}^n(x_i - \\bar x)^2}{n-1}} \\]\nHence, Variance Measures Dispersion within the Data Set while the standard deviation measures spread around the mean!\n\nprint(\"The variance of Microsoft's daily stock returns:\", round(np.var(msft_returns),5))\nprint(\"The standard deviation of Microsoft's daily stock returns:\", round(np.std(msft_returns),5))\n\nThe variance of Microsoft's daily stock returns: 0.00028\nThe standard deviation of Microsoft's daily stock returns: 0.01684"
  },
  {
    "objectID": "Stats/2022-08-21-Stats1.html#relationship-between-variables",
    "href": "Stats/2022-08-21-Stats1.html#relationship-between-variables",
    "title": "EDA PART 1, Summary Statistics",
    "section": "Relationship between variables",
    "text": "Relationship between variables\nRelationship between variables means, In a dataset, the values of one variable correspond to the values of another variable.\nBy conducting a non-graphical analysis of the relationship between variables, you can quantitatively assess their associations, dependencies, and impacts, providing valuable insights for further analysis and decision-making\nMoreover, non-graphical analysis of the relationship between variables involves examining the numerical values in a matrix to understand the connections between variables. A covariance matrix and a correlation matrix are square matrices that display the pairwise relationships between different variables in a dataset. They provide valuable insights into the strength and direction of the relationships between variables\n\nCovariance\nCovariance provides insight into how two variables are related to one another. More precisely, covariance refers to the measure of how two random variables in a data set will change together. A positive covariance means that the two variables at hand are positively related, and they move in the same direction. A negative covariance means that the variables are inversely related, or that they move in opposite directions. Both variance and covariance measure how data points are distributed around a calculated mean. However, variance measures the spread of data along a single axis, while covariance examines the directional relationship between two variables.\nPopulation formula:\n\\[\\sigma_{xy} = \\dfrac{\\sum_{i=1}^N(x_i - \\mu_x)*(y_i - \\mu_y)}N \\]\nWhile sample formula is:\n\\[s_{xy} = \\dfrac{\\sum_{i=1}^n(x_i - \\bar x)*(y_i - \\bar y)}{n-1} \\]\n\nassets = ['META','AMZN','NFLX','GOOG','MSFT','NVDA','TSLA']\npf_data = pd.DataFrame()\nfor a in assets:\n    pf_data[a] = yf.download(a, start=\"2021-10-01\", end=\"2021-12-31\", index_col = 'Date', parse_dates=True)['Adj Close']\n\nreturns = pf_data.pct_change(1).dropna()\ncov = returns.cov()\ncorr = returns.corr()\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\ncov\n\n\n\n\n\n\n\n\nMETA\nAMZN\nNFLX\nGOOG\nMSFT\nNVDA\nTSLA\n\n\n\n\nMETA\n0.000404\n0.000137\n0.000143\n0.000135\n0.000134\n0.000252\n0.000189\n\n\nAMZN\n0.000137\n0.000253\n0.000075\n0.000142\n0.000111\n0.000362\n0.000064\n\n\nNFLX\n0.000143\n0.000075\n0.000319\n0.000098\n0.000115\n0.000196\n0.000151\n\n\nGOOG\n0.000135\n0.000142\n0.000098\n0.000221\n0.000177\n0.000284\n0.000074\n\n\nMSFT\n0.000134\n0.000111\n0.000115\n0.000177\n0.000207\n0.000287\n0.000182\n\n\nNVDA\n0.000252\n0.000362\n0.000196\n0.000284\n0.000287\n0.001286\n0.000490\n\n\nTSLA\n0.000189\n0.000064\n0.000151\n0.000074\n0.000182\n0.000490\n0.001463\n\n\n\n\n\n\n\n\n\nCorrelation coefficient\nCorrelation shows the strength of a relationship between two variables and is expressed numerically by the correlation coefficient. While covariance measures the direction of a relationship between two variables, correlation measures the strength of that relationship. There are many different measures of correlation but the most common one, and the one I use is the Pearson Coefficient of Correlation.\nOutput values of the Pearson Correlation Coefficient range between values of +1 and -1, or 100% and -100%, where +1 represents perfect positive correlation and -1 perfect negative correlation. A measure of 0 would suggest the two variables are perfectly uncorrelated, and there is no linear relationship between them. However, that doesn’t necessarily mean the variables are independent – as they might have a relationship that is not linear. Scatterplot charts are a good way of visualizing various values for correlation\nPopulation formula:\n\\[p = \\dfrac{\\sigma_{xy}}{\\sigma_y\\sigma_x} \\]\nWhile sample formula is:\n\\[r = \\dfrac{s_{xy}}{s_ys_x} \\]\n\ncorr\n\n\n\n\n\n\n\n\nMETA\nAMZN\nNFLX\nGOOG\nMSFT\nNVDA\nTSLA\n\n\n\n\nMETA\n1.000000\n0.426954\n0.399251\n0.451141\n0.462463\n0.349047\n0.245190\n\n\nAMZN\n0.426954\n1.000000\n0.263913\n0.599965\n0.486596\n0.635210\n0.105951\n\n\nNFLX\n0.399251\n0.263913\n1.000000\n0.369411\n0.449536\n0.306624\n0.221220\n\n\nGOOG\n0.451141\n0.599965\n0.369411\n1.000000\n0.829273\n0.532455\n0.130574\n\n\nMSFT\n0.462463\n0.486596\n0.449536\n0.829273\n1.000000\n0.556526\n0.331061\n\n\nNVDA\n0.349047\n0.635210\n0.306624\n0.532455\n0.556526\n1.000000\n0.357039\n\n\nTSLA\n0.245190\n0.105951\n0.221220\n0.130574\n0.331061\n0.357039\n1.000000\n\n\n\n\n\n\n\nOne of the most common pitfalls of correlation analysis is that correlation is not causation!\nJust because two variables have shown a historic correlation doesn’t mean that one of the variables causes the other to move. The causation of the two variables moving with a positive or negative correlation could be a third completely unconsidered variable OR a combination of many factors. In theory, we want to try and understand the causes for relationships between variables so we can have a more accurate idea about when those relationships might change and if they will. The reality is that this is very hard to achieve and so practically speaking correlation analysis is often used to summarise relationships and use them as forward-looking predicator under the caveat that we understand it is likely that there are many factors at play that are responsible for the causation of the relationship.\n\n\nReferences\n\n“Quantitative Investment Analysis”, by DeFusco, McLeavey, Pinto, and Runkle\n“Practical Statistics for Data Scientists”, by Andrew Bruce, Peter C. Bruce, and Peter Gedeck"
  },
  {
    "objectID": "Quant/2022-07-03-ARIMA-prediction.html",
    "href": "Quant/2022-07-03-ARIMA-prediction.html",
    "title": "Stock Price Forecast Using ARIMA Model",
    "section": "",
    "text": "In this notebook i’m going to forecast the stock price of The Procter & Gamble Company (Ticker PG) using the Auto Regressive Integrated Moving Average (ARIMA) model\n\n\ntoc: true\nbadges: true\ncategories:[Quantitative Research]\nimage:images/time series.png\n\n\nDISCLAIMER!\nBefore proceeding, please make sure that you note the following important information:\nNOT FINANCIAL ADVICE!\nMy content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice, None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument to make any investment or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur\n\n\nIntro on time series:\nA time series is a sequence of data that attaches a time period to each value, the value can be anything measurable that depends on time in some way like rainfall measurements, heart rate measurements, annual retail sales, monthly subscription, trading volumes, security prices, exchange rates and so on.\nAll we need for a time series is a starting point and an ending point(a time period), all time periods must be equal and clearly defined which would result in a constant frequency. The frequency is how often the values of the data set are recorded which could be hourly, daily, monthly, quarterly, etc. There are also no limitations regarding the total time span of our time series.\nTime series analysis comprises methods for analyzing time series data in order to extract meaningful statistics like observing patterns in the data while Time series forecasting is the use of a model to predict future values based on previously observed patterns.\nData can be univariate or multivariate, univariate time series is when we are forecasting the value of a single variable based on patterns observed in its own past history. For example, predicting the closing stock price of apple tomorrow using its own closing price in the past 7 trading days. While multivariate time series is when we are forecasting the value of a single variable considering patterns in parallel time series. For example, predicting the closing stock price of apple tomorrow using the closing price of Microsoft and Nvidia in the past 7 trading days.\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom pmdarima.arima.utils import ndiffs\nimport datetime\nfrom pandas_datareader import data as wb\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(\"ignore\")\n%matplotlib inline\n\n\n\nObjective:\nThe American economy contracted an annualized 1.6% in Q1 2022. It was the first contraction since the pandemic-induced recession in 2020 as record trade deficits, supply constraints, worker shortages, and high inflation weigh.\nYear to date, the S&P500 is down approximately 20% this officially puts it in a bear market territory. As fear of recession weighs in, historically the consumer staple sector usually fare well during these downturns as the idea is that companies in these sectors produce goods & services that consumers will buy regardless of the economic conditions.\nHence, the objective of today’s blog is to forecast the stock price of The Procter & Gamble Company (Ticker PG) using the Auto Regressive Integrated Moving Average (ARIMA) model.\nThe reasons why I picked PG is because they have a diverse pool of products mainly 5 segments, Beauty, Grooming, Health Care, Fabric &Home Care, and Baby, Feminine & Family Care, they also operate in a lot of regions with consumers from Latin America, Europe, the Asia Pacific, Greater China, India, the Middle East, and Africa.\nNOTE: depending on the level of risk one is willing to take with his/her portfolio, a safer bet would be to trade the overall sector using an ETF like the Consumer Staples Select Sector SPDR Fund(Ticker XLP) or Vanguard Consumer Staples ETF(Ticker VDC) in order to get exposure of the sector in one’s portfolio.\n\n\nWhat is an ARIMA model:\nARIMA stands for Autoregressive Integrated Moving Average. ARIMA is also known as the Box-Jenkins approach. it is a combination of an AR model, MA model, and differencing (Integration).\n\nThe components of an ARIMA model:\nIn an ARIMA(p,d,q) model, p indicates the number or order of AR terms, d indicates the number or order of differences, and q indicates the number or order of MA terms. The p, d, and q parameters are integers equal to or greater than 0\n\n\nAR:\nA correlation of a variable with itself at different time periods in the past is known as “Autocorrelation”. AR model uses past values to make future predictions. It is indicated by the “p” value in the ARIMA model. The lag “p” term signifies how many prior time periods that each observation is highly correlated to, Increasing “p” (longer lag) would increase the dependency on previous values further.\nif \\(Y\\) is a time series variable and AR(2) then the equation would look like this:\n\n\n\\(Y_{t} = c + \\varphi_{1}Y_{t−1} + \\varphi_{2}Y_{t−2} + ϵ_{t}\\)\nWhere: - \\(c\\) is a constant - \\(Y_{t−1}\\) is the values of \\(Y\\) during the previous period - \\(\\varphi\\) is the magnitude of the autocorrelation - \\(ϵ_{t}\\) is the residuals for the current period (the difference between our prediction for period \\(_{t}\\) and the correct value)\n\n\nMA:\nMA model uses past errors to make a prediction. The “q” term is the number of lagged values of the error term it\nif \\(Y\\) is a time series variable and MA(2) then the equation would look like this:\n\n\n\\(Y_{t} = c + θ_{1}ϵ_{t−1} + θ_{2}ϵ_{t−2} + ϵ_{t}\\)\nWhere:\n\n\\(c\\) is a constant\n\\(θ\\) is the value of the autocorrelation of the error\n\\(ϵ_{t}\\) is the residuals for the current period\n\\(ϵ_{t−1}\\) is the residuals for the past period\n\n\n\nI:\nThe I stands for Integrated and is used to difference the time series data to remove the trend and convert a non-stationary time series to a stationary one. This is indicated by the “d” value in the ARIMA model. Hence we are combining AR and MA techniques into a single integrated model in order to achieve weak stationarity\nif \\(Y\\) is a time series variable, then first order differencing equation would look like:\n\n\n\\(Y_{t}^′ = Y_{t} − Y_{t−1}\\)\nWhere:\n\n\\(Y_{t}^′\\) is the difference between adjacent observation\n\n\n\nARIMA equation:\nThus if \\(Y\\) is a time series variable and ARIMA(2,1,2) then the equation would be:\n\n\n\\(Y_{t}^′ = c + \\varphi_{1}Y_{t−1}^′ + \\varphi_{2}Y_{t−2}^′ + θ_{1}ϵ_{t−1} + θ_{2}ϵ_{t−2} + ϵ_{t}\\)\nAn ARIMA model with 0 degrees of integration is simply an ARMA model, and so any ARIMA (p, 0, q) model is equivalent to an ARMA (p,q). likewise, any ARIMA(p, 0, 0) is equivalent to an AR(p) model and any ARIMA(0, 0, q) is equivalent to an MA(q) model\n\n\n\nEDA:\nThe objective now is to perform an Exploratory Data Analysis(EDA) on the dataset (Closing prices of PG) to find the values of p, q, and d\nBefore performing EDA, I’m going to split the data first and work only with the train data set which I will use to perform the EDA on, while the test data will be used to evaluate the prediction\n\n# Read data\ndf = yf.download('PG', '2018-06-01', '2022-06-30', interval= '1d', auto_adjust=True)\n\n# Only keep close column\ndf = df[['Close']]\n\n# Drop rows with missing values\ndf = df.dropna()\n\n#make a copy of my data just incase i need the original data\ndf_copy = df.copy()\n\n#Setting frequency to days\ndf.index = pd.DatetimeIndex(df.index).to_period('D')\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nPeriodIndex: 1028 entries, 2018-05-31 to 2022-06-29\nFreq: D\nData columns (total 1 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   Close   1028 non-null   float64\ndtypes: float64(1)\nmemory usage: 16.1 KB\n\n\n90/10 split, 90% of data will be used to train and 10% to test\n\nt = .9\nt = int(t*len(df))\n\n# Train dataset\ntrain = df[:t]\n\n# Test dataset\ntest = df[t:]\nprint(\"number of test samples :\", test.shape[0])\nprint(\"number of training samples:\",train.shape[0])\n\nnumber of test samples : 103\nnumber of training samples: 925\n\n\n\n\nTest for stationarity and finding the “d” value:\nThe primary purpose of differencing in the ARIMA model is to make the Time Series stationary. But if the time series is already stationary then we would use the ARMA model instead because the “d” value will be 0.\nA stationary time series means a time series without a trend, one having a constant mean and variance over time,a stationary series (also called a “white noise process”) is easier to analyse as it can be modelled with fewer parameters. While it may fluctuate, it will always revert to a constant mean and is thus easier to predict.\nNOTE: A time series is considered strictly stationary if the probability distribution of a sequence of observations is unchanged by shifts in time. Strictly stationary series are rare, and it is often enough to assume weak stationarity.\nbut how would we know whether our data is stationary or not?\nWe can check for stationarity visually and statistically\n\nVisual test:\n\ntrain['rolmean'] = train['Close'].rolling(20).mean()\ntrain['rolstd'] = train['Close'].rolling(20).std()\n\n\nplt.style.use('dark_background')\nplt.figure(figsize=(12, 6))\norig = plt.plot(df_copy.index[:925], train['Close'], color='red')\nplt.title('Price')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\nplt.style.use('dark_background')\nplt.figure(figsize=(12, 6))\nmean = plt.plot(df_copy.index[:925],train['rolmean'], color='red')\nplt.title('Rolling Mean')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\nplt.style.use('dark_background')\nplt.figure(figsize=(12, 6))\nstd = plt.plot(df_copy.index[:925],train['rolstd'], color='red')\nplt.title('Rolling Standard Deviation')\nplt.grid(False)\n\n\n\n\n\n\n\n\nVisually we can see clearly that PG has been trending higher for the past 5 years, it’s been making higher highs and higher lows, and on a monthly bases, the mean is also trending higher and the standard deviation is also not constant and fluctuates over time\n\n\nStatistical test:\nTo check for stationary statistically we use the Augmented Dickey-Fuller(ADF) which is more accurate than making a visual observation.\nIn statistics and econometrics, an augmented Dickey–Fuller test is used to test whether a given time series is stationary or not.\nThe p-value resulting from the ADF test has to be less than 0.05 or 5% for a time series to be stationary. If the p-value is greater than 0.05 or 5%, you conclude that the time series has a unit root which means that it is a non-stationary process.\n\nresult = adfuller(df['Close'])\nprint('Augmented Dickey-Fuller Test:')\nlabels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\nfor value,label in zip(result,labels):\n    print(label+' : '+str(value) )\n    \nif result[1] &lt;= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -1.7582424618957933\np-value : 0.4013812122884916\n#Lags Used : 9\nNumber of Observations Used : 1018\nweak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \n\n\nWe have made a mathematical and empirical observation that our time series is not stationary hence we rule out the idea of using an ARMA model because we in fact do need to difference our time series inorder to achieve (weak) stationarity\n\n\nDifferencing:\nDifferencing is a method of making a times series dataset stationary, by subtracting the observation in the previous time step from the current observation. This process can be repeated more than once, and the number of times differencing is performed is called the difference order.\nLet’s look at the equation again:\n\n\n\\(Y_{t}^′ = Y_{t} − Y_{t−1}\\)\nSo now the question is at what difference order? Once, twice?. The aim is to avoid over-differencing because it can lead to loss of valuable information about the time series and this often affects the construction of the model\nWe could repeat the visual and statistical test every time we difference our time series but fortunately, there is a helpful package we can use called pmdarima and it will tell us at what order should we difference our time series (under the hood it repeats our statistical test and constantly checks at what order will the null hypothesis be rejected)\n\n# from pmdarima.arima.utils import ndiffs\nndiffs(train['Close'], test=\"adf\")\n\n1\n\n\nSo in order to make the series stationary we only need to difference it once!\n\ndiff = train['Close'].diff().dropna()\ndiff_rolmean = diff.rolling(20).mean()\ndiff_rolstd = diff.rolling(20).std()\n\nplt.figure(figsize=(12, 6))\nplt.plot(df_copy.index[:924],diff, color='red', label='Original')\nplt.plot(diff_rolmean, color='green', label='Rolling Mean')\nplt.plot(diff_rolstd, color='blue', label = 'Rolling Std Deviation')\nplt.title('1st order difference')\nplt.legend(loc='best')\nplt.grid(False)\n\nresult2 = adfuller(diff)\nprint('Augmented Dickey-Fuller Test:')\n\nfor value,label in zip(result2,labels):\n    print(label+' : '+str(value) )\n    \nif result2[1] &lt;= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -10.202850337835578\np-value : 5.90798356244438e-18\n#Lags Used : 8\nNumber of Observations Used : 915\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\n\n\n\n\n\n\nNote: after differencing we can see it does look stationary but it is obvious that there are huge fluctuations in mid-2020 this was due to volatility being extremely high, in fact, On March 16, 2020, the VIX closed at a record high of 82.69 as investors and traders reacted to the pandemic.\nSo now we have the “d” value in the ARIMA which is 1\nARIMA(p,1,q)\n\n\n\nFinding the “p” and “q” value using PACF and ACF:\nThe next step is to determine the appropriate order of AR (p) and MA(q) processes by using the Partial Autocorrelation function (PACF)and Autocorrelation function (ACF)\n\nPACF for AR(p):\nThe order, p, of the autoregressive model can be determined by looking at the partial autocorrelation function (PACF) plot. Partial autocorrelation can be imagined as the correlation between the series and its lag, after excluding the contributions from the intermediate lags. In other words, partial autocorrelation is the relation between observed at two-time spots given that we consider both observations are correlated to the observations at other time spots. For example, today’s stock price can be correlated to the day before yesterday, and yesterday can also be correlated to the day before yesterday… day 1 with day 3, day 2 with day 3\n\n\nACF for MA(q):\nThe order q can be determined by the Autocorrelation Function plot (ACF) which checks for correlation between two different data points of a Time Series separated by a lag “h”. For example, for lag 1, today’s stock price can be correlated with yesterday’s and yesterday’s stock price and can be correlated with the day before yesterday… day 1 with day 2, day 2 with day 3, etc, etc\n\nplt.style.use('dark_background')\n\n#PACF to find p\nfig, ax = plt.subplots(figsize=(12,5))\nplot_pacf(diff, lags=20, ax=ax,color='red')\nplt.grid(False)\n\n#ACF to find q\nfig, ax = plt.subplots(figsize=(12,5))\nplot_acf(diff, lags=20, ax=ax, color='red') \nplt.grid(False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPACF and ACF Plot:\nThe light/shaded area shows the significant threshold value, and every vertical line indicates the PACF and ACF values at each time spot. So in the plot, only the vertical lines that exceed the light/shaded red area are considered significant.\nFor PACF we can see that the PACF lag 1,4,7,9 and 15 are significant, but I will pick lag 7 since it is well above the significance line compared to the rest. So, we will set the AR(p) value equal to 7.\nWhile for the ACF, the lags 1,4,7 and 9 are significant, but lag 4 seems to be well above the significance line compared to the other lags. So, we will set the MA(q) value equal to 4.\nNOTE: Before we continue something useful to remember is that the ACF can also be used to check for stationarity, for a stationary series, the autocorrelation in the ACF plot should decay quickly; with a non-stationary series, the ACF will decay slowly as we’ve already seen\n\n#ACF example for checking stationarity\nplt.style.use('dark_background')\nfig, ax = plt.subplots(figsize=(12,3))\nplot_acf(train['Close'], lags=50, ax=ax,color='red')\nplt.grid(False)\n\n\n\n\n\n\n\n\nSo we have our “p” and “q” values, which are 7 and 4.\nARIMA(7,1,4)\n\n\n\nModel development & Evaluation:\nNow that we have determined the parameters (p,d,q), we will estimate the accuracy of the ARIMA model on a training data set and then use the fitted model to forecast the values of the test data set using a forecasting function. In the end, we cross-check whether our forecasted values are in line with the actual values.\n\n# ARIMA Model\nmodel = ARIMA(train['Close'], order=(7,1,4))\nresult = model.fit()\n\n\n#Histogram to plot residuals\nresiduals = pd.DataFrame({'residuals':result.resid})\nplt.style.use('dark_background')\nresiduals[1:].plot(kind='hist',bins=50,figsize=(12,5),color='red')\nplt.title('Residuals')\nplt.grid(False)\n\n\n\n\n\n\n\n\nThe residuals are the error based of what it would predict for the train data, the closer the residuals are to zero the better\n\npred = result.get_forecast('2022-06-29').summary_frame()\npred = pred['2022-02-01':]\n\nplt.style.use('dark_background')\npred['mean']['2022-02-01':].plot(figsize=(12,8),color='black',label='Forecast')\ntest['Close'].plot(figsize=(12,8),color='red',label='Test Price')\ntrain['Close'].plot(figsize=(12,8),color='blue',label='Train Price')\n\n#plt.fill_between(pred.index, pred['mean_ci_lower'], pred['mean_ci_upper'], color='k', alpha=0.1);\nplt.fill_between(pred.index, pred['mean_ci_lower'], pred['mean_ci_upper'], \n                 color='gray')\nplt.title('ARIMA Price Forecast and Confidence Interval')\nplt.legend(loc='best')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\npred['mean']['2022-02-01':]\n\n2022-02-01    159.739671\n2022-02-02    159.739671\n2022-02-03    159.739671\n2022-02-04    159.739671\n2022-02-05    159.739671\n                 ...    \n2022-06-25    159.739671\n2022-06-26    159.739671\n2022-06-27    159.739671\n2022-06-28    159.739671\n2022-06-29    159.739671\nFreq: D, Name: mean, Length: 149, dtype: float64\n\n\nThe values seem to be flat, hence the prediction is far from accurate at the very least I was expecting the model to predict a trend in either direction\nA possible solution to get a more accurate result using the ARIMA would be to test with different lags, remember we got several lags that we could’ve used, we can test a combination of all of them to find the one which can offer better result\nBut obviously, this will take time, a solution to that is to use the same package that we used to get the number of differencing needed to make our time series stationary.\n\n\nAUTO ARIMA:\nWe can implement the Auto ARIMA model using the pmdarima time-series library which provides the auto_arima() function that automatically generates the optimal parameter values for all 3 p,d,q values\nThe auto_arima() function can take a number of paramters but to keep it simple i’ll just run the function and get the default best model, all we need to do is just pass in our time series data.\n\nfrom pmdarima.arima import auto_arima\nauto_arima = auto_arima(train[\"Close\"])\nauto_arima\n\nARIMA(order=(4, 1, 5), scoring_args={}, suppress_warnings=True)\n\n\n\nprint(f'The default best value is {auto_arima.fit(train[\"Close\"])}')\n\nThe default best value is  ARIMA(4,1,5)(0,0,0)[0] intercept\n\n\n\n# ARIMA Model\nmodel2 = ARIMA(train['Close'], order=(4,1,5))\nresult2 = model2.fit()\n\n\n#Histogram\nresiduals2 = pd.DataFrame({'residuals':result2.resid})\nplt.style.use('dark_background')\nresiduals2[1:].plot(kind='hist',bins=50,figsize=(12,5),color='red')\nplt.title('Residuals')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\npred2 = result2.get_forecast('2022-06-29').summary_frame()\npred2 = pred2['2022-02-01':] \n\nplt.style.use('dark_background')\npred2['mean']['2022-02-01':].plot(figsize=(12,8),color='black',label='Forecast')\ntest['Close'].plot(figsize=(12,8),color='red',label='Test Price')\ntrain['Close'].plot(figsize=(12,8),color='blue',label='Train Price')\n\nplt.fill_between(pred2.index, pred2['mean_ci_lower'], pred2['mean_ci_upper'], \n                 color='gray')\nplt.title('ARIMA Price Forecast and Confidence Interval')\nplt.legend(loc='best')\nplt.grid(False)\n\n\n\n\n\n\n\n\n\npred2['mean']['2022-02-01':]\n\n2022-02-01    159.472243\n2022-02-02    159.472243\n2022-02-03    159.472243\n2022-02-04    159.472243\n2022-02-05    159.472243\n                 ...    \n2022-06-25    159.472243\n2022-06-26    159.472243\n2022-06-27    159.472243\n2022-06-28    159.472243\n2022-06-29    159.472243\nFreq: D, Name: mean, Length: 149, dtype: float64\n\n\n\n\nConclusion:\nEven after the default best model, the values haven’t changed much as they look constant from afar.\nStock data is complex and are usually composed of linear and non-linear components. So just using a linear model to make predictions will not be efficient, especially using ARIMA to predict prices as we saw didn’t provide a realistic prediction, they may however be useful to predict the overall trend of a time series.\nAlternatively, we could try and predict stock returns instead because returns are considered stationary(at least visually) or we could use the SARIMA (Seasonal Auto-Regressive Integrated Moving Average) model which is an extension of the ARIMA, it adds seasonal components of the time series that can help improve prediction.\nBut again something to remember is that even if we continue to fine-tune the ARIMA and SARIMA models, the prediction could still be inaccurate because we will still be using univariate data, in order to get the best predictions it’s better to use multivariate data like using the closing prices or stock return of other correlated consumer staple stocks like Colgate-Palmolive Company (TICKER: CL) or The Clorox Company (TICKER: CLX) using models like the ARIMAX or SARIMAX which are just an extension of the ARIMA and SARIMA models, The ‘X’ stands for an exogenous variable(or variables) and in this example, CL and CLX could be those exogenous variables.\nFor now, my knowledge is still limited, more time is needed to research and learn about other models as well learning in-depth analysis and forecasting techniques, hence, In future blogs, I will try and incorporate all relevant models and leverage the pmdarima package even further by adding extra parameters to try to find the most accurate model"
  },
  {
    "objectID": "Quant/2022-05-07-Tech-Stocks-Portfolio.html",
    "href": "Quant/2022-05-07-Tech-Stocks-Portfolio.html",
    "title": "Portfolio Analysis, Efficient Frontier & Monte Carlo",
    "section": "",
    "text": "In this notebook, I’ll be constructing and analyzing an equal-weighted portfolio which will constitute a list of my favorite large-cap tech stocks that I had already screened using various metrics like debt to asset ratio, investor yield and other metrics. I will then use a monte carlo simulation to find the optimal weightings of the tech stocks which i will compare against certain benchmarks and pick the best one out of the lot\n\n\ntoc: true\nbadges: true\ncategories:[Quantitative Research]\nimage:images/pic.png\nuse_plotly: true\n\n\nDISCLAIMER!\nBefore proceeding, please make sure that you note the following important information:\nNOT FINANCIAL ADVICE!\nMy content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument, to make any investment, or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur\n\n#Importing all the relevant librabries and modules\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom pandas_datareader import data as wb\nfrom chart_studio import plotly as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom IPython.display import HTML\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)  \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('seaborn-darkgrid')\n\n\n\nPortfolio construction and analysis:\n\n# importing the data from yahoo finance and exploring the data\nassets = ['TXN','CSCO','INTC','AAPL','MSFT',\n          'NVDA','INFY','INTU','SAP','ADI',\n          'ANSS','CRM','ADBE','FB','AMD',\n          'AMZN','MA','VMW','GOOG','SNPS']\npf_data = pd.DataFrame()\nfor a in assets:\n    pf_data[a] = yf.download(a, start=\"2012-05-20\", end=\"2021-12-31\", index_col = 'Date', parse_dates=True)['Adj Close'] \n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\npf_data.head()\n\n\n\n\n\n\n\n\nTXN\nCSCO\nINTC\nAAPL\nMSFT\nNVDA\nINFY\nINTU\nSAP\nADI\nANSS\nCRM\nADBE\nFB\nAMD\nAMZN\nMA\nVMW\nGOOG\nSNPS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012-05-21\n22.610977\n12.278630\n19.480406\n17.139450\n24.290533\n2.821950\n4.160825\n50.348274\n49.764915\n28.088888\n61.000000\n37.262501\n32.009998\n34.029999\n6.30\n218.110001\n38.773224\n69.075218\n305.908386\n28.040001\n\n\n2012-05-22\n22.518942\n12.322823\n19.391010\n17.007828\n24.298697\n2.787507\n4.215657\n51.022617\n49.680264\n28.034071\n61.750000\n37.362499\n32.009998\n31.000000\n6.16\n215.330002\n39.043331\n68.748131\n299.278229\n28.040001\n\n\n2012-05-23\n22.350197\n12.293363\n18.951487\n17.422827\n23.767975\n2.856392\n4.168488\n51.460045\n49.773380\n27.940096\n62.119999\n37.652500\n32.180000\n32.000000\n6.08\n217.279999\n39.388008\n69.177422\n303.592072\n28.200001\n\n\n2012-05-24\n22.158449\n12.072392\n19.107927\n17.262810\n23.735321\n2.780620\n4.243171\n51.387135\n48.842247\n28.190681\n61.689999\n36.552502\n31.540001\n33.029999\n6.02\n215.240005\n39.646744\n65.027603\n300.702881\n29.850000\n\n\n2012-05-25\n22.196800\n12.028200\n19.174969\n17.170284\n23.727154\n2.847208\n4.172420\n51.441814\n48.308960\n28.339464\n62.090000\n36.750000\n31.600000\n31.910000\n6.22\n212.889999\n39.092426\n64.066803\n294.660553\n29.889999\n\n\n\n\n\n\n\n\n#Check to see if there are any missing values\npf_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 2420 entries, 2012-05-21 to 2021-12-30\nData columns (total 20 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   TXN     2420 non-null   float64\n 1   CSCO    2420 non-null   float64\n 2   INTC    2420 non-null   float64\n 3   AAPL    2420 non-null   float64\n 4   MSFT    2420 non-null   float64\n 5   NVDA    2420 non-null   float64\n 6   INFY    2420 non-null   float64\n 7   INTU    2420 non-null   float64\n 8   SAP     2420 non-null   float64\n 9   ADI     2420 non-null   float64\n 10  ANSS    2420 non-null   float64\n 11  CRM     2420 non-null   float64\n 12  ADBE    2420 non-null   float64\n 13  FB      2420 non-null   float64\n 14  AMD     2420 non-null   float64\n 15  AMZN    2420 non-null   float64\n 16  MA      2420 non-null   float64\n 17  VMW     2420 non-null   float64\n 18  GOOG    2420 non-null   float64\n 19  SNPS    2420 non-null   float64\ndtypes: float64(20)\nmemory usage: 397.0 KB\n\n\nI’ll use a built-in method in DataFrame that computes the percent change from one row to another\n\n#Calculate the daily returns of the stocks in the portfolio\nreturns = pf_data.pct_change(1).dropna()\nreturns\n\n\n\n\n\n\n\n\nTXN\nCSCO\nINTC\nAAPL\nMSFT\nNVDA\nINFY\nINTU\nSAP\nADI\nANSS\nCRM\nADBE\nFB\nAMD\nAMZN\nMA\nVMW\nGOOG\nSNPS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012-05-22\n-0.004070\n0.003599\n-0.004589\n-0.007679\n0.000336\n-0.012206\n0.013178\n0.013394\n-0.001701\n-0.001952\n0.012295\n0.002684\n0.000000\n-0.089039\n-0.022222\n-0.012746\n0.006966\n-0.004735\n-0.021674\n0.000000\n\n\n2012-05-23\n-0.007493\n-0.002391\n-0.022666\n0.024400\n-0.021842\n0.024712\n-0.011189\n0.008573\n0.001874\n-0.003352\n0.005992\n0.007762\n0.005311\n0.032258\n-0.012987\n0.009056\n0.008828\n0.006244\n0.014414\n0.005706\n\n\n2012-05-24\n-0.008579\n-0.017975\n0.008255\n-0.009184\n-0.001374\n-0.026527\n0.017916\n-0.001417\n-0.018707\n0.008969\n-0.006922\n-0.029214\n-0.019888\n0.032187\n-0.009868\n-0.009389\n0.006569\n-0.059988\n-0.009517\n0.058511\n\n\n2012-05-25\n0.001731\n-0.003661\n0.003509\n-0.005360\n-0.000344\n0.023947\n-0.016674\n0.001064\n-0.010919\n0.005278\n0.006484\n0.005403\n0.001902\n-0.033909\n0.033223\n-0.010918\n-0.013981\n-0.014775\n-0.020094\n0.001340\n\n\n2012-05-29\n0.014513\n0.015922\n0.013598\n0.017749\n0.017206\n0.025806\n0.023316\n0.003189\n0.020151\n0.019619\n0.015623\n-0.008912\n0.014240\n-0.096208\n0.038585\n0.008737\n0.005048\n0.009253\n0.004750\n0.009368\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-12-23\n0.002514\n0.012189\n0.006671\n0.003644\n0.004472\n0.008163\n0.008153\n0.006412\n0.004878\n0.002148\n0.007363\n0.001345\n0.010000\n0.014495\n0.015707\n0.000184\n0.008672\n0.012912\n0.001317\n0.003618\n\n\n2021-12-27\n0.023693\n0.018304\n0.012278\n0.022975\n0.023186\n0.044028\n0.024262\n0.026836\n0.008638\n0.016275\n0.018905\n0.020384\n0.014150\n0.032633\n0.056247\n-0.008178\n-0.000749\n-0.005219\n0.006263\n0.025782\n\n\n2021-12-28\n-0.003857\n0.001734\n-0.003466\n-0.005767\n-0.003504\n-0.020133\n-0.001184\n-0.004580\n0.001557\n-0.006212\n-0.003161\n-0.011034\n-0.014402\n0.000116\n-0.007839\n0.005844\n0.001304\n0.010320\n-0.010914\n-0.009159\n\n\n2021-12-29\n-0.001518\n0.006768\n0.001353\n0.000502\n0.002051\n-0.010586\n0.003162\n-0.002693\n-0.010388\n0.006537\n-0.006977\n-0.003562\n-0.000123\n-0.009474\n-0.031929\n-0.008555\n0.001414\n0.003405\n0.000386\n0.003466\n\n\n2021-12-30\n-0.007337\n-0.005316\n-0.001736\n-0.006578\n-0.007691\n-0.013833\n0.001182\n-0.007206\n0.002571\n-0.004216\n-0.003390\n0.003104\n0.002178\n0.004141\n-0.020977\n-0.003289\n-0.000830\n-0.005260\n-0.003427\n-0.007043\n\n\n\n\n2419 rows × 20 columns\n\n\n\nI expected the stocks in this portfolio to have a large number of positive correlations considering they are all part of the same sector in fact no pair is negatively correlated\nThe top 3 most correlated stocks are: Analog devices and Texas Instruments which have a strong positive correlation with 0.80 both companies are in the business of designing and fabrication of semiconductors and semiconductor devices which is a sub-industry of the overall technology sector,\nThe other two sets of stocks have a moderate positive correlation which is Ansys Inc, and Synopsys inc with 0.69 and Synopsys inc again but this time with financial software company Intuit inc with 0.66\n\ncorr = returns.corr()\nfig = px.imshow(corr)\nfig.update_layout(width=1000, height=800)\nfig.update_layout(template = \"plotly_dark\", title = 'The Correlation coefficient of the Assets in the Portfolio') \nfig.show()\n\n                                                \n\n\n\ncorr.unstack().sort_values().drop_duplicates()\n\nFB    AMD     0.234715\nAMD   INFY    0.235012\nFB    INFY    0.236786\nINFY  AMZN    0.248299\nVMW   AMD     0.264527\n                ...   \nMSFT  ADBE    0.662720\nSNPS  INTU    0.672901\nANSS  SNPS    0.698040\nADI   TXN     0.803991\nTXN   TXN     1.000000\nLength: 191, dtype: float64\n\n\n\n\nCreating an equal weight (EW) portfolio:\nEqual weight is a type of proportional measuring method that gives the same importance to each stock in a portfolio, index, or index fund. So stocks of the smallest companies are given equal statistical significance, or weight, to the largest companies when it comes to evaluating the overall group’s performance.\n\n#Equal weighted portfolio\nN = len(returns.columns)\nequal_weights = N * [1/N] # Shows 1/20, 20 times. Its not multiplication, but repetition! 20*[\"A\"]\nequal_weights\n\n[0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05,\n 0.05]\n\n\n\n#Calculate portfolio daily return\nportfolio_return = returns.dot(equal_weights)\nportfolio_return\n\nDate\n2012-05-22   -0.006508\n2012-05-23    0.003661\n2012-05-24   -0.004807\n2012-05-25   -0.002338\n2012-05-29    0.008578\n                ...   \n2021-12-23    0.006743\n2021-12-27    0.019035\n2021-12-28   -0.004217\n2021-12-29   -0.002838\n2021-12-30   -0.004248\nLength: 2419, dtype: float64\n\n\n\n#Formatting the dates to column and not index\npf_data.index\ndates = pf_data.index.to_frame().reset_index(drop=True)\n\nThe returns were noticeably volatile in 2018 November as that year a lot was happening like the federal reserve interest hike but the most notable event was a lot of the big tech were under scrutiny at the time and considering this is a tech portfolio the volatility shouldn’t be surprising\nAnother noticeable moment here is the pandemic in 2020, volatility was extremely high, in fact, On March 16, 2020, the VIX closed at a record high of 82.69 The markets were tumbling and a lot of trades were being made, some were covering short positions while others buying “the dip” and last but not least you have countless algorithims and retail traders day trading and taking advantage of the high volatility\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=dates['Date'], y=portfolio_return,\n                    mode='lines',\n                    line=dict(color='firebrick',width=2),\n                    name='lines'))\nfig.update_layout(template = \"plotly_dark\")\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));                    \n\n                                                \n\n\n\n#calculating the cummulative retrun of the equal weight portfolio\ncum_equal_returns =   (1 + portfolio_return).cumprod() - 1\ncum_equal_returns_perc = pd.Series(100 * cum_equal_returns)\n\nThe EW has done pretty well returning more than 1000%!\n\nfig = go.Figure([go.Scatter(x=dates['Date'], y=cum_equal_returns_perc)])\nfig.update_layout(template = \"plotly_dark\", title = 'Cummulative % Return') \nfig.show()\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\n                                                \n\n\n\n\nSharpe Ratio\nIn the next step i am going to calcluate sharpe ratio, but first we need the annual volatility, annual return and risk free rate\n\n#Calculating the mean return and volatility\nER = portfolio_return.mean()\nSTD = portfolio_return.std()\n\n\n#Annualzing the volatility\nASTD = STD * 252 ** 0.5\nASTD\n\n0.21152541130551866\n\n\n\n#Annualzing the expected return \nAER = ER * 252 \nAER\n\n0.2784416783450839\n\n\n\n#Annual sharpe ratio\nrf = 0.03 #risk free rate is the 10 year trasury bond as of april 2022\nexcess_return = AER - rf\nSR = excess_return/ASTD\nSR\n\n1.1745240291070507\n\n\nA Sharpe ratio of 1.17 is not the best but also considering that tech stocks are very volatile maybe there is a weight combination that would have a higher Sharpe ratio and/or lower volatility or even a higher expected return and that is what I’ll try to uncover in the next section\n\n\nModern Portfolio Theory & Monte Carlo simulation:\nMonte Carlo simulations are used to model the probability of different outcomes in a process that cannot easily be predicted due to the intervention of random variables. It is a technique used to understand the impact of risk and uncertainty in prediction and forecasting models.\nModern portfolio theory refers to the quantitative practice of asset allocation that maximizes projected (ex ante) return for a portfolio while holding constant its overall exposure to risk. Or, inversely, minimizing overall risk for a given target portfolio return. The theory considers the covariance of constituent assets or asset classes within a portfolio, and the impact of an asset allocation change on the overall expected risk/return profile of the portfolio.\nThe theory was originally proposed by nobel-winning economist Harry Markowitz in the 1952 Journal of Finance, and is now a cornerstone of portfolio management practice. Modern portfolio theory generally supports a practice of diversifying toward a mix of assets and asset classes with a low degree of mutual correlation.\nHence, I’m going to find the optimal portfolio using Monte Carlo simulations by building thousands of portfolios, using randomly assigned weights, and visualizing the results.\n\n#Saving a variable which will have the number of stocks in my portfolio which i will use later\nnum_assets = len(pf_data.columns)\nnum_assets\n\n20\n\n\n\n#calculating the log returns of the stocks in the portfolio\nlog_returns = np.log(pf_data/pf_data.shift(1))\nlog_returns.head()\n\n\n\n\n\n\n\n\nTXN\nCSCO\nINTC\nAAPL\nMSFT\nNVDA\nINFY\nINTU\nSAP\nADI\nANSS\nCRM\nADBE\nFB\nAMD\nAMZN\nMA\nVMW\nGOOG\nSNPS\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2012-05-21\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2012-05-22\n-0.004079\n0.003593\n-0.004599\n-0.007709\n0.000336\n-0.012280\n0.013093\n0.013305\n-0.001702\n-0.001953\n0.012220\n0.002680\n0.000000\n-0.093255\n-0.022473\n-0.012828\n0.006942\n-0.004746\n-0.021912\n0.000000\n\n\n2012-05-23\n-0.007521\n-0.002394\n-0.022927\n0.024107\n-0.022083\n0.024411\n-0.011252\n0.008536\n0.001873\n-0.003358\n0.005974\n0.007732\n0.005297\n0.031749\n-0.013072\n0.009015\n0.008790\n0.006225\n0.014311\n0.005690\n\n\n2012-05-24\n-0.008616\n-0.018139\n0.008221\n-0.009227\n-0.001375\n-0.026885\n0.017758\n-0.001418\n-0.018885\n0.008929\n-0.006946\n-0.029650\n-0.020089\n0.031680\n-0.009917\n-0.009433\n0.006548\n-0.061863\n-0.009562\n0.056863\n\n\n2012-05-25\n0.001729\n-0.003668\n0.003502\n-0.005374\n-0.000344\n0.023665\n-0.016815\n0.001063\n-0.010979\n0.005264\n0.006463\n0.005389\n0.001901\n-0.034497\n0.032683\n-0.010978\n-0.014080\n-0.014886\n-0.020299\n0.001339\n\n\n\n\n\n\n\n\n#the covariance matrix of the stocks\ncov = log_returns.cov()\ncov\n\n\n\n\n\n\n\n\nTXN\nCSCO\nINTC\nAAPL\nMSFT\nNVDA\nINFY\nINTU\nSAP\nADI\nANSS\nCRM\nADBE\nFB\nAMD\nAMZN\nMA\nVMW\nGOOG\nSNPS\n\n\n\n\nTXN\n0.000283\n0.000149\n0.000206\n0.000162\n0.000165\n0.000267\n0.000120\n0.000163\n0.000135\n0.000244\n0.000171\n0.000170\n0.000175\n0.000147\n0.000268\n0.000141\n0.000157\n0.000150\n0.000140\n0.000161\n\n\nCSCO\n0.000149\n0.000248\n0.000156\n0.000132\n0.000144\n0.000181\n0.000104\n0.000137\n0.000117\n0.000147\n0.000131\n0.000150\n0.000143\n0.000112\n0.000170\n0.000115\n0.000141\n0.000148\n0.000117\n0.000125\n\n\nINTC\n0.000206\n0.000156\n0.000349\n0.000157\n0.000176\n0.000243\n0.000121\n0.000161\n0.000134\n0.000202\n0.000161\n0.000155\n0.000166\n0.000145\n0.000229\n0.000131\n0.000152\n0.000150\n0.000138\n0.000152\n\n\nAAPL\n0.000162\n0.000132\n0.000157\n0.000317\n0.000167\n0.000223\n0.000107\n0.000154\n0.000123\n0.000165\n0.000157\n0.000161\n0.000167\n0.000169\n0.000218\n0.000154\n0.000150\n0.000128\n0.000148\n0.000145\n\n\nMSFT\n0.000165\n0.000144\n0.000176\n0.000167\n0.000262\n0.000233\n0.000109\n0.000178\n0.000135\n0.000164\n0.000170\n0.000189\n0.000199\n0.000160\n0.000212\n0.000168\n0.000162\n0.000147\n0.000168\n0.000161\n\n\nNVDA\n0.000267\n0.000181\n0.000243\n0.000223\n0.000233\n0.000664\n0.000142\n0.000232\n0.000170\n0.000276\n0.000236\n0.000260\n0.000262\n0.000227\n0.000473\n0.000216\n0.000208\n0.000203\n0.000201\n0.000231\n\n\nINFY\n0.000120\n0.000104\n0.000121\n0.000107\n0.000109\n0.000142\n0.000334\n0.000124\n0.000107\n0.000121\n0.000116\n0.000124\n0.000119\n0.000100\n0.000159\n0.000085\n0.000124\n0.000108\n0.000096\n0.000107\n\n\nINTU\n0.000163\n0.000137\n0.000161\n0.000154\n0.000178\n0.000232\n0.000124\n0.000282\n0.000132\n0.000166\n0.000182\n0.000196\n0.000202\n0.000159\n0.000213\n0.000146\n0.000178\n0.000143\n0.000149\n0.000176\n\n\nSAP\n0.000135\n0.000117\n0.000134\n0.000123\n0.000135\n0.000170\n0.000107\n0.000132\n0.000257\n0.000138\n0.000136\n0.000152\n0.000150\n0.000122\n0.000181\n0.000119\n0.000139\n0.000134\n0.000118\n0.000124\n\n\nADI\n0.000244\n0.000147\n0.000202\n0.000165\n0.000164\n0.000276\n0.000121\n0.000166\n0.000138\n0.000325\n0.000178\n0.000179\n0.000183\n0.000151\n0.000277\n0.000135\n0.000167\n0.000151\n0.000142\n0.000168\n\n\nANSS\n0.000171\n0.000131\n0.000161\n0.000157\n0.000170\n0.000236\n0.000116\n0.000182\n0.000136\n0.000178\n0.000286\n0.000198\n0.000202\n0.000150\n0.000226\n0.000154\n0.000166\n0.000150\n0.000147\n0.000184\n\n\nCRM\n0.000170\n0.000150\n0.000155\n0.000161\n0.000189\n0.000260\n0.000124\n0.000196\n0.000152\n0.000179\n0.000198\n0.000434\n0.000248\n0.000214\n0.000246\n0.000192\n0.000187\n0.000192\n0.000169\n0.000179\n\n\nADBE\n0.000175\n0.000143\n0.000166\n0.000167\n0.000199\n0.000262\n0.000119\n0.000202\n0.000150\n0.000183\n0.000202\n0.000248\n0.000340\n0.000192\n0.000249\n0.000190\n0.000177\n0.000170\n0.000171\n0.000189\n\n\nFB\n0.000147\n0.000112\n0.000145\n0.000169\n0.000160\n0.000227\n0.000100\n0.000159\n0.000122\n0.000151\n0.000150\n0.000214\n0.000192\n0.000521\n0.000201\n0.000199\n0.000161\n0.000141\n0.000186\n0.000149\n\n\nAMD\n0.000268\n0.000170\n0.000229\n0.000218\n0.000212\n0.000473\n0.000159\n0.000213\n0.000181\n0.000277\n0.000226\n0.000246\n0.000249\n0.000201\n0.001315\n0.000217\n0.000204\n0.000211\n0.000167\n0.000230\n\n\nAMZN\n0.000141\n0.000115\n0.000131\n0.000154\n0.000168\n0.000216\n0.000085\n0.000146\n0.000119\n0.000135\n0.000154\n0.000192\n0.000190\n0.000199\n0.000217\n0.000350\n0.000152\n0.000132\n0.000178\n0.000141\n\n\nMA\n0.000157\n0.000141\n0.000152\n0.000150\n0.000162\n0.000208\n0.000124\n0.000178\n0.000139\n0.000167\n0.000166\n0.000187\n0.000177\n0.000161\n0.000204\n0.000152\n0.000276\n0.000147\n0.000154\n0.000154\n\n\nVMW\n0.000150\n0.000148\n0.000150\n0.000128\n0.000147\n0.000203\n0.000108\n0.000143\n0.000134\n0.000151\n0.000150\n0.000192\n0.000170\n0.000141\n0.000211\n0.000132\n0.000147\n0.000471\n0.000127\n0.000134\n\n\nGOOG\n0.000140\n0.000117\n0.000138\n0.000148\n0.000168\n0.000201\n0.000096\n0.000149\n0.000118\n0.000142\n0.000147\n0.000169\n0.000171\n0.000186\n0.000167\n0.000178\n0.000154\n0.000127\n0.000250\n0.000135\n\n\nSNPS\n0.000161\n0.000125\n0.000152\n0.000145\n0.000161\n0.000231\n0.000107\n0.000176\n0.000124\n0.000168\n0.000184\n0.000179\n0.000189\n0.000149\n0.000230\n0.000141\n0.000154\n0.000134\n0.000135\n0.000241\n\n\n\n\n\n\n\n\nnum_ports = 20000 #the number of trials i will run\nall_weights = np.zeros((num_ports,num_assets))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    #weigths\n    weights = np.array(np.random.random(num_assets))\n    weights = weights/np.sum(weights)\n\n    #save weigths\n    all_weights[ind,:] = weights\n\n    #expected return\n    ret_arr[ind] = np.sum((log_returns.mean() * weights) * 250)\n\n    #expected volatility\n    vol_arr[ind] = np.sqrt(np.dot(weights.T,np.dot(log_returns.cov()*250,weights)))\n\n    #sharpe ratio\n    sharpe_arr[ind] = (ret_arr[ind] - rf)/vol_arr[ind]\n\nAfter the monte carlo is done it’s time to inspect and locate the results and look at the weightings of the portfolios we need a portfolio that might be better than my initial equal weighted portfolio, rememebr returns alone are not the objective but also volatility, we want the highest return for the lowest volatility possible hence the highest sharpe ratio\nIn the next step i will be creating a data frame that will contain not just the weigthings but also the expected return, volatility and even the sharpe ratio of all the portfolios generated which will help me locate where the optimal or tangency portfolio, the portfolio with minimum volatility and the portfolio with maximum expected return are at in the weightings that were generated\n\n#Each row is a portfolio with the coresponding details \ndata = pd.DataFrame({'Return': ret_arr, 'Volatility': vol_arr, 'Sharpe Ratio': sharpe_arr})#(ret_arr - rf) /vol_arr})\nfor counter, symbol in enumerate(pf_data.columns.tolist()):\n    data[symbol + 'weight'] = [w[counter]for w in all_weights]\n\nportfolios = pd.DataFrame(data)\nportfolios\n\n\n\n\n\n\n\n\nReturn\nVolatility\nSharpe Ratio\nTXNweight\nCSCOweight\nINTCweight\nAAPLweight\nMSFTweight\nNVDAweight\nINFYweight\n...\nANSSweight\nCRMweight\nADBEweight\nFBweight\nAMDweight\nAMZNweight\nMAweight\nVMWweight\nGOOGweight\nSNPSweight\n\n\n\n\n0\n0.235060\n0.213741\n0.959388\n0.061536\n0.091934\n0.040965\n0.067888\n0.076020\n0.054105\n0.049063\n...\n0.003004\n0.092469\n0.040359\n0.081557\n0.032928\n0.003992\n0.066299\n0.014090\n0.053930\n0.019123\n\n\n1\n0.229757\n0.219309\n0.910850\n0.061311\n0.031462\n0.050643\n0.049502\n0.001270\n0.077914\n0.029312\n...\n0.033376\n0.040043\n0.078451\n0.053226\n0.073149\n0.071961\n0.062006\n0.077717\n0.030547\n0.007948\n\n\n2\n0.239854\n0.218571\n0.960119\n0.022931\n0.034186\n0.000097\n0.095811\n0.055255\n0.098486\n0.032107\n...\n0.039784\n0.038417\n0.094715\n0.038167\n0.052099\n0.029348\n0.002343\n0.069246\n0.053960\n0.017094\n\n\n3\n0.223821\n0.206283\n0.939588\n0.021978\n0.088269\n0.028764\n0.031951\n0.023287\n0.044314\n0.061802\n...\n0.084773\n0.025944\n0.023086\n0.070774\n0.019001\n0.004489\n0.067659\n0.011433\n0.067834\n0.080970\n\n\n4\n0.252654\n0.220076\n1.011718\n0.032829\n0.038141\n0.031621\n0.038156\n0.032453\n0.071135\n0.023667\n...\n0.049051\n0.001107\n0.104740\n0.069692\n0.071773\n0.051470\n0.017169\n0.034197\n0.086193\n0.106864\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19995\n0.236976\n0.217382\n0.952134\n0.065015\n0.062946\n0.031630\n0.024278\n0.054483\n0.060273\n0.038499\n...\n0.071690\n0.029728\n0.083362\n0.078441\n0.073044\n0.005775\n0.036472\n0.059114\n0.075507\n0.040374\n\n\n19996\n0.211920\n0.207550\n0.876511\n0.059174\n0.066972\n0.080858\n0.028952\n0.050625\n0.050136\n0.074528\n...\n0.066560\n0.097057\n0.004428\n0.037434\n0.029613\n0.009752\n0.050356\n0.032781\n0.063095\n0.061517\n\n\n19997\n0.218922\n0.215106\n0.878273\n0.020343\n0.000088\n0.041725\n0.001719\n0.004495\n0.014127\n0.094459\n...\n0.033807\n0.010043\n0.056615\n0.023852\n0.106904\n0.110114\n0.094245\n0.093785\n0.106817\n0.022204\n\n\n19998\n0.231003\n0.213849\n0.939929\n0.070349\n0.038615\n0.025927\n0.055969\n0.053822\n0.084375\n0.079402\n...\n0.067857\n0.047777\n0.030630\n0.029430\n0.042781\n0.011784\n0.023835\n0.074925\n0.056096\n0.032982\n\n\n19999\n0.214351\n0.207301\n0.889289\n0.038387\n0.083181\n0.062824\n0.052065\n0.058076\n0.032348\n0.044598\n...\n0.071942\n0.011572\n0.003619\n0.072106\n0.048186\n0.023791\n0.094538\n0.051598\n0.099134\n0.007197\n\n\n\n\n20000 rows × 23 columns\n\n\n\nThe tangency portfolio:\nThe tangency or maximum Sharpe ratio portfolio in the Markowitz procedure possesses the highest potential return-for-risk tradeoff.\n\n#optimal risky portfolio/ Tangency Portfolio / Highest sharpe\noptimal_risky_portfolio = portfolios.iloc[portfolios['Sharpe Ratio'].idxmax()]\noptimal_risky_portfolio\n\nReturn          0.263745\nVolatility      0.215477\nSharpe Ratio    1.084784\nTXNweight       0.041522\nCSCOweight      0.060248\nINTCweight      0.001923\nAAPLweight      0.089765\nMSFTweight      0.099979\nNVDAweight      0.083038\nINFYweight      0.066468\nINTUweight      0.103850\nSAPweight       0.026235\nADIweight       0.014330\nANSSweight      0.024099\nCRMweight       0.008061\nADBEweight      0.076826\nFBweight        0.000352\nAMDweight       0.056392\nAMZNweight      0.091473\nMAweight        0.010688\nVMWweight       0.009695\nGOOGweight      0.061586\nSNPSweight      0.073470\nName: 13834, dtype: float64\n\n\nMinim vol is also known as the minimum variance portfolio:\nThe minimum variance portfolio (mvp) is the portfolio that provides the lowest variance (standard deviation) among all possible portfolios of risky assets.\n\n#Portfolio with minimum volatility\nmin_vol_port = portfolios.iloc[portfolios['Volatility'].idxmin()]\nmin_vol_port\n\nReturn          0.202356\nVolatility      0.197514\nSharpe Ratio    0.872627\nTXNweight       0.042340\nCSCOweight      0.105379\nINTCweight      0.077549\nAAPLweight      0.026698\nMSFTweight      0.002516\nNVDAweight      0.014645\nINFYweight      0.102012\nINTUweight      0.034337\nSAPweight       0.058483\nADIweight       0.076789\nANSSweight      0.010081\nCRMweight       0.010085\nADBEweight      0.006261\nFBweight        0.039513\nAMDweight       0.017248\nAMZNweight      0.033863\nMAweight        0.028866\nVMWweight       0.053479\nGOOGweight      0.132173\nSNPSweight      0.127684\nName: 5947, dtype: float64\n\n\nMax return portfolio: The portfolio with the highest return regardless of risk\n\n#Portfolio with the maximum expected return\nmax_er_port = portfolios.iloc[portfolios['Return'].idxmax()]\nmax_er_port\n\nReturn          0.267209\nVolatility      0.228768\nSharpe Ratio    1.036900\nTXNweight       0.086701\nCSCOweight      0.024911\nINTCweight      0.005748\nAAPLweight      0.085139\nMSFTweight      0.021823\nNVDAweight      0.135536\nINFYweight      0.039539\nINTUweight      0.023250\nSAPweight       0.006508\nADIweight       0.000708\nANSSweight      0.064148\nCRMweight       0.091271\nADBEweight      0.014365\nFBweight        0.006770\nAMDweight       0.076369\nAMZNweight      0.109782\nMAweight        0.038671\nVMWweight       0.031825\nGOOGweight      0.061691\nSNPSweight      0.075245\nName: 6471, dtype: float64\n\n\nWith a scatter plot I’ll be able to visually see the portfolios and where they lay on the frontier, but remember the correlation of the stocks, there was virtually no negative correlation and modern portfolio theory is about diversifying with UNCORELATED assets, hence I do not expect the plot to form the usual bullet like shape, but i will still be able to see the optimal portfolios across the edges of the fronteir\n\n# creating a scatter plot and pinpointing the location of the above portfolios\nplt.figure(figsize=(20,10))\nplt.scatter(portfolios['Volatility'],portfolios['Return'],c=sharpe_arr,cmap='RdBu')#ret_arr,vol_arr\nplt.colorbar(label='Sharpe Ratio')\nplt.xlabel('Risk (Volatility)')\nplt.ylabel('Expected Returns')\n\nplt.scatter(optimal_risky_portfolio[1],optimal_risky_portfolio[0], c='green', s=80)\nplt.scatter(min_vol_port[1],min_vol_port[0], c='purple', s=80)#\nplt.scatter(max_er_port[1],max_er_port[0], c='yellow', s=80)\nplt.style.use('dark_background')\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\nC:\\Users\\one\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:4: MatplotlibDeprecationWarning:\n\nAuto-removal of grids by pcolor() and pcolormesh() is deprecated since 3.5 and will be removed two minor releases later; please call grid(False) first.\n\n\n\n\n\n\n\n\n\n\nNow that we know the weights of each portfolio, I’ll Assign the weights to the stocks and check the cumulative returns of each of the portfolios\nBut, NOTE: You’ve might have noticed from the observations produced by the simulation, the tangency portfolio has a lower sharp than my initial equal weight portfolio which now means based on ALL the observations I have, the EW portfolio is my tangency/Optimal portfolio and i will treat it as my optimal portfolio going forward\n\n#min volatility portfolio weights\nmin_vol_weights = all_weights[5947,:]\nmin_vol_weights\n\narray([0.04234009, 0.10537944, 0.07754858, 0.02669783, 0.00251591,\n       0.0146454 , 0.10201163, 0.03433662, 0.0584827 , 0.07678887,\n       0.0100807 , 0.0100849 , 0.00626119, 0.03951333, 0.01724793,\n       0.03386276, 0.02886554, 0.05347914, 0.13217338, 0.12768405])\n\n\n\nmin_vol_port_return = returns.dot(min_vol_weights)\ncum_minvol_returns =   (1 + min_vol_port_return).cumprod() - 1\ncum_minvol_returns_perc = pd.Series(100 * cum_minvol_returns)\n\n#Plot\nfig = go.Figure([go.Scatter(x=dates['Date'], y=cum_minvol_returns_perc)])\nfig.update_layout(template = \"plotly_dark\", title = 'Cummulative % Return of the minimum variance portfolio') \nfig.show()\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\n                                                \n\n\n\n#max volatility portfolio weights\nmax_er_weights = all_weights[6471,:]\nmax_er_weights\n\narray([0.08670101, 0.02491136, 0.00574761, 0.08513945, 0.02182338,\n       0.13553581, 0.03953875, 0.02325034, 0.00650826, 0.00070845,\n       0.06414761, 0.09127111, 0.01436485, 0.00676964, 0.07636874,\n       0.1097819 , 0.03867063, 0.03182504, 0.06169064, 0.07524542])\n\n\n\nmax_er_port_return = returns.dot(max_er_weights)\ncum_maxer_returns =   (1 + max_er_port_return).cumprod() - 1\ncum_maxer_returns_perc = pd.Series(100 * cum_maxer_returns)\n\n#Plot\nfig = go.Figure([go.Scatter(x=dates['Date'], y=cum_maxer_returns_perc)])\nfig.update_layout(template = \"plotly_dark\", title = 'Cummulative % Return of the maximum expected return portfolio') \nfig.show()\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\n                                                \n\n\n\n\nPortfolio vs benchmarks:\nIt’s time to compare the all the portfolios against certain benchmarks which are going to be the Invesco QQQ fund which is a technology ETF, I chose this particular tech ETF as a benchmark because it has the highest NAV of 135 billion as of April 2022\nI’ll also include the NASDAQ composite as a benchmark as it is widely followed and considered as a benchmark by investors, the Nasdaq composite is even more relevant here because more than 50% of the stocks in the index are technology companies\n\n#Importing the benchmarks data\nQQQ = pd.DataFrame(yf.download('QQQ', start=\"2012-05-20\", end=\"2021-12-31\", index_col = 'Date', parse_dates=True)['Adj Close']) \nNASDAQ = pd.DataFrame(yf.download('^IXIC', start=\"2012-05-20\", end=\"2021-12-31\", index_col = 'Date', parse_dates=True)['Adj Close']) \n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\n#calculate daily return, cumm return for each bechmark and place them in their data frames\nfor funds in (QQQ,NASDAQ):\n    funds['Daily Return'] = funds.pct_change(1).dropna()\n    funds['Cumulative Return'] = (1 + funds['Daily Return']).cumprod() - 1\n    funds['Cumulative % Return'] = funds['Cumulative Return'] * 100\n\n#creating a data frame that has all the portfolios and benchmarks cummulative % return\ndata = {'QQQ':QQQ['Cumulative % Return'],\n       'NASDAQ':NASDAQ['Cumulative % Return'],\n       'Optimal Port':cum_equal_returns_perc,\n       'Min Variance':cum_minvol_returns_perc,\n       'Max ER Port':cum_maxer_returns_perc}\nfunds_cumm = pd.DataFrame(data)\nfunds_cumm.reset_index(drop=True, inplace=True)\nfunds_cumm.insert(loc=0, column=\"Dates\", value=dates)\nfunds_cumm.tail()\n\n\n\n\n\n\n\n\nDates\nQQQ\nNASDAQ\nOptimal Port\nMin Variance\nMax ER Port\n\n\n\n\n2415\n2021-12-23\n598.161713\n449.779269\n1057.179336\n765.628694\n1652.657742\n\n\n2416\n2021-12-27\n609.700431\n457.432012\n1079.205886\n780.031449\n1690.668661\n\n\n2417\n2021-12-28\n606.411117\n454.287177\n1074.233314\n776.898982\n1679.724874\n\n\n2418\n2021-12-29\n606.305615\n453.742785\n1070.900768\n777.018843\n1670.698787\n\n\n2419\n2021-12-30\n604.194880\n452.876668\n1065.927132\n773.809387\n1659.601642\n\n\n\n\n\n\n\nBoth benchmarks performed poorly compared to the portfolios but this is largely due to concentration, as the portfolios have only 20 stocks while the benchmarks usually have hundreds of stocks in them, but the portfolio’s strongest point is also its weakest as a concentrated portfolio will probably have a much larger drawdown even during corrections let alone recessions\nThe max expected return portfolio outperformed all the other portfolios and the benchmarks as expected\nWhile my initial EW portfolio now turned into my tangency optimal portfolio faired well by beating both the benchmarks by a mile!\n\nfig = px.line(funds_cumm, x=\"Dates\", y=funds_cumm.columns,\n              hover_data={\"Dates\": \"|%B %d, %Y\"},\n              title='Commulative % Return')\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.update_layout(template = \"plotly_dark\", title = '10 years Cummulative % Return of all tech portfolios and benchmarks')\nfig.show()\ndisplay(HTML(fig.to_html(include_plotlyjs='cdn')));\n\n                                                \n\n\n\n\nConclusion:\nAssuming I started with a $5,000 in 2012 and invested in the ideal two best portfolios Max return portfolio and Optimal portfolio, by comparison, how much would my investment be by the end of 2021? (without rebalancing)\nThe max expected return portfolio seems to have a better outcome and seems very attractive especially considering the Sharpe ratio difference isn’t that big but this portfolio was picked because of its high expected return unfortunately, it’s not practical due to the presence of large estimation errors in those expected return estimate. As I have estimated them using historical data and have assumed past performance will be the same in the future which is unlikely as businesses change ESPECIALLY in the ever-changing technology industry\nHence the more reliable portfolio based on this research would either be the min vol or EW/Tangency portfolio\n\nInitial_Investment = 5000\n\n\n#Minimum Variance\nMin_ASRr = str(round(portfolios['Sharpe Ratio'][5947],2))\nMin_AERr = str(round(portfolios['Return'][5947]* 100,2)) + '%' \nMin_ASTDr = str(round(portfolios['Volatility'].min()*100,2)) + '%'\ncumm = str(round(cum_minvol_returns_perc[2418],2)) + '%'\nEW_Value = Initial_Investment * (cum_minvol_returns_perc[2418]/100)\nAbsolute_Value = '$' + str(round(EW_Value,2))\nprint('THE MINIMUM VARIANCE PORTFOLIO:')\nprint(f'The annual sharpe ratio of the minimum variance portfolio is {Min_ASRr}')\nprint(f'The annual Volatility of the minimum variance portfolio is {Min_ASTDr}')\nprint(f'The annual Expected Return of the minimum variance portfolio is {Min_AERr}')\nprint(f'The 10 yr cummulative return of the minimum variance portfolio is {cumm}')\nprint(f'A ${Initial_Investment} investment in minimum variance portfolio in 2012 would be worth {Absolute_Value} by the end of 2021')\n\n#Tangency/Optimal Portfolio\nASRr = str(round(SR,2))\nAERr = str(round(AER* 100,2)) + '%' \nASTDr = str(round(ASTD*100,2)) + '%'\ncumm3 = str(round(cum_equal_returns_perc[2418],2)) + '%'\nEW_Value3 = Initial_Investment * (cum_equal_returns_perc[2418]/100)\nAbsolute_Value3 = '$' + str(round(EW_Value3,2))\nprint('\\nTHE OPTIMAL PORTFOLIO:')\nprint(f'The annual sharpe ratio of the optimal portfolio is {ASRr}')\nprint(f'The annual Volatility of the optimal portfolio is {ASTDr}')\nprint(f'The annual Expected Return of the optimal portfolio is {AERr}')\nprint(f'The 10 yr cummulative return of the optimal portfolio is {cumm3}')\nprint(f'A ${Initial_Investment} investment in the optimal portfolio in 2012 would be worth {Absolute_Value3} by the end of 2021')\n\nTHE MINIMUM VARIANCE PORTFOLIO:\nThe annual sharpe ratio of the minimum variance portfolio is 0.87\nThe annual Volatility of the minimum variance portfolio is 19.75%\nThe annual Expected Return of the minimum variance portfolio is 20.24%\nThe 10 yr cummulative return of the minimum variance portfolio is 773.81%\nA $5000 investment in minimum variance portfolio in 2012 would be worth $38690.47 by the end of 2021\n\nTHE OPTIMAL PORTFOLIO:\nThe annual sharpe ratio of the optimal portfolio is 1.17\nThe annual Volatility of the optimal portfolio is 21.15%\nThe annual Expected Return of the optimal portfolio is 27.84%\nThe 10 yr cummulative return of the optimal portfolio is 1065.93%\nA $5000 investment in the optimal portfolio in 2012 would be worth $53296.36 by the end of 2021"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Monewmetrics",
    "section": "",
    "text": "Monewmetrics is centered on three core areas:\nQuantitative Research: I analyze financial data and apply advanced statistical models such as linear regression and autoregressive time series models, to generate predictive insights. Python is my primary tool, given its robust libraries for data analysis, machine learning, and statistical modeling.\nStatistics & Probability: I explore a wide range of statistical concepts, I cover foundational topics like data location and dispersion measures and will cover more advanced principles, such as regression assumptions and their applications.\nFinancial Analysis: Through my LinkedIn or SeekingAlpha,I post comprehensive financial analysis on companies, both quantitatively and qualitatively. This includes 3-statement financial modeling, scenario and sensitivity analysis, DCF valuation, and deeper insights into business models, competitive advantages, industry trends, market positioning and more.\nDISCLAIMER! Before proceeding, please make sure that you note the following important information:\nNOT FINANCIAL ADVICE! My content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog and wish to rely upon in making any investment decision or otherwise.\nI accept no liability whatsoever for any loss or gain you may incur."
  },
  {
    "objectID": "Finance/2024-10-12-HSY.html",
    "href": "Finance/2024-10-12-HSY.html",
    "title": "The Hershey Company Stock Analysis and Valuation",
    "section": "",
    "text": "Read the full analysis on my LinkedIn\n\n\n\nhsy-logo"
  },
  {
    "objectID": "Finance/2020-12-25-Microsoft-Corp.html",
    "href": "Finance/2020-12-25-Microsoft-Corp.html",
    "title": "Microsoft Corporation Stock Analysis and Valuation",
    "section": "",
    "text": "Summary\n\nAZURE growing faster than AWS and GCP.\nSaas is at the center of the digital economy.\nThe Stock Price could potential grow by more than 40%.\n\n\n\nRead the full analysis on seeking alpha\n\n\nDownload the full excel model\n\n\n\nmicrosoft logo"
  },
  {
    "objectID": "Finance/2020-12-21-Amdocs-LTD.html",
    "href": "Finance/2020-12-21-Amdocs-LTD.html",
    "title": "Amdocs Limited Stock Analysis and Valuation",
    "section": "",
    "text": "Summary\n\nAmdocs role as a vendor is critical for CSP’s to exploit new 5G and IoT opportunities.\nB2C and B2B markets are the key areas for 5G IoT connectivity growth.\nCOVID-19 makes IoT a necessity to the most impacted industries.\nAmdocs top line to reach $8 Billion by 2030 with rich margins to create a huge influx of cash flow.\n\n\n\nRead the full analysis on seeking alpha\n\n\nDownload the full excel model\n\n\n\namdocs-logo"
  },
  {
    "objectID": "Finance/2021-01-21-Footlocker-Inc.html",
    "href": "Finance/2021-01-21-Footlocker-Inc.html",
    "title": "Footlocker Inc Stock Analysis and Valuation",
    "section": "",
    "text": "Summary\n\nThe Stock Is Already Up 6% This Year.\nThe Sneaker Culture Is Not An Ephemeral Trend.\nA Low P/E and High Shareholder Yield Makes It An Attractive Investment.\n\n\n\nRead the full analysis on seeking alpha\n\n\nDownload the full excel model\n\n\n\nfootlocker"
  },
  {
    "objectID": "Finance/index.html",
    "href": "Finance/index.html",
    "title": "Financial Analysis",
    "section": "",
    "text": "Amdocs Limited Stock Analysis and Valuation\n\n\nAmdocs LTD To Unlock Value From 5G IoT Connectivity Market.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft Corporation Stock Analysis and Valuation\n\n\nMicrosoft’s Growth Potential In The Cloud Industry.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFootlocker Inc Stock Analysis and Valuation\n\n\nFoot Locker, A good business at a cheap price?.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Hershey Company Stock Analysis and Valuation\n\n\nThe Hershey Company Potential For Global Dominance.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "my-site.html",
    "href": "my-site.html",
    "title": "my-site",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "my-site.html#quarto",
    "href": "my-site.html#quarto",
    "title": "my-site",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org."
  },
  {
    "objectID": "Quant/2022-06-01-btc-usd-price-prediction.html",
    "href": "Quant/2022-06-01-btc-usd-price-prediction.html",
    "title": "BTC/USD Price Prediction Using Linear Regression",
    "section": "",
    "text": "In this notebook I’m going to try and predict the BTC/USD close price(target/dependant variable) for the month of June 2022 using the linear regression as the chosen model, I will also use a set of relevant independent variables like moving averages, volume, etc. I’ll follow a basic data science workflow from importing data to model development and evaluation. Lastly, assuming the model passes the validation stage, I will bactest and measure its performance as a trading strategy to further gain insights on how reliable it can be before placing any trade based on the prediction made\n\n\ntoc: true\nbadges: true\ncategories:[Quantitative Research]\nimage:images/lr.png\nuse_plotly: true\n\n\nDISCLAIMER!\nBefore proceeding, please make sure that you note the following important information:\nNOT FINANCIAL ADVICE!\nMy content is intended to be used and must be used for informational and educational purposes only. I am not an attorney, CPA, or financial advisor, nor am I holding myself out to be, and the information contained on this blog/notebook is not a substitute for financial advice, None of the information contained here constitutes an offer (or solicitation of an offer) to buy or sell any security or financial instrument to make any investment or to participate in any particular trading strategy. Always seek advice from a professional who is aware of the facts and circumstances of your individual situation. Or, Independently research and verify any information that you find on my blog/notebook and wish to rely upon in making any investment decision or otherwise. I accept no liability whatsoever for any loss or damage you may incur\n\nimport pandas as pd\nimport numpy as np\nimport yfinance as yf\nfrom scipy import stats\nfrom sklearn import linear_model\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom datetime import datetime\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nfrom IPython.display import HTML\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n\nImport Data\n\n#Downloading all relevant data\n\n#BTC price and volume\n\n#you will import Gold price when you start creating the indipendant variables\n\n# Read data\nDf = yf.download('BTC-USD', '2012-01-01', '2022-05-31', interval= '1mo', auto_adjust=True)\n\n# Only keep close columns\nDf = Df[['Close','Volume']]\n\n# Drop rows with missing values\nDf = Df.dropna()\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\nData Cleaning and Wrangling\nBefore any modeling can be done, there are a few steps needed to prepare the data before feeding it to the model, at least by arranging the data set in a way it makes sense\n\n#Reset date index and arrange rows in same format\ndf = Df.reset_index()\nfor i in ['Close', 'Volume']: \n      df[i]  =  df[i].astype('float64')\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\n\n\n\n\n0\n2014-10-01\n338.321014\n9.029944e+08\n\n\n1\n2014-11-01\n378.046997\n6.597334e+08\n\n\n2\n2014-12-01\n320.192993\n5.531023e+08\n\n\n3\n2015-01-01\n217.464005\n1.098812e+09\n\n\n4\n2015-02-01\n254.263000\n7.115187e+08\n\n\n...\n...\n...\n...\n\n\n87\n2022-01-01\n38483.125000\n9.239790e+11\n\n\n88\n2022-02-01\n43193.234375\n6.713360e+11\n\n\n89\n2022-03-01\n45538.675781\n8.309438e+11\n\n\n90\n2022-04-01\n37714.875000\n8.301159e+11\n\n\n91\n2022-05-01\n31792.310547\n1.105689e+12\n\n\n\n\n92 rows × 3 columns\n\n\n\nBecause I’m working with monthly data, I’ll drop the days in the date to avoid confusion\n\ndate_format = \"%Y/%m\"\ndf['Date'] = df['Date'].dt.strftime(date_format)\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\n\n\n\n\n0\n2014/10\n338.321014\n9.029944e+08\n\n\n1\n2014/11\n378.046997\n6.597334e+08\n\n\n2\n2014/12\n320.192993\n5.531023e+08\n\n\n3\n2015/01\n217.464005\n1.098812e+09\n\n\n4\n2015/02\n254.263000\n7.115187e+08\n\n\n...\n...\n...\n...\n\n\n87\n2022/01\n38483.125000\n9.239790e+11\n\n\n88\n2022/02\n43193.234375\n6.713360e+11\n\n\n89\n2022/03\n45538.675781\n8.309438e+11\n\n\n90\n2022/04\n37714.875000\n8.301159e+11\n\n\n91\n2022/05\n31792.310547\n1.105689e+12\n\n\n\n\n92 rows × 3 columns\n\n\n\nBitcoin is often referred to as “digital gold” by its backers hence I’ll add the gold price and volume data as potential independent variables and I’ll explore further to see its relationship and whether or not it will be a good predictor\n\n# Import gold\ngold = yf.download('GLD', '2014-10-01', '2022-05-31', interval= '1mo', auto_adjust=True)\ngold = gold[['Close','Volume']]\ngld = gold.reset_index()\nfor i in ['Close', 'Volume']: \n      gld[i]  =  gld[i].astype('float64')\ngld\n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\n\n\n\n\n0\n2014-10-01\n112.660004\n155183900.0\n\n\n1\n2014-11-01\n112.110001\n147594200.0\n\n\n2\n2014-12-01\n113.580002\n153722200.0\n\n\n3\n2015-01-01\n123.449997\n198034100.0\n\n\n4\n2015-02-01\n116.160004\n125686200.0\n\n\n...\n...\n...\n...\n\n\n87\n2022-01-01\n168.089996\n211125100.0\n\n\n88\n2022-02-01\n178.380005\n254601300.0\n\n\n89\n2022-03-01\n180.649994\n377087100.0\n\n\n90\n2022-04-01\n176.910004\n195346400.0\n\n\n91\n2022-05-01\n171.139999\n179902200.0\n\n\n\n\n92 rows × 3 columns\n\n\n\nThe other two independent variables will be the moving averages and volume\nmoving averages are often used by technical analysts to keep track of price trends for specific securities. I’ll use the 3 and 6 month exponential moving averages but whether it’s simple, weighted, or exponential in general it doesn’t really make much of a difference (but this could be a good hypothesis to test)\nVolume is also a well-known indicator of price movement, Trading volume is the total number of shares/units of a security traded during a given period of time.\n\n#add the first two indipendant variables: 3ema, 6ema\ndf['ema3'] = df['Close'].ewm(span=3, adjust=False).mean()\ndf['ema6'] = df['Close'].ewm(span=6, adjust=False).mean()\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\nema3\nema6\n\n\n\n\n0\n2014/10\n338.321014\n9.029944e+08\n338.321014\n338.321014\n\n\n1\n2014/11\n378.046997\n6.597334e+08\n358.184006\n349.671295\n\n\n2\n2014/12\n320.192993\n5.531023e+08\n339.188499\n341.248923\n\n\n3\n2015/01\n217.464005\n1.098812e+09\n278.326252\n305.881804\n\n\n4\n2015/02\n254.263000\n7.115187e+08\n266.294626\n291.133574\n\n\n...\n...\n...\n...\n...\n...\n\n\n87\n2022/01\n38483.125000\n9.239790e+11\n44519.703176\n46247.967191\n\n\n88\n2022/02\n43193.234375\n6.713360e+11\n43856.468776\n45375.186387\n\n\n89\n2022/03\n45538.675781\n8.309438e+11\n44697.572278\n45421.897642\n\n\n90\n2022/04\n37714.875000\n8.301159e+11\n41206.223639\n43219.891173\n\n\n91\n2022/05\n31792.310547\n1.105689e+12\n36499.267093\n39954.868137\n\n\n\n\n92 rows × 5 columns\n\n\n\n\n# adding the other two indipendant variables download gold price and godl volume (btc volume already there)\ndf['Gold Close'] = gld['Close']\ndf['Gold Volume'] = gld['Volume']\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\nema3\nema6\nGold Close\nGold Volume\n\n\n\n\n0\n2014/10\n338.321014\n9.029944e+08\n338.321014\n338.321014\n112.660004\n155183900.0\n\n\n1\n2014/11\n378.046997\n6.597334e+08\n358.184006\n349.671295\n112.110001\n147594200.0\n\n\n2\n2014/12\n320.192993\n5.531023e+08\n339.188499\n341.248923\n113.580002\n153722200.0\n\n\n3\n2015/01\n217.464005\n1.098812e+09\n278.326252\n305.881804\n123.449997\n198034100.0\n\n\n4\n2015/02\n254.263000\n7.115187e+08\n266.294626\n291.133574\n116.160004\n125686200.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n87\n2022/01\n38483.125000\n9.239790e+11\n44519.703176\n46247.967191\n168.089996\n211125100.0\n\n\n88\n2022/02\n43193.234375\n6.713360e+11\n43856.468776\n45375.186387\n178.380005\n254601300.0\n\n\n89\n2022/03\n45538.675781\n8.309438e+11\n44697.572278\n45421.897642\n180.649994\n377087100.0\n\n\n90\n2022/04\n37714.875000\n8.301159e+11\n41206.223639\n43219.891173\n176.910004\n195346400.0\n\n\n91\n2022/05\n31792.310547\n1.105689e+12\n36499.267093\n39954.868137\n171.139999\n179902200.0\n\n\n\n\n92 rows × 7 columns\n\n\n\nNow i’m going to generate the dependant/target variable that i’m going to try and predict\n\n#Creating the dependant variable which is next month close price and adding it to the dataframe \ndf['Next Month Close'] = df['Close'].shift(-1)\ndf\n\n\n\n\n\n\n\n\nDate\nClose\nVolume\nema3\nema6\nGold Close\nGold Volume\nNext Month Close\n\n\n\n\n0\n2014/10\n338.321014\n9.029944e+08\n338.321014\n338.321014\n112.660004\n155183900.0\n378.046997\n\n\n1\n2014/11\n378.046997\n6.597334e+08\n358.184006\n349.671295\n112.110001\n147594200.0\n320.192993\n\n\n2\n2014/12\n320.192993\n5.531023e+08\n339.188499\n341.248923\n113.580002\n153722200.0\n217.464005\n\n\n3\n2015/01\n217.464005\n1.098812e+09\n278.326252\n305.881804\n123.449997\n198034100.0\n254.263000\n\n\n4\n2015/02\n254.263000\n7.115187e+08\n266.294626\n291.133574\n116.160004\n125686200.0\n244.223999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n87\n2022/01\n38483.125000\n9.239790e+11\n44519.703176\n46247.967191\n168.089996\n211125100.0\n43193.234375\n\n\n88\n2022/02\n43193.234375\n6.713360e+11\n43856.468776\n45375.186387\n178.380005\n254601300.0\n45538.675781\n\n\n89\n2022/03\n45538.675781\n8.309438e+11\n44697.572278\n45421.897642\n180.649994\n377087100.0\n37714.875000\n\n\n90\n2022/04\n37714.875000\n8.301159e+11\n41206.223639\n43219.891173\n176.910004\n195346400.0\n31792.310547\n\n\n91\n2022/05\n31792.310547\n1.105689e+12\n36499.267093\n39954.868137\n171.139999\n179902200.0\nNaN\n\n\n\n\n92 rows × 8 columns\n\n\n\n\n#We don't need the current close price hence i'll remove it from the dataframe\nbtc_close = df['Close'] #But i'll save the close price just incase i need it \n\n#Then i'll remove the previous month btc close price so that i'm left with only the relevant data that i need\ndf.drop(columns='Close', inplace=True)\n\n\n#i'll drop all NA values in the data frame\nDf = df.dropna()\n\n#Now i should have a good clean dataframe ready for some EDA\nDf\n\n\n\n\n\n\n\n\nDate\nVolume\nema3\nema6\nGold Close\nGold Volume\nNext Month Close\n\n\n\n\n0\n2014/10\n9.029944e+08\n338.321014\n338.321014\n112.660004\n155183900.0\n378.046997\n\n\n1\n2014/11\n6.597334e+08\n358.184006\n349.671295\n112.110001\n147594200.0\n320.192993\n\n\n2\n2014/12\n5.531023e+08\n339.188499\n341.248923\n113.580002\n153722200.0\n217.464005\n\n\n3\n2015/01\n1.098812e+09\n278.326252\n305.881804\n123.449997\n198034100.0\n254.263000\n\n\n4\n2015/02\n7.115187e+08\n266.294626\n291.133574\n116.160004\n125686200.0\n244.223999\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n86\n2021/12\n9.570472e+11\n50556.281353\n49353.904068\n170.960007\n151214100.0\n38483.125000\n\n\n87\n2022/01\n9.239790e+11\n44519.703176\n46247.967191\n168.089996\n211125100.0\n43193.234375\n\n\n88\n2022/02\n6.713360e+11\n43856.468776\n45375.186387\n178.380005\n254601300.0\n45538.675781\n\n\n89\n2022/03\n8.309438e+11\n44697.572278\n45421.897642\n180.649994\n377087100.0\n37714.875000\n\n\n90\n2022/04\n8.301159e+11\n41206.223639\n43219.891173\n176.910004\n195346400.0\n31792.310547\n\n\n\n\n91 rows × 7 columns\n\n\n\n\n\nExplanatory Data Analysis\nIntuitley I know that traders like to use the ema lines and volume to predict BTC price. But as I mentioned before here I get the chance to explore whether gold price and its volume can help predict BTC price.\n\n\nThe Pearson correlation coefficient and p value\n\nPearson Correlation:\nCorrelation between sets of data is a measure of how well they are related. The most common measure of correlation in stats is the Pearson Correlation.The full name is the Pearson Product Moment Correlation (PPMC). It shows the linear relationship between two sets of data. In simple terms, it answers the question, Can I draw a line graph to represent the data?\nIt is a number between –1 and 1 that measures the strength and direction of the relationship between two variables, where:\n\n1: Perfect positive linear correlation.\n\n&lt;li&gt;&lt;b&gt;0&lt;/b&gt;: No linear correlation, the two variables most likely do not affect each other.&lt;/li&gt;\n&lt;li&gt;&lt;b&gt;-1&lt;/b&gt;: Perfect negative linear correlation.&lt;/li&gt;\n\n\nP-Value:\nA p-value measures the probability of obtaining the observed results, assuming that the null hypothesis is true. The lower the p-value, the greater the statistical significance of the observed difference. A p-value of 0.05 or lower is generally considered statistically significant which means that we are 95% confident that the correlation between the variables is significant.\nBy convention, when the\n\n\np-value is &lt; 0.001: we say there is strong evidence that the correlation is significant.\n\n\nthe p-value is &lt; 0.05: there is moderate evidence that the correlation is significant.\n\n\nthe p-value is &lt; 0.1: there is weak evidence that the correlation is significant.\n\n\nthe p-value is &gt; 0.1: there is no evidence that the correlation is significant.\n\n\nTwo things keeps to keep in mind when interprating the results:\n\n\nThe null hypothesis is that the two variables are uncorrelated .\n\n\nThe p value is in scientific notation, it’s decimal form is e.g 4.2e-7 = 0.00000042.\n\n\n\n#checking pearson correlation for gold price and volume\n#I will start form the second last row to avoid errors bcz of nan value \npearson_coef, p_value = stats.pearsonr(df['Gold Close'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nThe Pearson Correlation Coefficient is 0.7706711768570538  with a P-value of P = 6.555477090301474e-19\n\n\nIn this case,\nThe p-value is &lt; 0.001 hence, there is strong evidence that the correlation between gold price and BTC price is statistically significant, and the linear relationship is quite strong (0.77, close to 1)\n\npearson_coef, p_value = stats.pearsonr(df['Gold Volume'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nThe Pearson Correlation Coefficient is 0.08476385450593574  with a P-value of P = 0.42700282567060693\n\n\nThe p-value is &lt; 0.001 hence, there is moderate evidence that the correlation between gold volume and BTC price is statistically significant, and there is no linear relationship (0.08, almost 0)\nVisually we can see that there is almost no linear relationship between gold volume and btc price\n\n# make linear regression chart of btc next month price and gold price and do for volume\nfig = px.scatter(\n    df, x='Gold Volume', y='Next Month Close', opacity=0.65,\n    trendline='ols', trendline_color_override='firebrick'\n)\nfig.update_layout(template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nSo we now know that we can use gold price but not its volume, it would have destroyed value and it wouldn’t have added anything to the model if anything it would have probably ruined our prediction\n\n#Dropping gold volume\ndf.drop(columns='Gold Volume', inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nDate\nVolume\nema3\nema6\nGold Close\nNext Month Close\n\n\n\n\n0\n2014/10\n9.029944e+08\n338.321014\n338.321014\n112.660004\n378.046997\n\n\n1\n2014/11\n6.597334e+08\n358.184006\n349.671295\n112.110001\n320.192993\n\n\n2\n2014/12\n5.531023e+08\n339.188499\n341.248923\n113.580002\n217.464005\n\n\n3\n2015/01\n1.098812e+09\n278.326252\n305.881804\n123.449997\n254.263000\n\n\n4\n2015/02\n7.115187e+08\n266.294626\n291.133574\n116.160004\n244.223999\n\n\n\n\n\n\n\nWhat about the the other indipendant variables?\n\nprint('EMA 3')\npearson_coef, p_value = stats.pearsonr(df['ema3'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nprint('\\nEMA 6')\npearson_coef, p_value = stats.pearsonr(df['ema6'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nprint('\\nVolume')\npearson_coef, p_value = stats.pearsonr(df['Volume'][:90], df['Next Month Close'][:90])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n\nEMA 3\nThe Pearson Correlation Coefficient is 0.9518278081533298  with a P-value of P = 5.913915911148407e-47\n\nEMA 6\nThe Pearson Correlation Coefficient is 0.9357719658423489  with a P-value of P = 1.3124438502636034e-41\n\nVolume\nThe Pearson Correlation Coefficient is 0.7995337431531997  with a P-value of P = 3.465784507142274e-21\n\n\nThe other indipendant variables are all statistically significant, and their linear relationship are very strong with p-values of &lt; 0.001\n\n\nNOTE:\nCausation is the relationship between cause and effect between two variables.\nIt is important to know the difference between correlation and causation. Correlation does not imply causation. Determining correlation is much simpler than determining causation as causation may require independent experimentation.\n\n\n\nModel Development\nBefore we continue let’s clarify the objective again: I’m using the 3 and 6 month ema, BTC previous month volume, and Gold close price of the preceding month to predict what BTC close price of the impending month\n\n#Define the indipendant variables\nx = Df[['ema3','ema6','Volume','Gold Close']]\n\n# Define the dependent variable\ny = Df['Next Month Close']\n\nI am going to spilt the data, 80% of the data will be used to train the model and 20% will be used to test the prediction made from that 80%\n\n# Split the data into train and test dataset\nt = .8\nt = int(t*len(Df))\n\n# Train dataset\nx_train = x[:t]\ny_train = y[:t]\n\n# Test dataset\nx_test = x[t:]\ny_test = y[t:]\n\n\n#print the lengths\nprint(\"number of test samples :\", y_test.shape[0])\nprint(\"number of training samples:\",y_train.shape[0])\n\nnumber of test samples : 19\nnumber of training samples: 72\n\n\n\n#fit the regression\nreg = linear_model.LinearRegression()\nreg.fit(x_train,y_train)\n\nLinearRegression()\n\n\nThe constant came back negative which is confusing but i’ll get back to this later\n\n#Intercept value\nreg.intercept_ \n\n-1262.8899087871669\n\n\n\n#make coefficient table\ncoeff_df = pd.DataFrame(reg.coef_.T, x.columns, columns=['Coefficient']) \ncoeff_df \n\n\n\n\n\n\n\n\nCoefficient\n\n\n\n\nema3\n1.777304e+00\n\n\nema6\n-9.488261e-01\n\n\nVolume\n9.555041e-10\n\n\nGold Close\n1.521999e+01\n\n\n\n\n\n\n\nThe Multiple linear regression formula:\n\n\\[ y = β_{0} \\;+ \\;β_{1} * \\;X_{1} + \\;β_{2} * \\;X_{2} + \\;β_{3} * \\;X_{3} + \\;β_{4} * \\;X_{4}\\]\n\nprint(\"Linear Regression model\")\nprint(\"BTC Price (y) = %.2f (constant) + %.2f * EM3 (x1) + %.2f * EMA6 (x2) + %.4f * Volume (x3) + %.2f * Gold Close (x4)\" % (reg.intercept_,reg.coef_[0], reg.coef_[1],reg.coef_[2],reg.coef_[3]))\n\nLinear Regression model\nBTC Price (y) = -1262.89 (constant) + 1.78 * EM3 (x1) + -0.95 * EMA6 (x2) + 0.0000 * Volume (x3) + 15.22 * Gold Close (x4)\n\n\n\n\n\nModel Evaluation\nIn this step, I will evaluate the model’s accuracy but before that happens I’m going to make the predictions first\n\n#make prediction of test data\npredicted_price = reg.predict(x_test)\n\nThe R square is 0.05 which means the model’s predicitive power is poor in fact it is worse than what I expected it predicts little to nothing of the target variable\n\n#check r2 and MSE's of testing data\ntest_r2_score = r2_score(y_test,predicted_price)\nprint('The test R-square is: ', test_r2_score)\n\nThe test R-square is:  0.053315838215583056\n\n\nI will also look at other evaluation methods\n\n#Test r2\ntest_r2_score = r2_score(y_test,predicted_price)\nprint('The test R-square is: ', test_r2_score)\n\ntest_mse = mean_squared_error(y_test, predicted_price)\nprint('The test mean square error of target variable and predicted value is: ', test_mse)\n\ntest_mae = mean_absolute_error(y_test, predicted_price)\nprint('The test mean absolute error of target variable and predicted value is: ', test_mae)\n\ntest_rmse=np.sqrt(test_mse)\nprint('The test root mean square error of target variable and predicted value is: ', test_rmse)\n\nThe test R-square is:  0.053315838215583056\nThe test mean square error of target variable and predicted value is:  107191024.05803142\nThe test mean absolute error of target variable and predicted value is:  8821.373009960726\nThe test root mean square error of target variable and predicted value is:  10353.309811747711\n\n\nMean Square Error (MSE) is the average difference of actual values and predicted values There is no correct value for MSE. Simply put, the lower the value the better, and 0 means the model is perfect.\nMean Absolute Error (MAE) is the sum of the absolute difference between actual and predicted values in this case the average difference is $8821\nI’m going to evaluate further and try to see what other insights I can gather from the predicted price I’ll start by creating a data frame and add the predicted price and actual price so that I can plot the prices side by side\n\nbtc = pd.DataFrame()\n\n#btc['Close Previous Month'] = btc_close[t:]\nbtc['Date'] = Df['Date'][t:]\nbtc['Predicted Close'] = predicted_price\nbtc['Actual Close'] = btc_close[t:].shift(-1)#btc['Close Previous Month'].shift(-1)\nbtc\n\n\n\n\n\n\n\n\nDate\nPredicted Close\nActual Close\n\n\n\n\n72\n2020/10\n13607.631908\n19625.835938\n\n\n73\n2020/11\n17737.624104\n29001.720703\n\n\n74\n2020/12\n25451.933856\n33114.359375\n\n\n75\n2021/01\n31623.812022\n45137.769531\n\n\n76\n2021/02\n40797.895059\n58918.832031\n\n\n77\n2021/03\n52011.667637\n57750.175781\n\n\n78\n2021/04\n55687.583652\n37332.855469\n\n\n79\n2021/05\n43935.783612\n35040.835938\n\n\n80\n2021/06\n35882.237824\n41626.195312\n\n\n81\n2021/07\n36483.673993\n47166.687500\n\n\n82\n2021/08\n40423.918840\n43790.894531\n\n\n83\n2021/09\n39815.250546\n61318.957031\n\n\n84\n2021/10\n50330.042450\n57005.425781\n\n\n85\n2021/11\n51684.668062\n46306.445312\n\n\n86\n2021/12\n45279.181353\n38483.125000\n\n\n87\n2022/01\n37422.063673\n43193.234375\n\n\n88\n2022/02\n36986.624235\n45538.675781\n\n\n89\n2022/03\n38624.254997\n37714.875000\n\n\n90\n2022/04\n34450.675253\n31792.310547\n\n\n\n\n\n\n\n\n# make a chart of predicted next month close VS actual next month close\nfig = px.line(btc, x=\"Date\", y=btc.columns,              \n              title='Predicted Close Vs Actual Close')\nfig.update_xaxes(\n    rangeslider_visible=True,\n    rangeselector=dict(\n        buttons=list([\n            dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),\n            dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),\n            dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),\n            dict(step=\"all\")\n        ])\n    )\n)\nfig.update_layout(template = \"plotly_dark\")\n\n                                                \n\n\nWe have already seen the mean of these residuals (mean squared error), now I’ll look at the residuals of each month in absolute and in % to see how far off are the predictions for each month (Remember, the residual is the difference between the observed value and the estimated value)\n\nbtc['Residual'] = btc_close[t:] - btc['Actual Close'] #The difference in absolute $ terms\nbtc['Residual in %'] = np.absolute(btc['Residual']/btc['Actual Close']*100) #The difference in % \nbtc\n\n\n\n\n\n\n\n\nDate\nPredicted Close\nActual Close\nResidual\nResidual in %\n\n\n\n\n72\n2020/10\n13607.631908\n19625.835938\n-5844.840820\n29.781360\n\n\n73\n2020/11\n17737.624104\n29001.720703\n-9375.884766\n32.328719\n\n\n74\n2020/12\n25451.933856\n33114.359375\n-4112.638672\n12.419502\n\n\n75\n2021/01\n31623.812022\n45137.769531\n-12023.410156\n26.637138\n\n\n76\n2021/02\n40797.895059\n58918.832031\n-13781.062500\n23.389911\n\n\n77\n2021/03\n52011.667637\n57750.175781\n1168.656250\n2.023641\n\n\n78\n2021/04\n55687.583652\n37332.855469\n20417.320312\n54.689951\n\n\n79\n2021/05\n43935.783612\n35040.835938\n2292.019531\n6.540996\n\n\n80\n2021/06\n35882.237824\n41626.195312\n-6585.359375\n15.820229\n\n\n81\n2021/07\n36483.673993\n47166.687500\n-5540.492188\n11.746621\n\n\n82\n2021/08\n40423.918840\n43790.894531\n3375.792969\n7.708892\n\n\n83\n2021/09\n39815.250546\n61318.957031\n-17528.062500\n28.585063\n\n\n84\n2021/10\n50330.042450\n57005.425781\n4313.531250\n7.566878\n\n\n85\n2021/11\n51684.668062\n46306.445312\n10698.980469\n23.104733\n\n\n86\n2021/12\n45279.181353\n38483.125000\n7823.320312\n20.329223\n\n\n87\n2022/01\n37422.063673\n43193.234375\n-4710.109375\n10.904739\n\n\n88\n2022/02\n36986.624235\n45538.675781\n-2345.441406\n5.150438\n\n\n89\n2022/03\n38624.254997\n37714.875000\n7823.800781\n20.744602\n\n\n90\n2022/04\n34450.675253\n31792.310547\n5922.564453\n18.628921\n\n\n\n\n\n\n\nAs you can see from the residual, the difference is pretty large but this is due to BTC being very volatile hence anything between 7-10% difference (this is subjective based on my observations from BTC trading) could be good but to expect a residual of less than 5% consistently would be very unlikely from an asset class this volatile\n\n\nConclusion\nIf the model had been at least 50-60% accurate (have an R square of 0.50-0.60), I would have proceeded with backtesting and then take the model live by predicting the close price of this month (June 2022)\nThe linear regression is not a good model to use when predicting BTC/USD prices, maybe it would’ve been more efficient in predicting the returns instead. There were many red flags and based on the R square alone I would never take this model live and risk real money on it\nThe MSE was way too high and very far from 0 since MSE is a measure of how close a fitted line is to data points\nAnother red flag was the constant being negative, This means when the independent variables are 0 the mean price of BTC will be -1262. A negative constant doesn’t mean the regression made a mistake but rather it’s the data being modeled, realistically any security price can go to 0 but no security price can fall below 0 and turn negative, The your position value of that asset can turn negative but not the actual asset price. which is also why I think predicting returns instead of price would have been more accurate and much more realistic\nThis is also a good example to showcase how a machine learning model is only as useful as the features selected and in-order to select the right features depends on the knowledge one has of that data set!"
  },
  {
    "objectID": "Quant/index.html",
    "href": "Quant/index.html",
    "title": "Quantitative Research",
    "section": "",
    "text": "Portfolio Analysis, Efficient Frontier & Monte Carlo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBTC/USD Price Prediction Using Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStock Price Forecast Using ARIMA Model\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Stats/2022-10-07-Stats2.html",
    "href": "Stats/2022-10-07-Stats2.html",
    "title": "EDA PART 2, Graphical Analysis",
    "section": "",
    "text": "In this we’ll go through some visualization techniques performed during explanatory data analysis\n\n\ntoc: true\nbadges: true\ncategories:[Statistics & Probability]\nimage:images/plots.jpg\nuse_plotly: true\n\nWhile non-graphical analysis provides us with valuable insights, graphical techniques provide a visual representation that enhances our understanding of the data. graphical analysis serves as a vital tool for exploring and understanding data.\nGraphical analysis allows us to examine the distribution of data, providing insights into the patterns, shape, and spread of values within a dataset. Histograms, for example, display the frequency or proportion of data points within specific intervals, enabling us to identify peaks, gaps, or skewed distributions. Box plots provide a visual representation of the minimum, maximum, median, and quartiles, helping us understand the central tendencies and variability in the data. By exploring the distribution of data graphically, we can gain a deeper understanding of its characteristics and identify any outliers or anomalies.\nFurthermore, Graphical analysis enables us to explore the relationships between different variables in a dataset. Scatter plots, for instance, plot data points as dots on a graph, allowing us to observe the correlation or association between two variables. This helps us identify trends, patterns, or potential dependencies. Additionally, line plots or time series plots provide a visual representation of how variables change over time, highlighting any trends or seasonal patterns. By examining these graphical representations, we can uncover valuable insights into the relationships and dependencies between variables, enabling us to make informed decisions or predictions.\nLastly, Graphical analysis facilitates the comparison of different datasets or categories, allowing us to identify similarities, differences, or trends. Bar charts or column charts, for example, provide a visual representation of categorical data, making it easy to compare the frequency or proportions of different categories. Grouped bar charts or stacked bar charts can be used to compare multiple categories simultaneously. By visually comparing data, we can identify variations, spot outliers, or detect patterns across different groups or time periods. This helps us make data-driven decisions and identify areas of improvement.\nBy leveraging visual representations, we gain valuable insights into the underlying patterns, trends, and characteristics of the data, empowering us to make informed decisions and draw meaningful conclusions.\nWe will explore some of the most commonly used visualization tools that I frequently use when performing graphical analysis\n\n#importing relevant libraries\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom yahoofinancials import YahooFinancials as yfin\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom IPython.display import HTML\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)  \nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\nHEATMAP\nThe correlation heatmap allows for a quick visual assessment of the relationships between variables in a dataset. It helps identify strong positive or negative correlations, patterns, and clusters among variables. This graphical representation provides a more intuitive and comprehensive understanding of the interdependencies among multiple variables compared to a tabular format.\nUsing the same portfolio of stocks from the previous part, when we examine the return correlation between the stocks, we can quickly observe which stocks move together. As suspected, there is no pair with a negative correlation, considering that all the stocks share common traits. They are all part of the S&P 500, and they all belong to the technology sector, except for Tesla. This explains why Tesla is the only stock that exhibits a less positive correlation with every other stock. However, since Tesla utilizes technology heavily in its electric vehicles, it still considered a tech company by some despite its primary focus on automotive production. Therefore, it exhibits similar characteristics to other tech stocks, which may explain why the correlation is not negative but still not as strong as the correlations among the purely technology-focused stocks.\n\nassets = ['META','AMZN','NFLX','GOOG','MSFT','NVDA','TSLA']\npf_data = pd.DataFrame()\nfor a in assets:\n    pf_data[a] = yf.download(a, start=\"2021-10-01\", end=\"2021-12-31\")['Adj Close']\n\nreturns = pf_data.pct_change(1).dropna()\ncov = returns.cov()\ncorr = returns.corr()\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n(NOTE: REFRESH PAGE IF GRAPHS ARE NOT LOADING)\n\nfig = px.imshow(corr)\nfig.update_layout(width=1000, height=800)\nfig.update_layout(template = \"plotly_dark\", title = 'A heat Map Of Stock Returns In a Portfolio') \nfig.show()\n\n                                                \n\n\n\n\nSCATTER MATRIX & SCATTER PLOT\nAside from a heatmap we can also use a scatter matrix to check the relationships between the stocks.\nA scatter matrix, also known as a scatterplot matrix or pair plot, is a graphical tool used to explore the relationships between multiple variables in a dataset. It consists of a grid of scatterplots, where each scatterplot represents the relationship between two variables.\nA scatter plot is a type of graph that displays the relationship between two variables. It consists of a horizontal x-axis and a vertical y-axis, where each data point is represented by a dot or marker on the plot. The position of each dot on the scatter plot corresponds to the values of the two variables being analyzed.\nScatter plots are particularly useful for visualizing the correlation or relationship between two continuous variables. By plotting the data points on the scatter plot, it becomes easier to observe any patterns, trends, or associations between the variables. The general shape or direction of the points on the scatter plot can provide insights into the strength and direction of the relationship.\n\nfig = px.scatter_matrix(returns, title='A Scatter Matrix Of Stock Returns In A Portfolio', color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1200, height=800)\nfig.update_layout(template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nWhen we examine the relationships from both the heatmap and scatter matrix, the pair that exhibits the highest correlation is Microsoft and Google. This can be explained by the fact that these two companies are often mentioned in the same context, as they share similarities in their business models. For instance, Microsoft has Bing while Google dominates the search engine market, both companies have cloud platforms (Azure and Google Cloud Platform), and they offer productivity suites such as Excel and Google Sheets, Word and Google Docs, Gmail and Outlook, among others. To gain a deeper understanding of the relationship between these two stocks, we can visualize their relationship separately and analyze it more closely.\n\nfig = px.scatter(returns, x='MSFT', y='GOOG', title='Scatter Plot Of MSFT Return and GOOG Return',color=\"MSFT\")#,trendline='ols', trendline_color_override='firebrick')#color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1000, height=800)\nfig.update_layout(template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nWe can definitely see a linear relationship here, when one does well the other does well and vice versa, this should be taken into consideration whenever one includes both of them in a portfolio\nWe can also plot a trendline, which is also known as an Ordinary Least Squares (OLS) line. it is often used in scatter plots to depict the overall trend or relationship between two variables.\nBy using a trendline or OLS line in a scatter plot, we can visually observe the direction and strength of the relationship between the variables. The line is positioned to best represent the general pattern of the data points, whether it be a positive or negative correlation, or even no apparent correlation.\n\nfig = px.scatter(returns, x='MSFT', y='GOOG', title='Scatter Plot and Trendline Of MSFT Return and GOOG Return',color=\"MSFT\",trendline='ols', trendline_color_override='firebrick')\nfig.update_layout(width=1000, height=800)\nfig.update_layout(template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nIn addition to representing the overall trend, a trendline or OLS line in a scatter plot can also be utilized for prediction purposes using linear regression. By fitting a linear regression model to the data, we can establish a mathematical relationship between the variables, enabling us to make predictions or estimates based on this model.\nLinear regression allows us to determine the equation of the line that best fits the data points, providing us with a predictive model. This model can then be used to forecast or estimate the value of one variable based on the value of the other variable. By leveraging the linear regression analysis, we can make informed predictions and gain insights into how changes in one variable may impact the other.\nBut again, caution should be exercised when interpreting the relationship between variables based on a scatter plot and linear regression analysis. Although changes in one variable may be associated with changes in the other variable, it does not necessarily imply causation!\n\n\nBAR CHART\nA bar chart, also known as a bar graph, is a popular visualization tool that presents data using rectangular bars of varying heights. Each bar represents a category or group, and the height of the bar corresponds to the value or frequency of that category. Bar charts are effective in comparing different categories or groups and visually displaying patterns or trends in the data.\nFor this particular case, I wanted to examine the volume of trades for each stock in the 4th quarter of 2021. As anticipated, Tesla emerged as the leader of the pack, with approximately 5 billion shares traded. Amazon closely followed with 4 billion shares traded. Surprisingly, Netflix has not been receiving the level of trading activity that I would have expected, considering the company’s size and prominence\n\npf_data2 = pd.DataFrame()\nfor b in assets:\n    pf_data2[b] = yf.download(b, start=\"2021-01-01\", end=\"2021-12-31\", interval='3mo')['Volume']\n\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n[*********************100%***********************]  1 of 1 completed\n\n\n\nfig = px.bar(pf_data2.loc['2021-10-01'],color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nSTACKED BAR CHART\nA stacked bar chart is a visualization tool that represents different categories or groups of data as stacked bars, where each bar segment corresponds to a subcategory or a portion of the whole\nUsing the same example, instead of focusing on the volume traded in a single quarter, I now intend to analyze the volume traded in each of the four quarters of 2021. The verdict remains consistent, with Tesla consistently trading the highest volume. Additionally, another noteworthy observation is that the volume for the tech stocks in the portfolio decreased in the 3rd quarter, the volume had been in a downward trend decreasing QoQ since early 2021.\n\nfig = px.bar(pf_data2)\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nAlternatively, we can plot a side-by-side bar graph for each stock. While a stacked bar chart provides information on the total volume, a side-by-side bar graph presents a clearer depiction of how the volume for each stock has evolved throughout the year.\n\nfig = px.bar(pf_data2,barmode='group')\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nLINE CHART\nLine charts, also known as line graphs or time series plots, are effective visual representations for displaying trends and patterns over time. They are particularly useful when analyzing data that is continuous or sequential in nature, such as stock prices, temperature fluctuations, or population growth.\nIf we plot the stock prices, we can observe the progression of each stock throughout 2021.\n\nfig = px.line(pf_data)\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nHowever, I typically examine how each stock has performed relative to one another. To achieve this, I normalize the data using cumulative returns, making it much easier to identify outperforming and underperforming stocks. By doing so, we can see that Nvidia is leading the pack, followed by Tesla, while Netflix and Meta have significantly underperformed in comparison.\n\ncum_returns =   (1 + returns).cumprod() - 1\nfig = px.line(cum_returns)\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nPIE CHART\nA pie chart is a circular graphical representation that is divided into slices to illustrate the proportional composition of different categories or parts of a whole. Each slice represents a specific category, and its size is determined by the proportion or percentage it contributes to the total. Pie charts are effective in visualizing categorical data and showing the relative sizes or distributions of different categories. They are particularly useful for displaying data that can be grouped into distinct categories and highlighting the relative importance or contribution of each category. By examining a pie chart, we can quickly grasp the overall composition and relative significance of different components within the data.\nIf we want to determine the sectors that contribute the most or are most represented in the S&P 500, we can use a pie chart for this analysis. From the pie chart, we can observe that there are five dominant sectors, with four of them being cyclical. This observation may explain why the S&P 500 tends to underperform during economic downturns. Additionally, the telecommunication sector has the fewest number of companies in the index, indicating that it is the least represented sector\nData Source: https://www.kaggle.com/datasets/paytonfisher/sp-500-companies-with-financial-information?resource=download\n\nSnP_500 = pd.read_csv('/home/mj22/data/financials.csv')\n\nfig = px.pie(SnP_500, names='Sector')\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nBOX PLOT\nA box plot is a graphical representation that provides a visual summary of the distribution of a dataset. It is particularly useful for comparing the distribution of multiple variables or groups. The plot consists of a box that represents the interquartile range (IQR), which contains the middle 50% of the data, with a line inside the box indicating the median. The whiskers extend from the box to the minimum and maximum values, excluding any outliers, which are represented as individual points. By examining the box plot, we can identify the central tendency, spread, skewness, and potential outliers in the data, making it a powerful tool for exploratory data analysis.\nWhen we examine the box plot for Price/Sales for all the stocks in the index, we quickly notice the presence of outliers. The maximum value is 20, the minimum value is 0.15, and the median is 2.89. The first quartile is 1.62, and the third quartile is 4.71.\n\nfig = px.box(SnP_500, y='Price/Sales', color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nWe can also analyze the Price/Sales ratio for each sector in the index, providing us with a more insightful observation. From this analysis, we can observe that the real estate sector has a significantly higher P/S ratio compared to other sectors, while the telecommunications sector appears to have the lowest. However, it’s important to note that this is an inductive observation, as the index only includes the 500 largest companies, which may not present the complete picture.\n\nfig = px.box(SnP_500, y='Price/Sales', x='Sector', color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\n\n\nVIOLIN PLOT\nAlternatively we can use a violin plot, a violin plot combines a box plot with a kernel density plot. It displays the same summary statistics as a box plot, but also provides a more detailed view of the distribution. The plot is symmetrical and resembles a violin or a mirrored density plot. The width of the violin at each point represents the density of data points, with wider areas indicating higher density.\n\nfig = px.violin(SnP_500, y='Earnings/Share', color_discrete_sequence=['firebrick'])\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nCompared to box plots, violin plots offer additional insights into the shape and multimodality of the distribution. They provide a more comprehensive visualization of the data, allowing for a better understanding of its characteristics. However, box plots are more compact and straightforward, making them useful for quick comparisons between multiple groups or variables.\nAdditionally, with a violin plot, we can plot scatter dots alongside to better visualize the concentration of data.\nWe can see that most of the companies in the index have earnings per share between 12 and -3, but the data is more concentrated around the range of 3 to 1. Additionally, the majority of companies have a positive earnings per share.\n\nfig = px.violin(SnP_500, y='Earnings/Share', color_discrete_sequence=['firebrick'], points=\"all\")\nfig.update_layout(width=1000, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nAgain, we could also observe the earnings per share (EPS) for all the sectors. It appears that almost all of the sectors have a higher number of companies with positive earnings, except for the energy sector. This discrepancy raises further questions and warrants investigation to understand why the energy sector has more companies with negative earnings compared to the rest.\n\nfig = px.violin(SnP_500, y='Earnings/Share', x='Sector', color_discrete_sequence=['firebrick'],points=\"all\")\nfig.update_layout(width=1200, height=800, template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nBoth violin plots and box plots serve as valuable tools in exploratory data analysis, helping to identify central tendencies, dispersion, skewness, and potential outliers in a dataset. The choice between the two depends on the specific requirements and the level of detail desired in visualizing the data distribution.\n\n\nHistogram\nA histogram is a graphical representation that displays the distribution of a dataset. It consists of a series of bars, where each bar represents a range of values and the height of the bar corresponds to the frequency or count of observations falling within that range. Histograms provide a visual depiction of the data’s frequency distribution, allowing us to identify patterns, skewness, and central tendencies. They are particularly useful for understanding the shape of the data and detecting any outliers or unusual patterns. By examining the histogram, we can gain insights into the underlying characteristics and distribution of the variable being analyzed\nWhen we plot the weekly distribution of returns for the MSCI World Index, we can observe that most of the data points are situated around -1.5% to 2.5%. Additionally, we can identify outliers scattered along the distribution, indicating extreme or unusual returns compared to the majority of data points.\n\nworld = yf.download('IXUS', \"2013-01-01\", \"2022-06-01\",interval=\"1wk\")['Adj Close']\nworld_returns = world.pct_change(1).dropna()   \n\n[*********************100%***********************]  1 of 1 completed\n\n\n\n# Create histogram using Plotly Express\nfig = px.histogram(world_returns, x=world_returns, nbins=80, color_discrete_sequence=['firebrick'])\n\n# Customize the layout if needed\nfig.update_layout(title=\"Distribution of Returns\", xaxis_title=\"Returns\", yaxis_title=\"Count\",width=1500, height=900,template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nBut the way I always use the distribution of returns is by analyzing the percentage of occurrence for each bin, which provides insights into the potential expectations for the asset going forward. Interpreting the data, we can derive the following conclusions:\n\nIn approximately 66% of the time, the MSCI returns anywhere from -1.5% to +2.5% in a given week.\nIf you were looking to go long on the index with a target return of 3%, the probability of achieving that would be 5.4%.\nConversely, if you were aiming to go short and expecting at least a 2% return, the probability of achieving that return is approximately 9%. However, both probabilities are statistically insignificant.\n\nFrom this analysis, we can conclude that expecting a 3% return or a 2% return is unrealistic, and it may be necessary to adjust your target returns accordingly. Let’s consider targeting a 1% return instead:\n\nIf you were to go long with a minimum target of a 1% return, the probability of that happening would be 33%.\nOn the other hand, if you were to go short with a minimum target of a 1% return, the probability of achieving that desired return would be 23%.\n\nAlthough the odds appear to be better when adjusting our expectations, it is still statistically insignificant. Comparing it to a skydiving scenario where the parachute only works 33% of the time, taking such a risk would not be advisable.\n\n# Create histogram using Plotly Express\nfig = px.histogram(world_returns, nbins=80, color_discrete_sequence=['firebrick'], text_auto=True, histnorm='percent')\n\n\n# Customize the layout if needed\nfig.update_layout(title=\"Distribution of Returns\", xaxis_title=\"Returns\", yaxis_title=\"Count\",width=1500, height=900,template = \"plotly_dark\")\nfig.show()\n\n                                                \n\n\nHistograms are a great tool for exploring data. However, there is still more to uncover. As you may have noticed, we have begun delving into the realm of probability. In our next blog, we will continue this exploration so that we can gain a deeper understanding of distributions and appreciate concepts such as skewness, fat tails, probability of events, and more. By incorporating these theories, we will further enhance our data analysis capabilities.\n\nReferences\n\n“Quantitative Investment Analysis”, by DeFusco, McLeavey, Pinto, and Runkle\n“Introduction to Modern Statistics”, by Mine Çetinkaya-Rundel and Johanna Hardin"
  }
]